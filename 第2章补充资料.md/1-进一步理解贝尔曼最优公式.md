Chapter 3Optimal State Values and BellmanOptimality EquationChapter 2:Bellman EquationChapter 3:Bellman Optimality EquationChapter 4:Value Iteration & Policy IterationChapter 5: Monte Carlo MethodsChapter 7:Temporal-Difference MethodsChapter 8:Value Function MethodsChapter 9:Policy Gradient MethodsChapter 10:Actor-Critic MethodsChapter 6:Stochastic Approximationwith modeltowithout modeltabular representationtofunction representationFundamental toolsAlgorithms/MethodsChapter 1:Basic Conceptspolicy-basedplusvalue-basedFigure 3.1:  Where we are in this book.The ultimate goal of reinforcement learning is to seekoptimal policies.  It is, therefore,necessary to define what optimal policies are.  In this chapter, we introducea core conceptandan  important  tool.  The core concept is theoptimal  state  value, based on which wecan defineoptimal policies.  The important tool is theBellman optimality equation, fromwhich we can solve the optimal state values and policies.The relationship between the previous, present, and subsequent chapters is as follows.The previous chapter (Chapter 2) introduced the Bellman equation of any given policy.35
3.1.   Motivating example:  How to improve policies?The present chapter introduces the Bellman optimality equation, which is a special Bell-man equation whose corresponding policy is optimal.  The next chapter (Chapter 4) willintroduce an important algorithm called value iteration, which is exactly the algorithmfor solving the Bellman optimality equation as introduced in the present chapter.Be  prepared  that  this  chapter  is  slightly  mathematically  intensive.   However,  it  isworth it because many fundamental questions can be clearly answered.3.1    Motivating example:  How to improve policies?r=−1r= 1r= 1r= 1s1s2s3s4Figure 3.2:  An example for demonstrating policy improvement.Consider the policy shown in Figure 3.2.  Here, the orange and blue cells represent theforbidden and target areas, respectively.  The policy here isnot goodbecause it selectsa2(rightward) in states1.  How can we improve the given policy to obtain a better policy?The answer lies in state values and action values.Intuition:It is intuitively clear that the policy can improve if it selectsa3(downward)instead ofa2(rightward) ats1.  This is because moving downward enables the agentto avoid entering the forbidden area.Mathematics:The above intuition can be realized based on the calculation of statevalues and action values.First,  we  calculate  the  state  values  of  the  given  policy.   In  particular,  the  Bellmanequation of this policy isvπ(s1) =−1 +γvπ(s2),vπ(s2) = +1 +γvπ(s4),vπ(s3) = +1 +γvπ(s4),vπ(s4) = +1 +γvπ(s4).36
3.2.   Optimal state values and optimal policiesLetγ= 0.9.  It can be easily solved thatvπ(s4) =vπ(s3) =vπ(s2) = 10,vπ(s1) = 8.Second, we calculate the action values for states1:qπ(s1,a1) =−1 +γvπ(s1) = 6.2,qπ(s1,a2) =−1 +γvπ(s2) = 8,qπ(s1,a3) = 0 +γvπ(s3) = 9,qπ(s1,a4) =−1 +γvπ(s1) = 6.2,qπ(s1,a5) = 0 +γvπ(s1) = 7.2.It is notable that actiona3has the greatest action value:qπ(s1,a3)≥qπ(s1,ai),for alli6= 3.Therefore, we can update the policy to selecta3ats1.This  example  illustrates  that  we  can  obtain  a  better  policy  if  we  update  the  poli-cy  to  select  the  action  with  thegreatest  action  value.   This  is  the  basic  idea  of  manyreinforcement learning algorithms.This example is very simple in the sense that the given policy is only not good forstates1.  If the policy is also not good for the other states, will selecting the action withthe greatest action value still generate a better policy?  Moreover, whether there alwaysexist  optimal  policies?   What  does  an  optimal  policy  look  like?   We  will  answer  all  ofthese questions in this chapter.3.2    Optimal state values and optimal policiesWhile  the  ultimate  goal  of  reinforcement  learning  is  to  obtain  optimal  policies,  it  isnecessary  to  first  define  what  an  optimal  policy  is.   The  definition  is  based  on  statevalues.  In particular, consider two given policiesπ1andπ2.  If the state value ofπ1isgreater than or equal to that ofπ2for any state:vπ1(s)≥vπ2(s),for alls∈S,thenπ1is said to be better thanπ2.  Furthermore, if a policy is better than all the otherpossible policies, then this policy is optimal.  This is formally stated below.37
3.3.   Bellman optimality equationDefinition  3.1(Optimal  policy  and  optimal  state  value).A  policyπ∗is  optimal  ifvπ∗(s)≥vπ(s)for  alls∈ Sand  for  any  other  policyπ.  The  state  values  ofπ∗are  theoptimal state values.The  above  definition  indicates  that  an  optimal  policy  has  the  greatest  state  valuefor  every  state  compared  to  all  the  other  policies.   This  definition  also  leads  to  manyquestions:Existence:  Does the optimal policy exist?Uniqueness:  Is the optimal policy unique?Stochasticity:  Is the optimal policy stochastic or deterministic?Algorithm:  How to obtain the optimal policy and the optimal state values?These  fundamental  questions  must  be  clearly  answered  to  thoroughly  understandoptimal  policies.   For  example,  regarding  the  existence  of  optimal  policies,  if  optimalpolicies do not exist, then we do not need to bother to design algorithms to find them.We will answer all these questions in the remainder of this chapter.3.3    Bellman optimality equationThe tool for analyzing optimal policies and optimal state values is theBellman optimalityequation(BOE). By solving this equation,  we can obtain optimal policies and optimalstate values.  We next present the expression of the BOE and then analyze it in detail.For everys∈S, the elementwise expression of the BOE isv(s) =    maxπ(s)∈Π(s)∑a∈Aπ(a|s)(∑r∈Rp(r|s,a)r+γ∑s′∈Sp(s′|s,a)v(s′))=    maxπ(s)∈Π(s)∑a∈Aπ(a|s)q(s,a),(3.1)wherev(s),v(s′) are unknown variables to be solved andq(s,a).=∑r∈Rp(r|s,a)r+γ∑s′∈Sp(s′|s,a)v(s′).Here,π(s) denotes a policy for states, and Π(s) is the set of all possible policies fors.The BOE is an elegant and powerful tool for analyzing optimal policies.  However,it may be nontrivial to understand this equation.  For example,  this equation has twounknown variablesv(s) andπ(a|s).  It may be confusing to beginners how to solve twounknown variables from one equation.  Moreover, the BOE is actually a special Bellmanequation.  However, it is nontrivial to see that since its expression is quite different fromthat of the Bellman equation. We also need to answer the following fundamental questionsabout the BOE.38
3.3.   Bellman optimality equationExistence:  Does this equation have a solution?Uniqueness:  Is the solution unique?Algorithm:  How to solve this equation?Optimality:  How is the solution related to optimal policies?Once we can answer these questions, we will clearly understand optimal state values andoptimal policies.3.3.1    Maximization of the right-hand side of the BOEWe  next  clarify  how  to  solve  the  maximization  problem  on  the  right-hand  side  of  theBOE in (3.1).  At first glance, it may be confusing to beginners how to solvetwounknownvariablesv(s) andπ(a|s) fromoneequation.  In fact, these two unknown variables canbe solved one by one.  This idea is illustrated by the following example.Example 3.1.Consider two unknown variablesx,y∈Rthat satisfyx= maxy∈R(2x−1−y2).The first step is to solveyon the right-hand side of the equation.  Regardless of the valueofx, we always havemaxy(2x−1−y2) = 2x−1, where the maximum is achieved wheny= 0.  The  second  step  is  to  solvex.  Wheny= 0,  the  equation  becomesx= 2x−1,which leads tox= 1.  Therefore,y= 0andx= 1are the solutions of the equation.We now turn to the maximization problem on the right-hand side of the BOE. TheBOE in (3.1) can be written concisely asv(s) =    maxπ(s)∈Π(s)∑a∈Aπ(a|s)q(s,a),  s∈S.Inspired by Example 3.1, we can first solve the optimalπon the right-hand side.  How todo that?  The following example demonstrates its basic idea.Example 3.2.Givenq1,q2,q3∈R,  we  would  like to  find  the  optimal  values  ofc1,c2,c3to maximize3∑i=1ciqi=c1q1+c2q2+c3q3,wherec1+c2+c3= 1andc1,c2,c3≥0.Without  loss  of  generality,  suppose  thatq3≥q1,q2.   Then,  the  optimal  solution  isc∗3= 1andc∗1=c∗2= 0.  This is becauseq3= (c1+c2+c3)q3=c1q3+c2q3+c3q3≥c1q1+c2q2+c3q3for anyc1,c2,c3.39
3.3.   Bellman optimality equationInspired by the above example, since∑aπ(a|s) = 1, we have∑a∈Aπ(a|s)q(s,a)≤∑a∈Aπ(a|s) maxa∈Aq(s,a) = maxa∈Aq(s,a),where equality is achieved whenπ(a|s) ={1, a=a∗,0, a6=a∗.Here,a∗= arg maxaq(s,a).  In summary, the optimal policyπ(s) is the one that selectsthe action that has the greatest value ofq(s,a).3.3.2    Matrix-vector form of the BOEThe BOE refers to a set of equations defined for all states.  If we combine these equations,we can obtain a concise matrix-vector form, which will be extensively used in this chapter.The matrix-vector form of the BOE isv= maxπ∈Π(rπ+γPπv),(3.2)wherev∈R|S|and maxπis performed in an elementwise manner.  The structures ofrπandPπare the same as those in the matrix-vector form of the normal Bellman equation:[rπ]s.=∑a∈Aπ(a|s)∑r∈Rp(r|s,a)r,[Pπ]s,s′=p(s′|s).=∑a∈Aπ(a|s)p(s′|s,a).Since the optimal value ofπis determined byv, the right-hand side of (3.2) is a functionofv, denoted asf(v).= maxπ∈Π(rπ+γPπv).Then, the BOE can be expressed in a concise form asv=f(v).(3.3)In the remainder of this section, we show how to solve this nonlinear equation.3.3.3    Contraction mapping theoremSince the BOE can be expressed as a nonlinear equationv=f(v),  we next introducethe contraction mapping theorem [6] to analyze it.  The contraction mapping theorem isa powerful tool for analyzing general nonlinear equations.  It is also known as the fixed-point theorem.  Readers who already know this theorem can skip this part.  Otherwise,the reader is advised to be familiar with this theorem since it is the key to analyzing the40
3.3.   Bellman optimality equationBOE.Consider a functionf(x), wherex∈Rdandf:Rd→Rd.  A pointx∗is called afixedpointiff(x∗) =x∗.The  interpretation  of  the  above  equation  is  that  the  map  ofx∗is  itself.   This  is  thereason whyx∗is called “fixed”.  The functionfis acontraction mapping(or contractivefunction) if there existsγ∈(0,1) such that‖f(x1)−f(x2)‖≤γ‖x1−x2‖for anyx1,x2∈Rd.  In this book,‖·‖denotes a vector or matrix norm.Example 3.3.We  present  three  examples  to  demonstrate  fixed  points  and  contractionmappings.x=f(x) = 0.5x,x∈R.It is easy to verify thatx= 0is a fixed point since0 = 0.5·0.  Moreover,f(x) = 0.5xis  a  contraction  mapping  because‖0.5x1−0.5x2‖= 0.5‖x1−x2‖ ≤γ‖x1−x2‖foranyγ∈[0.5,1).x=f(x) =Ax, wherex∈Rn,A∈Rn×nand‖A‖≤γ <1.It  is  easy  to  verify  thatx= 0is  a  fixed  point  since0 =A0.  To  see  the  contractionproperty,‖Ax1−Ax2‖=‖A(x1−x2)‖ ≤ ‖A‖‖x1−x2‖ ≤γ‖x1−x2‖.   Therefore,f(x) =Axis a contraction mapping.x=f(x) = 0.5 sinx,x∈R.It  is  easy  to  see  thatx= 0is  a  fixed  point  since0 = 0.5 sin 0.  Moreover,  it  followsfrom themean value theorem[7, 8] that∣∣∣∣0.5 sinx1−0.5 sinx2x1−x2∣∣∣∣=|0.5 cosx3|≤0.5,  x3∈[x1,x2].As  a  result,|0.5 sinx1−0.5 sinx2| ≤0.5|x1−x2|and  hencef(x)  =  0.5 sinxis  acontraction mapping.The relationship between a fixed point and the contraction property is characterizedby the following classic theorem.Theorem 3.1(Contraction mapping theorem).For any equation that has the formx=f(x)wherexandf(x)are real vectors, iffis a contraction mapping, then the followingproperties hold.Existence:  There exists a fixed pointx∗satisfyingf(x∗) =x∗.41
3.3.   Bellman optimality equationUniqueness:  The fixed pointx∗is unique.Algorithm:  Consider the iterative process:xk+1=f(xk),wherek= 0,1,2,....  Then,xk→x∗ask→ ∞for any initial guessx0.  Moreover,the convergence rate is exponentially fast.The contraction mapping theorem not only can tell whether the solution of a nonlinearequation  exists  but  also  suggests  a  numerical  algorithm  for  solving  the  equation.   Theproof of the theorem is given in Box 3.1.The following example demonstrates how to calculate the fixed points of some equa-tions using the iterative algorithm suggested by the contraction mapping theorem.Example  3.4.Let  us  revisit  the  abovementioned  examples:x=  0.5x,x=Ax,  andx= 0.5 sinx.  While it has been shown that the right-hand sides of these three equationsare all contraction mappings,  it follows from the contraction mapping theorem that theyeach  have  auniquefixed  point,  which  can  be  easily  verified  to  bex∗= 0.  Moreover,  thefixed points of the three equations can be iteratively solved by the following algorithms:xk+1= 0.5xk,xk+1=Axk,xk+1= 0.5 sinxk,given any initial guessx0.Box 3.1:  Proof of the contraction mapping theoremPart 1:  We prove that the sequence{xk}∞k=1withxk=f(xk−1)is convergent.The  proof  relies  onCauchy  sequences.   A  sequencex1,x2,...is  calledCauchyif for any smallε >0,  there existsNsuch that‖xm−xn‖< εfor allm,n > N.The intuitive interpretation is that there exists a finite integerNsuch that all theelements afterNare sufficiently close to each other.  Cauchy sequences are importantbecause it is guaranteed that a Cauchy sequence converges to a limit.  Its convergenceproperty will be used to prove the contraction mapping theorem.  Note that we musthave‖xm−xn‖< εfor  allm,n > N.   If  we  simply  havexn+1−xn→0,  it  isinsufficient to claim that the sequence is a Cauchy sequence.  For example, it holdsthatxn+1−xn→0 forxn=√n, but apparently,xn=√ndiverges.We next show that{xk=f(xk−1)}∞k=1is a Cauchy sequence and hence converges.42
3.3.   Bellman optimality equationFirst, sincefis a contraction mapping, we have‖xk+1−xk‖=‖f(xk)−f(xk−1)‖≤γ‖xk−xk−1‖.Similarly, we have‖xk−xk−1‖≤γ‖xk−1−xk−2‖, . . . ,‖x2−x1‖≤γ‖x1−x0‖.  Thus,we have‖xk+1−xk‖≤γ‖xk−xk−1‖≤γ2‖xk−1−xk−2‖...≤γk‖x1−x0‖.Sinceγ <1, we know that‖xk+1−xk‖converges to zero exponentially fast ask→∞given  anyx1,x0.   Notably,  the  convergence  of{‖xk+1−xk‖}is  not  sufficient  forimplying the convergence of{xk}.  Therefore, we need to further consider‖xm−xn‖for anym > n.  In particular,‖xm−xn‖=‖xm−xm−1+xm−1−···−xn+1+xn+1−xn‖≤‖xm−xm−1‖+···+‖xn+1−xn‖≤γm−1‖x1−x0‖+···+γn‖x1−x0‖=γn(γm−1−n+···+ 1)‖x1−x0‖≤γn(1 +···+γm−1−n+γm−n+γm−n+1+...)‖x1−x0‖=γn1−γ‖x1−x0‖.(3.4)As a result, for anyε, we can always findNsuch that‖xm−xn‖< εfor allm,n > N.Therefore, this sequence is Cauchy and hence converges to a limit point denoted asx∗= limk→∞xk.Part 2:  We show that the limitx∗= limk→∞xkis a fixed point.To do that, since‖f(xk)−xk‖=‖xk+1−xk‖≤γk‖x1−x0‖,we  know  that‖f(xk)−xk‖converges  to  zero  exponentially  fast.   Hence,  we  havef(x∗) =x∗at the limit.Part  3:  We  show  that  the  fixed  point  is  unique.Suppose that there is anotherfixed pointx′satisfyingf(x′) =x′.  Then,‖x′−x∗‖=‖f(x′)−f(x∗)‖≤γ‖x′−x∗‖.43
3.3.   Bellman optimality equationSinceγ <1, this inequality holds if and only if‖x′−x∗‖= 0.  Therefore,x′=x∗.Part  4:  We  show  thatxkconverges  tox∗exponentially  fast.Recall that‖xm−xn‖≤γn1−γ‖x1−x0‖, as proven in (3.4).  Sincemcan be arbitrarily large, we have‖x∗−xn‖=  limm→∞‖xm−xn‖≤γn1−γ‖x1−x0‖.Sinceγ <1, the error converges to zero exponentially fast asn→∞.3.3.4    Contraction property of the right-hand side of the BOEWe next show thatf(v) in the BOE in (3.3) is a contraction mapping.  Thus, the con-traction mapping theorem introduced in the previous subsection can be applied.Theorem 3.2(Contraction property off(v)).The functionf(v)on the right-hand sideof the BOE in(3.3)is a contraction mapping.  In particular, for anyv1,v2∈R|S|, it holdsthat‖f(v1)−f(v2)‖∞≤γ‖v1−v2‖∞,whereγ∈(0,1)is  the  discount  rate,  and‖·‖∞is  the  maximum  norm,  which  is  themaximum absolute value of the elements of a vector.The proof of the theorem is given in Box 3.2.  This theorem is important because wecan use the powerful contraction mapping theorem to analyze the BOE.Box 3.2:  Proof of Theorem 3.2Consider any two vectorsv1,v2∈R|S|, and suppose thatπ∗1.= arg maxπ(rπ+γPπv1)andπ∗2.= arg maxπ(rπ+γPπv2).  Then,f(v1) = maxπ(rπ+γPπv1) =rπ∗1+γPπ∗1v1≥rπ∗2+γPπ∗2v1,f(v2) = maxπ(rπ+γPπv2) =rπ∗2+γPπ∗2v2≥rπ∗1+γPπ∗1v2,where≥is an elementwise comparison.  As a result,f(v1)−f(v2) =rπ∗1+γPπ∗1v1−(rπ∗2+γPπ∗2v2)≤rπ∗1+γPπ∗1v1−(rπ∗1+γPπ∗1v2)=γPπ∗1(v1−v2).44
