## 3.1 激励性例子：如何改进策略？

**(一个用于演示策略改进的例子)**

考虑一个例子，其中有状态 $s_1, s_2, s_3, s_4$。环境中还有禁区（奖励为 $r=-1$）和目标区域（奖励为 $r=1$）。

考虑一个策略，该策略在状态 $s_1$ 时选择动作 $a_2$（向右）。这个策略并不好。我们如何改进给定的策略以获得一个更好的策略？

答案在于状态价值和动作价值。

**直观理解：**

直观上很清楚，如果在 $s_1$ 不选择 $a_2$（向右）而是选择 $a_3$（向下），策略可以得到改进。这是因为向下移动能使智能体避免进入禁区。

**数学实现：**

上述直觉可以通过计算状态价值和动作价值来实现。

首先，我们计算给定策略的状态价值。具体来说，该策略的贝尔曼方程是：

$$
v_π(s_1) = -1 + γv_π(s_2)
$$
$$
v_π(s_2) = +1 + γv_π(s_4)
$$
$$
v_π(s_3) = +1 + γv_π(s_4)
$$
$$
v_π(s_4) = +1 + γv_π(s_4)
$$

## 3.2. 最优状态价值和最优策略

设 $γ= 0.9$。可以很容易地解出：
$v_π(s_4) = v_π(s_3) = v_π(s_2) = 10$，
$v_π(s_1) = 8$。

其次，我们计算状态 $s_1$ 的动作价值：
$q_π(s_1, a_1) = -1 + γv_π(s_1) = 6.2$，
$q_π(s_1, a_2) = -1 + γv_π(s_2) = 8$，
$q_π(s_1, a_3) = 0 + γv_π(s_3) = 9$，
$q_π(s_1, a_4) = -1 + γv_π(s_1) = 6.2$，
$q_π(s_1, a_5) = 0 + γv_π(s_1) = 7.2$。

值得注意的是，动作 $a_3$ 具有最大的动作价值：$q_π(s_1, a_3) ≥ q_π(s_1, a_i)$，对于所有 $i≠3$。
因此，我们可以更新策略，在 $s_1$ 处选择 $a_3$。

这个例子说明，如果我们更新策略以选择具有**最大动作价值**的动作，我们就能获得一个更好的策略。这是许多强化学习算法的基本思想。

这个例子非常简单，因为给定的策略仅在状态 $s_1$ 处表现不佳。如果该策略在其他状态下也表现不佳，选择具有最大动作价值的动作是否仍能产生更好的策略？此外，最优策略是否总是存在？最优策略是什么样的？我们将在本章中回答所有这些问题。

## 3.2 最优状态价值和最优策略

虽然强化学习的最终目标是获得最优策略，但有必要首先定义什么是最优策略。该定义基于状态价值。具体来说，考虑两个给定的策略 $π_1$ 和 $π_2$。如果对于任何状态，$π_1$ 的状态价值大于或等于 $π_2$ 的状态价值：
$v_{π_1}(s) ≥ v_{π_2}(s)$，对于所有 $s∈S$，
那么称 $π_1$ 比 $π_2$ 更好。此外，如果一个策略比所有其他可能的策略都好，那么这个策略就是最优的。下面是正式的陈述。

---
## 3.3. 贝尔曼最优方程

**定义 3.1（最优策略和最优状态价值）。** 如果对于所有 $s∈S$ 和任何其他策略 $π$，$v_{π^*}(s) ≥ v_π(s)$，则策略 $π^*$ 是最优的。$π^*$ 的状态价值是**最优状态价值**。

上述定义表明，与所有其他策略相比，最优策略在每个状态下都具有最大的状态价值。这个定义也引出了许多问题：

*   **存在性：** 最优策略是否存在？
*   **唯一性：** 最优策略是否唯一？
*   **随机性：** 最优策略是随机的还是确定性的？
*   **算法：** 如何获得最优策略和最优状态价值？

这些基本问题必须得到明确的回答，才能彻底理解最优策略。例如，关于最优策略的存在性，如果最优策略不存在，那么我们就不需要费心去设计算法来寻找它们了。
我们将在本章的剩余部分回答所有这些问题。

## 3.3 贝尔曼最优方程

分析最优策略和最优状态价值的工具是**贝尔曼最优方程（BOE）**。通过求解这个方程，我们可以获得最优策略和最优状态价值。我们接下来介绍 BOE 的表达式，然后详细分析它。

对于每个 $s∈S$，BOE 的逐元素表达式是：
$$
v(s) = \max_{\pi(s) \in \Pi(s)} \sum_{a \in A} \pi(a|s) \left( \sum_{r \in R} p(r|s,a)r + \gamma \sum_{s' \in S} p(s'|s,a)v(s') \right)
$$
$$
= \max_{\pi(s) \in \Pi(s)} \sum_{a \in A} \pi(a|s)q(s,a), \quad (3.1)
$$
其中 $v(s)$、$v(s')$ 是待求解的未知变量，且
$$
q(s,a) = \sum_{r \in R} p(r|s,a)r + \gamma \sum_{s' \in S} p(s'|s,a)v(s')
$$
这里，$π(s)$ 表示状态 $s$ 的一个策略，而 $Π(s)$ 是 $s$ 的所有可能策略的集合。

BOE 是一个用于分析最优策略的优雅而强大的工具。然而，理解这个方程可能并非易事。例如，这个方程有两个未知变量 $v(s)$ 和 $π(a|s)$。对于初学者来说，如何从一个方程中求解两个未知变量可能会感到困惑。此外，BOE 实际上是一个特殊的贝尔曼方程。然而，这一点也不容易看出来，因为它的表达式与贝尔曼方程的表达式大相径庭。我们还需要回答关于 BOE 的以下基本问题。
## 3.3. 贝尔曼最优方程

*   **存在性：** 这个方程有解吗？
*   **唯一性：** 解是唯一的吗？
*   **算法：** 如何求解这个方程？
*   **最优性：** 解与最优策略有何关系？

一旦我们能够回答这些问题，我们就能清楚地理解最优状态价值和最优策略。

### 3.3.1 BOE 右侧的最大化

我们接下来阐明如何求解 (3.1) 中 BOE 右侧的最大化问题。乍一看，初学者可能会对如何从**一个**方程中求解**两个**未知变量 $v(s)$ 和 $\pi(a|s)$ 感到困惑。实际上，这两个未知变量可以逐一求解。这个思想可以通过下面的例子来说明。

**例 3.1.**
考虑两个满足以下方程的未知变量 $x, y \in \mathbb{R}$：
$$x = \max_{y \in \mathbb{R}}(2x-1-y^2)$$
第一步是求解方程右侧的 $y$。无论 $x$ 的值是多少，我们总是有
$$\max_{y}(2x-1-y^2) = 2x-1$$
其中最大值在 $y=0$ 时取得。第二步是求解 $x$。当 $y=0$ 时，方程变为 $x = 2x - 1$，解得 $x=1$。因此，$y=0$ 和 $x=1$ 是该方程的解。

我们现在回到 BOE 右侧的最大化问题。 (3.1) 中的 BOE 可以简洁地写为：
$$
v(s) = \max_{\pi(s) \in \Pi(s)} \sum_{a \in A} \pi(a|s)q(s,a), \quad s \in S
$$
受例 3.1 的启发，我们可以先求解右侧的最优 $\pi$。如何做到呢？下面的例子展示了其基本思想。

**例 3.2.**
给定 $q_1, q_2, q_3 \in \mathbb{R}$，我们希望找到 $c_1, c_2, c_3$ 的最优值来最大化
$$
\sum_{i=1}^3 c_i q_i = c_1q_1+c_2q_2+c_3q_3
$$
其中 $c_1+c_2+c_3=1$ 且 $c_1, c_2, c_3 \ge 0$。
不失一般性，假设 $q_3 \ge q_1, q_2$。那么，最优解是 $c^*_3=1$ 和 $c^*_1=c^*_2=0$。这是因为对于任何 $c_1, c_2, c_3$，都有：
$$
q_3 = (c_1+c_2+c_3)q_3 = c_1q_3+c_2q_3+c_3q_3 \ge c_1q_1+c_2q_2+c_3q_3
$$

---
## 3.3. 贝尔曼最优方程

受上述例子的启发，由于 $\sum_a \pi(a|s) = 1$，我们有
$$
\sum_{a \in A} \pi(a|s)q(s,a) \le \sum_{a \in A} \pi(a|s) \max_{a \in A} q(s,a) = \max_{a \in A} q(s,a)
$$
其中等号在以下情况成立：
$$
\pi(a|s) = 
\begin{cases}
1, & a=a^*, \\
0, & a \ne a^*.
\end{cases}
$$
这里，$a^* = \arg\max_a q(s,a)$。总而言之，最优策略 $\pi(s)$ 是选择具有最大 $q(s,a)$ 值的动作的策略。

### 3.3.2 BOE 的矩阵-向量形式

BOE 是指为所有状态定义的一组方程。如果我们将这些方程组合起来，我们可以得到一个简洁的矩阵-向量形式，这在本章中将被广泛使用。

BOE 的矩阵-向量形式是
$$
v = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v), \quad (3.2)
$$
其中 $v \in \mathbb{R}^{|S|}$ 并且 $\max_\pi$ 是逐元素执行的。$r_\pi$ 和 $P_\pi$ 的结构与普通贝尔曼方程的矩阵-向量形式中的结构相同：
$$
[r_\pi]_s = \sum_{a \in A} \pi(a|s) \sum_{r \in R} p(r|s,a)r,
$$
$$
[P_\pi]_{s,s'} = p(s'|s) = \sum_{a \in A} \pi(a|s) p(s'|s,a).
$$
由于 $\pi$ 的最优值由 $v$ 决定，(3.2) 的右侧是 $v$ 的一个函数，记为
$$
f(v) = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v).
$$
那么，BOE 可以用一个简洁的形式表示为
$$
v = f(v). \quad (3.3)
$$
在本节的剩余部分，我们将展示如何求解这个非线性方程。

### 3.3.3 压缩映射定理

由于 BOE 可以表示为一个非线性方程 $v=f(v)$，我们接下来介绍**压缩映射定理** [6] 来对其进行分析。压缩映射定理是分析一般非线性方程的强大工具。它也被称为**不动点定理**。已经了解这个定理的读者可以跳过这部分。否则，建议读者熟悉这个定理，因为它是分析的关键。
## 3.3. 贝尔曼最优方程

BOE。考虑一个函数 $f(x)$，其中 $x \in \mathbb{R}^d$ 且 $f: \mathbb{R}^d \to \mathbb{R}^d$。如果 $f(x^*) = x^*$，则一个点 $x^*$ 被称为**不动点**。

上述方程的解释是 $x^*$ 的映射是它自身。这就是为什么 $x^*$ 被称为“固定的”。如果存在 $γ \in (0,1)$ 使得对于任何 $x_1, x_2 \in \mathbb{R}^d$ 都有 $‖f(x_1) - f(x_2)‖ \le γ‖x_1 - x_2‖$，那么函数 $f$ 就是一个**压缩映射**（或压缩函数）。在本书中，$‖·‖$ 表示一个向量或矩阵范数。

**例 3.3.**
我们给出三个例子来演示不动点和压缩映射。

*   $x=f(x) = 0.5x, x \in \mathbb{R}$。
    很容易验证 $x=0$ 是一个不动点，因为 $0 = 0.5 \cdot 0$。此外，$f(x) = 0.5x$ 是一个压缩映射，因为对于任何 $γ \in [0.5,1)$，都有 $‖0.5x_1 - 0.5x_2‖ = 0.5‖x_1 - x_2‖ \le γ‖x_1 - x_2‖$。

*   $x=f(x) =Ax$，其中 $x \in \mathbb{R}^n, A \in \mathbb{R}^{n \times n}$ 且 $‖A‖ \le γ < 1$。
    很容易验证 $x=0$ 是一个不动点，因为 $0=A0$。要看其压缩性质，$‖Ax_1 - Ax_2‖ = ‖A(x_1 - x_2)‖ \le ‖A‖‖x_1 - x_2‖ \le γ‖x_1 - x_2‖$。因此，$f(x)=Ax$ 是一个压缩映射。

*   $x=f(x) = 0.5 \sin x, x \in \mathbb{R}$。
    很容易看出 $x=0$ 是一个不动点，因为 $0 = 0.5 \sin 0$。此外，根据**中值定理**[7, 8]可得：
    $$
    \left| \frac{0.5 \sin x_1 - 0.5 \sin x_2}{x_1 - x_2} \right| = |0.5 \cos x_3| \le 0.5, \quad x_3 \in [x_1, x_2].
    $$
    因此，$|0.5 \sin x_1 - 0.5 \sin x_2| \le 0.5|x_1 - x_2|$，所以 $f(x) = 0.5 \sin x$ 是一个压缩映射。

不动点和压缩性质之间的关系由以下经典定理刻画。

**定理 3.1 (压缩映射定理).**
对于任何形如 $x=f(x)$ 的方程，其中 $x$ 和 $f(x)$ 是实向量，如果 $f$ 是一个压缩映射，那么以下性质成立。

*   **存在性：** 存在一个不动点 $x^*$ 满足 $f(x^*) = x^*$。
*   **唯一性：** 不动点 $x^*$ 是唯一的。
*   **算法：** 考虑迭代过程：$x_{k+1} = f(x_k)$，其中 $k=0,1,2,...$。那么，对于任意初始猜测 $x_0$，当 $k \to \infty$ 时 $x_k \to x^*$。此外，收敛速度是指数级的。

压缩映射定理不仅可以判断一个非线性方程的解是否存在，还提出了一个求解该方程的数值算法。该定理的证明在专栏 3.1 中给出。

下面的例子演示了如何使用压缩映射定理建议的迭代算法来计算一些方程的不动点。

**例 3.4.**
让我们重新审视上述例子：$x = 0.5x$, $x=Ax$, 以及 $x=0.5 \sin x$。虽然已经证明这三个方程的右侧都是压缩映射，但根据压缩映射定理，它们各自都有一个**唯一**的不动点，可以很容易地验证这个不动点是 $x^* = 0$。此外，这三个方程的不动点可以通过以下算法迭代求解：
$$
x_{k+1} = 0.5x_k,
$$
$$
x_{k+1} = Ax_k,
$$
$$
x_{k+1} = 0.5 \sin x_k,
$$
给定任意初始猜测 $x_0$。

---
**专栏 3.1：压缩映射定理的证明**

**第一部分：我们证明序列 $\{x_k\}_{k=1}^\infty$ (其中 $x_k = f(x_{k-1})$) 是收敛的。**

证明依赖于**柯西序列**。一个序列 $x_1, x_2, ...$ 被称为**柯西序列**，如果对于任意小的 $ε > 0$，存在一个 $N$ 使得对于所有 $m,n > N$ 都有 $‖x_m - x_n‖ < ε$。直观的解释是，存在一个有限整数 $N$，使得所有在 $N$ 之后的元素都彼此足够接近。柯西序列很重要，因为可以保证柯西序列会收敛到一个极限。它的收敛性质将被用来证明压缩映射定理。注意，我们必须有对于**所有** $m, n > N$ 都满足 $‖x_m - x_n‖ < ε$。如果我们仅仅有 $x_{n+1} - x_n \to 0$，这不足以断言该序列是柯西序列。例如，对于 $x_n = \sqrt{n}$，虽然 $x_{n+1} - x_n \to 0$ 成立，但显然 $x_n = \sqrt{n}$ 是发散的。

我们接下来证明 $\{x_k = f(x_{k-1})\}_{k=1}^\infty$ 是一个柯西序列，因此是收敛的。

首先，由于 $f$ 是一个压缩映射，我们有
$$
‖x_{k+1} - x_k‖ = ‖f(x_k) - f(x_{k-1})‖ \le γ‖x_k - x_{k-1}‖.
$$
类似地，我们有 $‖x_k - x_{k-1}‖ \le γ‖x_{k-1} - x_{k-2}‖, \dots, ‖x_2 - x_1‖ \le γ‖x_1 - x_0‖$。因此，我们有
$$
‖x_{k+1} - x_k‖ \le γ‖x_k - x_{k-1}‖ \le γ^2‖x_{k-1} - x_{k-2}‖ \dots \le γ^k‖x_1 - x_0‖.
$$
由于 $γ < 1$，我们知道对于任意 $x_1, x_0$，当 $k \to \infty$ 时，$‖x_{k+1} - x_k‖$ 会以指数速度收敛到零。值得注意的是，$\{‖x_{k+1} - x_k‖\}$ 的收敛性不足以意味着 $\{x_k\}$ 的收敛性。因此，我们需要进一步考虑对于任意 $m > n$ 的 $‖x_m - x_n‖$。具体来说，
$$
\begin{align*}
‖x_m - x_n‖ &= ‖x_m - x_{m-1} + x_{m-1} - \dots - x_{n+1} + x_{n+1} - x_n‖ \\
&\le ‖x_m - x_{m-1}‖ + \dots + ‖x_{n+1} - x_n‖ \\
&\le γ^{m-1}‖x_1 - x_0‖ + \dots + γ^n‖x_1 - x_0‖ \\
&= γ^n(γ^{m-1-n} + \dots + 1)‖x_1 - x_0‖ \\
&\le γ^n(1 + \dots + γ^{m-1-n} + γ^{m-n} + γ^{m-n+1} + \dots)‖x_1 - x_0‖ \\
&= \frac{γ^n}{1-γ}‖x_1 - x_0‖. \quad (3.4)
\end{align*}
$$
因此，对于任意 $ε$，我们总能找到一个 $N$ 使得对于所有 $m, n > N$ 都有 $‖x_m - x_n‖ < ε$。因此，这个序列是柯西序列，所以它会收敛到一个极限点，记为 $x^* = \lim_{k\to\infty} x_k$。

**第二部分：我们证明极限 $x^* = \lim_{k\to\infty} x_k$ 是一个不动点。**

为此，由于 $‖f(x_k) - x_k‖ = ‖x_{k+1} - x_k‖ \le γ^k‖x_1 - x_0‖$，我们知道 $‖f(x_k) - x_k‖$ 以指数速度收敛到零。因此，在极限处我们有 $f(x^*) = x^*$。

**第三部分：我们证明不动点是唯一的。**

假设存在另一个不动点 $x'$ 满足 $f(x') = x'$。那么，
$$
‖x' - x^*‖ = ‖f(x') - f(x^*)‖ \le γ‖x' - x^*‖.
$$
由于 $γ < 1$，这个不等式成立当且仅当 $‖x' - x^*‖ = 0$。因此，$x' = x^*$。

**第四部分：我们证明 $x_k$ 以指数速度收敛到 $x^*$。**

回想一下，正如在 (3.4) 中证明的，$‖x_m - x_n‖ \le \frac{γ^n}{1-γ}‖x_1 - x_0‖$。由于 $m$ 可以任意大，我们有
$$
‖x^* - x_n‖ = \lim_{m \to \infty}‖x_m - x_n‖ \le \frac{γ^n}{1-γ}‖x_1 - x_0‖.
$$
由于 $γ < 1$，当 $n \to \infty$ 时，误差以指数速度收敛到零。

---

### 3.3.4 BOE 右侧的压缩性质

我们接下来证明在 BOE (3.3) 中的 $f(v)$ 是一个压缩映射。因此，上一小节介绍的压缩映射定理可以被应用。

**定理 3.2 ($f(v)$ 的压缩性质).**
BOE (3.3) 右侧的函数 $f(v)$ 是一个压缩映射。特别地，对于任何 $v_1, v_2 \in \mathbb{R}^{|S|}$，成立
$$
‖f(v_1) - f(v_2)‖_∞ \le γ‖v_1 - v_2‖_∞,
$$
其中 $γ \in (0,1)$ 是折扣率，而 $‖·‖_∞$ 是最大范数，即向量元素绝对值的最大值。

该定理的证明在专栏 3.2 中给出。这个定理很重要，因为我们可以使用强大的压缩映射定理来分析 BOE。

---
**专栏 3.2：定理 3.2 的证明**

考虑任意两个向量 $v_1, v_2 \in \mathbb{R}^{|S|}$，并假设
$$
\pi^*_1 = \arg\max_\pi (r_\pi + \gamma P_\pi v_1)
$$
且
$$
\pi^*_2 = \arg\max_\pi (r_\pi + \gamma P_\pi v_2).
$$
那么，
$$
f(v_1) = \max_\pi(r_\pi + \gamma P_\pi v_1) = r_{\pi^*_1} + \gamma P_{\pi^*_1} v_1 \ge r_{\pi^*_2} + \gamma P_{\pi^*_2} v_1,
$$
$$
f(v_2) = \max_\pi(r_\pi + \gamma P_\pi v_2) = r_{\pi^*_2} + \gamma P_{\pi^*_2} v_2 \ge r_{\pi^*_1} + \gamma P_{\pi^*_1} v_2,
$$
其中 $\ge$ 是逐元素的比较。因此，
$$
f(v_1) - f(v_2) = r_{\pi^*_1} + \gamma P_{\pi^*_1} v_1 - (r_{\pi^*_2} + \gamma P_{\pi^*_2} v_2) \le r_{\pi^*_1} + \gamma P_{\pi^*_1} v_1 - (r_{\pi^*_1} + \gamma P_{\pi^*_1} v_2) = \gamma P_{\pi^*_1}(v_1 - v_2).
$$