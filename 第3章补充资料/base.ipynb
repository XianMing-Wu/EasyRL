{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛方法核心设计解析\n",
    "\n",
    "## 📚 教程目标\n",
    "\n",
    "本教程深入解析 `MC_Basic.ipynb` 和 `MC_Epsilon_Greedy.ipynb` 中的核心设计理念，帮助你理解：\n",
    "\n",
    "- **Why (为什么)**：设计决策的背后原因\n",
    "- **What (是什么)**：核心概念和数据结构\n",
    "- **How (怎么做)**：函数间的关联和工作流程\n",
    "\n",
    "我们将通过简洁的、解耦的代码示例，逐步揭示项目设计的精妙之处。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：状态表示设计 - `_pos_to_state` 和 `_state_to_pos`\n",
    "\n",
    "### 🤔 Why：为什么需要状态转换？\n",
    "\n",
    "在网格世界中，我们有两种表示智能体位置的方式：\n",
    "\n",
    "1. **二维坐标 (row, col)**：直观，易于可视化和环境交互\n",
    "2. **一维索引 state**：适合用作数组索引，存储 Q 值表\n",
    "\n",
    "**核心问题**：如何在这两种表示之间高效转换？\n",
    "\n",
    "**设计决策**：使用行优先的线性映射，保证：\n",
    "- ✅ 双向转换的唯一性\n",
    "- ✅ O(1) 时间复杂度\n",
    "- ✅ 连续性（相邻位置 → 接近的状态索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "from typing import Annotated\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"✅ 库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 What：核心映射公式\n",
    "\n",
    "对于一个 `size × size` 的网格：\n",
    "\n",
    "**位置 → 状态**（行优先编号）：\n",
    "```\n",
    "state = row × size + col\n",
    "```\n",
    "\n",
    "**状态 → 位置**（逆向解码）：\n",
    "```\n",
    "row = state // size  （整除）\n",
    "col = state % size   （取余）\n",
    "```\n",
    "\n",
    "**为什么这样设计？**\n",
    "\n",
    "这是一个经典的**二维数组线性化**技巧：\n",
    "- 将二维矩阵按行展开成一维数组\n",
    "- 每一行占据 `size` 个连续位置\n",
    "- 通过整除和取余可以完美还原原始位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：4×4 网格的状态编号可视化\n",
    "def visualize_state_mapping(\n",
    "    size: Annotated[int, \"网格的边长\"] = 4,\n",
    ") -> None:\n",
    "    \"\"\"可视化二维位置到一维状态的映射关系\"\"\"\n",
    "    \n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"  {size}×{size} 网格的状态编号（行优先）\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print()\n",
    "    \n",
    "    # 打印表头\n",
    "    print(\"     \", end=\"\")\n",
    "    for col in range(size):\n",
    "        print(f\"col{col:2d}  \", end=\"\")\n",
    "    print()\n",
    "    print(\"    \" + \"-\" * (size * 7))\n",
    "    \n",
    "    # 打印每一行\n",
    "    for row in range(size):\n",
    "        print(f\"row{row} | \", end=\"\")\n",
    "        for col in range(size):\n",
    "            state = row * size + col\n",
    "            print(f\"  {state:2d}   \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    print(\"💡 观察：\")\n",
    "    print(\"   - 每行的状态索引是连续的\")\n",
    "    print(\"   - 从左到右、从上到下递增\")\n",
    "    print(f\"   - 总共有 {size * size} 个状态（0 到 {size * size - 1}）\")\n",
    "    print()\n",
    "    print(\"🎯 实际意义：\")\n",
    "    print(\"   - Q[state, action] 可以直接用状态索引访问\")\n",
    "    print(\"   - policy[state] 也能高效存储每个状态的策略\")\n",
    "\n",
    "visualize_state_mapping(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 How：函数实现与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateConverter:\n",
    "    \"\"\"状态转换器：演示位置-状态互转的核心逻辑\"\"\"\n",
    "    \n",
    "    def __init__(self, size: Annotated[int, \"网格的边长\"] = 4):\n",
    "        self.size = size\n",
    "    \n",
    "    def pos_to_state(\n",
    "        self, \n",
    "        pos: Annotated[np.ndarray, \"二维位置坐标 [row, col]\"],\n",
    "    ) -> Annotated[int, \"对应的一维状态索引\"]:\n",
    "        \"\"\"将二维位置转换为一维状态索引\n",
    "        \n",
    "        核心公式：state = row × size + col\n",
    "        \n",
    "        为什么这样设计？\n",
    "        - 行优先编号保证了空间连续性\n",
    "        - 相邻的位置对应相近的状态索引\n",
    "        - 便于在数组中存储和访问 Q 值\n",
    "        \"\"\"\n",
    "        return pos[0] * self.size + pos[1]\n",
    "    \n",
    "    def state_to_pos(\n",
    "        self, \n",
    "        state: Annotated[int, \"一维状态索引\"],\n",
    "    ) -> Annotated[np.ndarray, \"对应的二维位置坐标 [row, col]\"]:\n",
    "        \"\"\"将一维状态索引转换为二维位置\n",
    "        \n",
    "        核心公式：\n",
    "        - row = state // size  （整除得到行号）\n",
    "        - col = state % size   （取余得到列号）\n",
    "        \n",
    "        数学原理：\n",
    "        如果 state = row × size + col\n",
    "        那么 state // size = row（因为 col < size）\n",
    "        且   state % size = col（取余操作）\n",
    "        \"\"\"\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return np.array([row, col])\n",
    "\n",
    "# 创建转换器并测试\n",
    "converter = StateConverter(size=4)\n",
    "\n",
    "print(\"🧪 测试状态转换的正确性：\")\n",
    "print()\n",
    "\n",
    "# 测试 1：位置 → 状态\n",
    "test_positions = [\n",
    "    np.array([0, 0]),  # 左上角\n",
    "    np.array([1, 3]),  # 目标位置（MC 示例中的目标）\n",
    "    np.array([2, 1]),  # 中间某个位置\n",
    "    np.array([3, 3]),  # 右下角\n",
    "]\n",
    "\n",
    "for pos in test_positions:\n",
    "    state = converter.pos_to_state(pos)\n",
    "    print(f\"位置 {pos} → 状态 {state}\")\n",
    "\n",
    "print()\n",
    "print(\"🔄 测试双向转换的一致性：\")\n",
    "print()\n",
    "\n",
    "# 测试 2：往返转换验证\n",
    "for state in [0, 7, 10, 15]:\n",
    "    pos = converter.state_to_pos(state)\n",
    "    back_to_state = converter.pos_to_state(pos)\n",
    "    print(f\"状态 {state:2d} → 位置 {pos} → 状态 {back_to_state:2d}  {'✅' if state == back_to_state else '❌'}\")\n",
    "\n",
    "print()\n",
    "print(\"💡 关键洞察：\")\n",
    "print(\"   1. 转换是双射（一一对应），保证了唯一性\")\n",
    "print(\"   2. 无论正向还是反向，转换都是 O(1) 时间复杂度\")\n",
    "print(\"   3. 这种设计让 Q[state, action] 可以高效访问\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 设计优势总结\n",
    "\n",
    "| 方面 | 优势 | 说明 |\n",
    "|------|------|------|\n",
    "| **存储效率** | Q 值表只需二维数组 | `Q[num_states, num_actions]` 而非三维 |\n",
    "| **访问速度** | O(1) 直接索引 | 无需哈希或查找 |\n",
    "| **空间连续性** | 相邻位置 → 相近索引 | 有利于缓存友好性 |\n",
    "| **可扩展性** | 适用于任意大小网格 | 公式统一，无特殊情况 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：策略表示 - `np.random.choice(4, p=policy[state])`\n",
    "\n",
    "### 🤔 Why：为什么用概率分布表示策略？\n",
    "\n",
    "在强化学习中，策略可以是：\n",
    "1. **确定性策略**：每个状态固定选择一个动作\n",
    "2. **随机策略**：每个状态以一定概率选择动作\n",
    "\n",
    "**设计决策**：使用概率分布表示策略，因为：\n",
    "- ✅ **统一表示**：确定性策略是特殊情况（某个动作概率为 1）\n",
    "- ✅ **支持探索**：ε-贪心策略需要概率表示\n",
    "- ✅ **数学优雅**：与策略梯度等高级方法兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 What：策略矩阵的数据结构\n",
    "\n",
    "策略表示为一个二维数组：\n",
    "```python\n",
    "policy.shape = (num_states, num_actions)\n",
    "policy[s, a] = 在状态 s 选择动作 a 的概率\n",
    "```\n",
    "\n",
    "**约束条件**：\n",
    "- 对于每个状态 s：`sum(policy[s, :]) = 1.0`（概率和为 1）\n",
    "- 所有概率非负：`policy[s, a] >= 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_policy_types(\n",
    "    num_states: Annotated[int, \"状态空间大小\"] = 16,\n",
    "    num_actions: Annotated[int, \"动作空间大小\"] = 4,\n",
    ") -> None:\n",
    "    \"\"\"演示不同类型的策略表示\"\"\"\n",
    "    \n",
    "    print(\"📊 策略类型示例（以状态 7 为例）\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 确定性策略（贪婪策略）\n",
    "    greedy_policy = np.zeros((num_states, num_actions))\n",
    "    best_action = 2  # 假设状态 7 的最优动作是 2（左）\n",
    "    greedy_policy[7, best_action] = 1.0\n",
    "    \n",
    "    print(\"\\n1️⃣ 确定性策略（完全贪婪）：\")\n",
    "    print(f\"   policy[7] = {greedy_policy[7]}\")\n",
    "    print(f\"   → 总是选择动作 {best_action}（左）\")\n",
    "    print(f\"   → 概率和验证：{greedy_policy[7].sum():.1f} ✅\")\n",
    "    \n",
    "    # 2. 均匀随机策略\n",
    "    uniform_policy = np.ones((num_states, num_actions)) / num_actions\n",
    "    \n",
    "    print(\"\\n2️⃣ 均匀随机策略（完全探索）：\")\n",
    "    print(f\"   policy[7] = {uniform_policy[7]}\")\n",
    "    print(f\"   → 每个动作概率相同（各 25%）\")\n",
    "    print(f\"   → 概率和验证：{uniform_policy[7].sum():.1f} ✅\")\n",
    "    \n",
    "    # 3. ε-贪心策略（平衡探索与利用）\n",
    "    epsilon = 0.1\n",
    "    epsilon_greedy = np.ones((num_states, num_actions)) * epsilon / num_actions\n",
    "    epsilon_greedy[7, best_action] += (1.0 - epsilon)\n",
    "    \n",
    "    print(f\"\\n3️⃣ ε-贪心策略（ε={epsilon}）：\")\n",
    "    print(f\"   policy[7] = {epsilon_greedy[7]}\")\n",
    "    print(f\"   → 最优动作概率：{epsilon_greedy[7, best_action]:.2f} (90% + 2.5%)\")\n",
    "    print(f\"   → 其他动作概率：{epsilon_greedy[7, 0]:.2f} (2.5% 各)\")\n",
    "    print(f\"   → 概率和验证：{epsilon_greedy[7].sum():.4f} ✅\")\n",
    "    \n",
    "    print(\"\\n💡 关键洞察：\")\n",
    "    print(\"   - 确定性策略：纯利用，可能陷入局部最优\")\n",
    "    print(\"   - 均匀策略：纯探索，学习效率低\")\n",
    "    print(\"   - ε-贪心：平衡探索与利用，实用性最强\")\n",
    "\n",
    "demonstrate_policy_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 How：`np.random.choice` 的精妙之处\n",
    "\n",
    "**核心代码**：\n",
    "```python\n",
    "action = np.random.choice(4, p=policy[state])\n",
    "```\n",
    "\n",
    "**这一行代码的精妙设计**：\n",
    "\n",
    "1. **参数 `4`**：动作空间大小（0, 1, 2, 3 对应 上、下、左、右）\n",
    "2. **参数 `p=policy[state]`**：在状态 `state` 下各动作的概率分布\n",
    "3. **返回值**：按概率随机选择的一个动作索引\n",
    "\n",
    "**为什么这样设计很巧妙？**\n",
    "- ✅ **统一接口**：无论是贪婪策略还是 ε-贪心，都用同一行代码\n",
    "- ✅ **自动采样**：NumPy 自动处理概率分布采样，无需手动实现\n",
    "- ✅ **正确性保证**：只要 `policy[state]` 是有效概率分布，就能正确采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_action_sampling(\n",
    "    num_samples: Annotated[int, \"采样次数\"] = 10000,\n",
    ") -> None:\n",
    "    \"\"\"演示动作采样过程及其统计特性\"\"\"\n",
    "    \n",
    "    print(\"🎲 动作采样演示\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 定义一个 ε-贪心策略（状态 7，最优动作是 2）\n",
    "    epsilon = 0.1\n",
    "    num_actions = 4\n",
    "    best_action = 2\n",
    "    \n",
    "    # 构建策略\n",
    "    policy_state_7 = np.ones(num_actions) * epsilon / num_actions\n",
    "    policy_state_7[best_action] += (1.0 - epsilon)\n",
    "    \n",
    "    print(f\"\\n策略设置（状态 7）：\")\n",
    "    print(f\"   ε = {epsilon}\")\n",
    "    print(f\"   最优动作 = {best_action}（左）\")\n",
    "    print(f\"   策略概率分布：{policy_state_7}\")\n",
    "    print()\n",
    "    \n",
    "    # 方法 1：使用 np.random.choice（推荐）\n",
    "    print(\"1️⃣ 使用 np.random.choice 采样：\")\n",
    "    action_counts_method1 = np.zeros(num_actions)\n",
    "    for _ in range(num_samples):\n",
    "        action = np.random.choice(num_actions, p=policy_state_7)\n",
    "        action_counts_method1[action] += 1\n",
    "    \n",
    "    print(f\"   采样 {num_samples} 次的结果：\")\n",
    "    action_names = ['上', '下', '左', '右']\n",
    "    for a in range(num_actions):\n",
    "        actual_prob = action_counts_method1[a] / num_samples\n",
    "        expected_prob = policy_state_7[a]\n",
    "        print(f\"   动作 {a}（{action_names[a]}）: 实际={actual_prob:.4f}, 期望={expected_prob:.4f}, 误差={abs(actual_prob - expected_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\n💡 为什么使用 np.random.choice？\")\n",
    "    print(\"   ✅ 代码简洁：一行代码完成采样\")\n",
    "    print(\"   ✅ 性能优化：C 语言实现，速度快\")\n",
    "    print(\"   ✅ 数值稳定：自动处理浮点数精度问题\")\n",
    "    print(\"   ✅ 可读性强：意图清晰，易于理解\")\n",
    "\n",
    "demonstrate_action_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 从 Q 值到策略的转换模式\n",
    "\n",
    "#### 模式 1：创建 ε-贪心策略的公式推导\n",
    "\n",
    "**目标**：对于每个状态，最优动作有更高概率，其他动作保留探索概率\n",
    "\n",
    "**数学表达**：\n",
    "$$\n",
    "\\pi(a|s) = \\begin{cases}\n",
    "1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max Q(s,a) \\\\\n",
    "\\frac{\\epsilon}{|A|} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**代码实现**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy_explained(\n",
    "    Q: Annotated[np.ndarray, \"状态-动作价值表\"],\n",
    "    epsilon: Annotated[float, \"探索概率\"],\n",
    "    num_states: Annotated[int, \"状态数量\"],\n",
    "    num_actions: Annotated[int, \"动作数量\"],\n",
    ") -> Annotated[np.ndarray, \"ε-贪心策略矩阵\"]:\n",
    "    \"\"\"创建 ε-贪心策略（详细注释版本）\"\"\"\n",
    "    \n",
    "    # 步骤 1：初始化所有动作的基础探索概率\n",
    "    # 每个动作都有 ε/|A| 的基础概率\n",
    "    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n",
    "    print(f\"步骤 1：基础探索概率 = {epsilon}/{num_actions} = {epsilon/num_actions:.4f}\")\n",
    "    \n",
    "    # 步骤 2：为最优动作增加额外的利用概率\n",
    "    for s in range(num_states):\n",
    "        best_a = np.argmax(Q[s])  # 找到 Q 值最大的动作\n",
    "        # 最优动作获得 (1-ε) 的额外概率\n",
    "        policy[s, best_a] += (1.0 - epsilon)\n",
    "    \n",
    "    print(f\"步骤 2：最优动作额外概率 = {1.0 - epsilon:.4f}\")\n",
    "    print(f\"\\n最终概率分布：\")\n",
    "    print(f\"   - 最优动作：{epsilon/num_actions:.4f} + {1.0-epsilon:.4f} = {epsilon/num_actions + 1.0-epsilon:.4f}\")\n",
    "    print(f\"   - 其他动作：{epsilon/num_actions:.4f}\")\n",
    "    print(f\"   - 概率和验证：{epsilon/num_actions * (num_actions-1) + (epsilon/num_actions + 1.0-epsilon):.4f}\")\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# 示例：创建一个简单的 Q 值表\n",
    "Q_example = np.array([\n",
    "    [0.1, 0.3, 0.8, 0.2],  # 状态 0：动作 2 最优\n",
    "    [0.5, 0.2, 0.1, 0.9],  # 状态 1：动作 3 最优\n",
    "])\n",
    "\n",
    "print(\"\\n🧪 ε-贪心策略创建演示：\")\n",
    "print(\"=\" * 60)\n",
    "policy_example = create_epsilon_greedy_policy_explained(\n",
    "    Q_example, epsilon=0.1, num_states=2, num_actions=4\n",
    ")\n",
    "\n",
    "print(\"\\n生成的策略：\")\n",
    "print(\"状态 0:\", policy_example[0])\n",
    "print(\"状态 1:\", policy_example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：函数关联与工作流程\n",
    "\n",
    "### 🔗 完整的数据流与函数调用链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustrate_complete_workflow() -> None:\n",
    "    \"\"\"图解蒙特卡洛方法的完整工作流程\"\"\"\n",
    "    \n",
    "    workflow = \"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════╗\n",
    "║           蒙特卡洛方法完整工作流程（函数调用链）                   ║\n",
    "╚════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  阶段 1：初始化                                                    │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  Q ← zeros(num_states, num_actions)    # Q 值表初始化            │\n",
    "│  policy ← uniform_policy()             # 策略初始化              │\n",
    "│  returns ← defaultdict(list)           # 回报记录               │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  阶段 2：生成回合（Episode Generation）                           │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  1. reset() → 初始化环境                                          │\n",
    "│     ├─ 设置起始位置（随机或指定）                                 │\n",
    "│     └─ pos_to_state(start_pos) → state  【状态转换】             │\n",
    "│                                                                    │\n",
    "│  2. 循环生成轨迹：                                                │\n",
    "│     while not terminated:                                          │\n",
    "│        ├─ action = np.random.choice(4, p=policy[state]) 【采样】  │\n",
    "│        │    └─ 根据策略概率选择动作                               │\n",
    "│        │                                                           │\n",
    "│        ├─ next_state, reward = step(action)  【环境交互】        │\n",
    "│        │    ├─ new_pos = pos + direction                          │\n",
    "│        │    ├─ reward = rewards[new_pos]                          │\n",
    "│        │    └─ next_state = pos_to_state(new_pos) 【状态转换】   │\n",
    "│        │                                                           │\n",
    "│        └─ episode.append((state, action, reward))                 │\n",
    "│           state = next_state                                       │\n",
    "│                                                                    │\n",
    "│  3. 返回完整回合：[(s₀,a₀,r₁), (s₁,a₁,r₂), ..., (sₜ,aₜ,rₜ₊₁)]  │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  阶段 3：计算回报（Return Calculation）                           │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  G ← 0                                                             │\n",
    "│  for (s, a, r) in reversed(episode):   # 从后向前遍历            │\n",
    "│      G = r + γ × G                      # 累积折扣回报            │\n",
    "│      returns[(s,a)].append(G)           # 记录该 (s,a) 的回报    │\n",
    "│                                                                    │\n",
    "│  公式：Gₜ = Rₜ₊₁ + γRₜ₊₂ + γ²Rₜ₊₃ + ...                         │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  阶段 4：更新 Q 值（Q-Value Update）                              │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  for each (s, a) in episode:                                       │\n",
    "│      Q[s, a] = mean(returns[(s, a)])  # 平均所有回报             │\n",
    "│                                                                    │\n",
    "│  核心思想：Q(s,a) ≈ 从 (s,a) 开始的期望回报                      │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  阶段 5：策略改进（Policy Improvement）                           │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  方式 1：贪婪策略（MC Basic）                                     │\n",
    "│    for each state s:                                               │\n",
    "│        best_a = argmax(Q[s])                                       │\n",
    "│        policy[s, best_a] = 1.0                                     │\n",
    "│        policy[s, others] = 0.0                                     │\n",
    "│                                                                    │\n",
    "│  方式 2：ε-贪心策略（MC Epsilon-Greedy）                         │\n",
    "│    policy = ones(num_states, num_actions) × ε / num_actions      │\n",
    "│    for each state s:                                               │\n",
    "│        best_a = argmax(Q[s])                                       │\n",
    "│        policy[s, best_a] += (1 - ε)                               │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│  关键函数的输入输出关系                                            │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  _pos_to_state:                                                    │\n",
    "│    输入：np.array([row, col])                                      │\n",
    "│    输出：int (state index)                                         │\n",
    "│    用途：环境 → 算法的桥梁                                        │\n",
    "│                                                                    │\n",
    "│  _state_to_pos:                                                    │\n",
    "│    输入：int (state index)                                         │\n",
    "│    输出：np.array([row, col])                                      │\n",
    "│    用途：算法 → 环境的桥梁                                        │\n",
    "│                                                                    │\n",
    "│  np.random.choice:                                                 │\n",
    "│    输入：num_actions, p=policy[state]                              │\n",
    "│    输出：int (action index)                                        │\n",
    "│    用途：策略 → 动作的采样                                        │\n",
    "│                                                                    │\n",
    "│  generate_episode:                                                 │\n",
    "│    输入：policy, start_state (可选)                                │\n",
    "│    输出：[(s, a, r), ...] 轨迹                                     │\n",
    "│    用途：数据收集                                                  │\n",
    "│                                                                    │\n",
    "│  calculate_return:                                                 │\n",
    "│    输入：episode, gamma                                            │\n",
    "│    输出：float (discounted return)                                 │\n",
    "│    用途：评估轨迹价值                                              │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "💡 核心洞察：\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. 状态转换函数是桥梁：连接直观的二维空间与高效的一维索引\n",
    "2. 概率策略是核心：统一了探索与利用，简化了采样逻辑\n",
    "3. 回合生成是基础：通过与环境交互收集经验数据\n",
    "4. Q 值更新是关键：将经验转化为知识（动作价值估计）\n",
    "5. 策略改进是目标：基于 Q 值不断优化决策\n",
    "    \"\"\"\n",
    "    \n",
    "    print(workflow)\n",
    "\n",
    "illustrate_complete_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 MC Basic vs MC Epsilon-Greedy 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mc_algorithms() -> None:\n",
    "    \"\"\"对比两种蒙特卡洛算法的差异\"\"\"\n",
    "    \n",
    "    comparison = \"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════╗\n",
    "║              MC Basic vs MC Epsilon-Greedy 核心差异对比            ║\n",
    "╚════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "┌─────────────────┬──────────────────────┬──────────────────────┐\n",
    "│   对比维度      │    MC Basic          │  MC Epsilon-Greedy   │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 回合起点        │ 必须从每个 (s,a) 开始│ 随机起点即可         │\n",
    "│                 │ （探索性起点）       │                      │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 探索方式        │ 起点保证探索         │ ε-贪心策略保证探索   │\n",
    "│                 │                      │                      │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 策略类型        │ 确定性贪婪策略       │ 随机策略（概率分布） │\n",
    "│                 │ policy[s, a] ∈ {0,1} │ policy[s, a] ∈ [0,1] │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 评估-改进分离   │ 显式分离             │ 隐式融合             │\n",
    "│                 │ 先评估所有(s,a)      │ 每个回合都在优化     │\n",
    "│                 │ 再统一改进策略       │                      │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 采样效率        │ 较低                 │ 较高                 │\n",
    "│                 │ 每个(s,a)都需多次采样│ 经验复用更充分       │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 实用性          │ 理论教学价值高       │ 实际应用更广泛       │\n",
    "│                 │ 清晰展示算法结构     │ 更符合现实约束       │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ 收敛速度        │ 较快（如果能探索全） │ 较慢但更稳定         │\n",
    "│                 │                      │                      │\n",
    "└─────────────────┴──────────────────────┴──────────────────────┘\n",
    "\n",
    "代码对比示例：\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "【MC Basic】\n",
    "────────────────────────────────────────────────────────────────\n",
    "# 1. 必须从每个 (s,a) 开始生成回合\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        episode = env.generate_episode(\n",
    "            start_state=state,      # 指定起点\n",
    "            start_action=action,    # 指定首个动作\n",
    "            policy=policy\n",
    "        )\n",
    "        G = calculate_return(episode, gamma)\n",
    "        returns[(state, action)].append(G)\n",
    "\n",
    "# 2. 更新 Q 值\n",
    "for (s, a) in all_state_action_pairs:\n",
    "    Q[s, a] = mean(returns[(s, a)])\n",
    "\n",
    "# 3. 策略改进（贪婪）\n",
    "for s in range(num_states):\n",
    "    best_a = argmax(Q[s])\n",
    "    policy[s, best_a] = 1.0  # 确定性策略\n",
    "    policy[s, others] = 0.0\n",
    "\n",
    "【MC Epsilon-Greedy】\n",
    "────────────────────────────────────────────────────────────────\n",
    "# 1. 从随机起点生成回合\n",
    "for episode_num in range(num_episodes):\n",
    "    # 策略自动包含最新的 Q 值\n",
    "    policy = create_epsilon_greedy_policy(Q, epsilon)\n",
    "    \n",
    "    # 随机起点\n",
    "    episode = env.generate_episode(policy)  # 无需指定起点\n",
    "    \n",
    "    # 2. 更新 Q 值（首次访问）\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for (s, a, r) in reversed(episode):\n",
    "        G = r + gamma * G\n",
    "        if (s, a) not in visited:\n",
    "            returns[(s, a)].append(G)\n",
    "            Q[s, a] = mean(returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "    \n",
    "    # 3. 策略隐式改进（下一轮自动使用新 Q 值）\n",
    "\n",
    "核心差异总结：\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "MC Basic:\n",
    "  优点：算法结构清晰，评估-改进明确分离\n",
    "  缺点：需要探索性起点，实际应用中难以满足\n",
    "  适用：教学演示，理解蒙特卡洛方法的基本原理\n",
    "\n",
    "MC Epsilon-Greedy:\n",
    "  优点：无需特殊起点，通过策略本身保证探索\n",
    "  缺点：需要更多回合才能收敛\n",
    "  适用：实际应用，更符合现实约束\n",
    "\n",
    "共同点：\n",
    "  - 都是无模型（model-free）方法\n",
    "  - 都基于完整回合进行学习\n",
    "  - 都使用蒙特卡洛估计 Q 值\n",
    "  - 都保证收敛到最优策略（在一定条件下）\n",
    "    \"\"\"\n",
    "    \n",
    "    print(comparison)\n",
    "\n",
    "compare_mc_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结：设计哲学与最佳实践\n",
    "\n",
    "### 🎯 核心设计原则\n",
    "\n",
    "1. **抽象的力量**\n",
    "   - 状态转换：隐藏实现细节，提供简洁接口\n",
    "   - 策略表示：统一确定性和随机策略\n",
    "   - 回合生成：封装复杂的交互逻辑\n",
    "\n",
    "2. **单一职责**\n",
    "   - 每个函数做好一件事\n",
    "   - `_pos_to_state`：只负责转换\n",
    "   - `generate_episode`：只负责生成轨迹\n",
    "   - `calculate_return`：只负责计算回报\n",
    "\n",
    "3. **数据驱动**\n",
    "   - Q 值表：存储学到的知识\n",
    "   - 策略矩阵：编码决策规则\n",
    "   - 回报记录：追踪学习历史\n",
    "\n",
    "### 💡 关键洞察\n",
    "\n",
    "- **好的设计是看不见的**：使用时感觉自然，那就是好设计\n",
    "- **抽象的价值**：正确的抽象让复杂问题变简单\n",
    "- **统一的优雅**：统一的接口让代码简洁优美\n",
    "- **类型注解的力量**：自文档化，减少错误，易于维护\n",
    "\n",
    "### 🚀 扩展建议\n",
    "\n",
    "1. **改进探索策略**\n",
    "   - 递减 ε：`ε = ε_start × decay^episode`\n",
    "   - UCB 探索：选择不确定性高的动作\n",
    "   - 好奇心驱动：奖励未探索的状态\n",
    "\n",
    "2. **优化采样效率**\n",
    "   - 经验回放：重复利用历史回合\n",
    "   - 重要性采样：纠正 off-policy 偏差\n",
    "   - n-step returns：平衡偏差和方差\n",
    "\n",
    "3. **函数逼近**\n",
    "   - 线性函数：Q(s,a) ≈ w^T φ(s,a)\n",
    "   - 神经网络：深度 Q 网络 (DQN)\n",
    "   - 特征工程：手工设计状态特征\n",
    "\n",
    "### 🎓 下一步\n",
    "\n",
    "现在你已经深入理解了蒙特卡洛方法的设计精髓，可以：\n",
    "\n",
    "1. 回到原始代码，重新审视每个设计决策\n",
    "2. 尝试实现一个变种（如递减 ε 或 n-step MC）\n",
    "3. 思考如何将这些模式应用到其他强化学习算法\n",
    "\n",
    "**记住**：理解设计的 Why、What、How，比记住代码本身更重要！\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
