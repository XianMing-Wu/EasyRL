{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Îµ-è´ªå¿ƒç®—æ³•æ•™ç¨‹ï¼šæ›´å®ç”¨çš„è’™ç‰¹å¡æ´›æ–¹æ³•\n",
    "\n",
    "## ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦ MC Îµ-è´ªå¿ƒï¼Ÿ\n",
    "\n",
    "åœ¨ `MC Basic` ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è’™ç‰¹å¡æ´›æ–¹æ³•çš„åŸºæœ¬åŸç†ã€‚ä½†å®ƒæœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ï¼š**æ¢ç´¢æ€§èµ·ç‚¹ (Exploring Starts)**ã€‚è¿™æ„å‘³ç€ä¸ºäº†è¯„ä¼°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œæˆ‘ä»¬éœ€è¦ä»æ¯ä¸€ä¸ªå¯èƒ½çš„ (s,a) å¯¹å¼€å§‹ç”Ÿæˆå›åˆã€‚\n",
    "\n",
    "åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè¿™ä¸ªå‡è®¾å¾€å¾€éš¾ä»¥æ»¡è¶³ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•éšæ„è®¾ç½®æœºå™¨äººçš„åˆå§‹çŠ¶æ€å’Œåˆå§‹åŠ¨ä½œã€‚\n",
    "\n",
    "**MC Îµ-è´ªå¿ƒç®—æ³•**é€šè¿‡å¼•å…¥â€œæ¢ç´¢â€æœºåˆ¶è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½¿å…¶æ›´åŠ å®ç”¨ï¼š\n",
    "\n",
    "- **æ— éœ€æ¢ç´¢æ€§èµ·ç‚¹**ï¼šé€šè¿‡ Îµ-è´ªå¿ƒç­–ç•¥åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¿›è¡Œæ¢ç´¢ã€‚\n",
    "- **å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨**ï¼šä»¥ 1-Îµ çš„æ¦‚ç‡é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ï¼Œä»¥ Îµ çš„æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ã€‚\n",
    "- **åœ¨çº¿ç­–ç•¥ (On-policy)**ï¼šç”¨äºç”Ÿæˆæ•°æ®çš„ç­–ç•¥å’Œè¢«è¯„ä¼°æ”¹è¿›çš„ç­–ç•¥æ˜¯åŒä¸€ä¸ªã€‚\n",
    "- **ä¿è¯æ”¶æ•›**ï¼šåœ¨ä¸€å®šæ¡ä»¶ä¸‹ï¼Œå®ƒèƒ½æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚\n",
    "\n",
    "## ğŸ¯ æœ¬æ•™ç¨‹ç›®æ ‡\n",
    "\n",
    "æˆ‘ä»¬å°†ä»é›¶å¼€å§‹å®ç° MC Îµ-è´ªå¿ƒç®—æ³•ï¼Œå¹¶åœ¨åŒä¸€ä¸ª 4Ã—4 ç½‘æ ¼ä¸–ç•Œä¸­è¿›è¡Œå¯è§†åŒ–ï¼š\n",
    "\n",
    "- **æ ¸å¿ƒå†…å®¹**ï¼š\n",
    "  - ç†è§£ Îµ-è´ªå¿ƒç­–ç•¥å¦‚ä½•å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚\n",
    "  - å®ç°åŸºäºå›åˆé‡‡æ ·çš„ on-policy è’™ç‰¹å¡æ´›æ§åˆ¶ã€‚\n",
    "  - å¯è§†åŒ– Q å€¼å’Œç­–ç•¥åœ¨æ¢ç´¢ä¸­çš„æ”¶æ•›è¿‡ç¨‹ã€‚\n",
    "\n",
    "- **å­¦ä¹ è·¯å¾„**ï¼š\n",
    "  1. ç†è§£ Îµ-è´ªå¿ƒç­–ç•¥ã€‚\n",
    "  2. æ„å»ºæ”¯æŒéšæœºèµ·å§‹å›åˆçš„ç¯å¢ƒã€‚\n",
    "  3. å®ç° MC Îµ-è´ªå¿ƒæ ¸å¿ƒç®—æ³•ã€‚\n",
    "  4. å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹ï¼ˆGIF åŠ¨ç”»ï¼‰ã€‚\n",
    "  5. åˆ†æä¸ MC Basic çš„å¼‚åŒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "è®©æˆ‘ä»¬å¼€å§‹æ¢ç´¢è¿™ç§æ›´å¼ºå¤§çš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€æ­¥ï¼šå®‰è£…å’Œå¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom typing import Optional, Tuple, Dict, Any, List, Annotated\nfrom collections import defaultdict\n\n# è®¾ç½® matplotlib åç«¯ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ pyplot ä¹‹å‰ï¼‰\nimport matplotlib\nmatplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆäº‘ç«¯ç¯å¢ƒ\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom IPython.display import display, clear_output, Image as IPImage\nimport imageio\nfrom io import BytesIO\n\n# é…ç½® matplotlib ä¸­æ–‡æ˜¾ç¤º\nplt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(f\"âœ… Gymnasium ç‰ˆæœ¬: {gym.__version__}\")\nprint(f\"âœ… NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"âœ… Matplotlib ç‰ˆæœ¬: {matplotlib.__version__}\")\nprint(f\"âœ… Matplotlib åç«¯: {matplotlib.get_backend()}\")\nprint(f\"âœ… ä¸­æ–‡å­—ä½“é…ç½®å®Œæˆ\")\nprint(f\"âœ… imageio å·²å¯¼å…¥ï¼Œæ”¯æŒ GIF åŠ¨ç”»ç”Ÿæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ”¯æŒ MC å­¦ä¹ çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\n",
    "\n",
    "ä¸ºäº†é€‚åº” Îµ-è´ªå¿ƒç®—æ³•ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç¯å¢ƒåšä¸€äº›å¾®è°ƒã€‚ä¸»è¦åŒºåˆ«åœ¨äºå›åˆçš„ç”Ÿæˆæ–¹å¼ï¼š\n",
    "\n",
    "- **éšæœºèµ·ç‚¹**ï¼šå›åˆå¯ä»¥ä»ä»»æ„éç»ˆæ­¢çŠ¶æ€éšæœºå¼€å§‹ã€‚\n",
    "- **ç­–ç•¥é©±åŠ¨**ï¼šæ•´ä¸ªå›åˆéƒ½ç”±ä¸€ä¸ªç»™å®šçš„ç­–ç•¥ï¼ˆå³ Îµ-è´ªå¿ƒç­–ç•¥ï¼‰é©±åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GridWorldEpsilonGreedy(gym.Env):\n    \"\"\"æ”¯æŒ MC Îµ-è´ªå¿ƒå­¦ä¹ çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\"\"\"\n    \n    def __init__(\n        self,\n        size: Annotated[int, \"ç½‘æ ¼çš„è¾¹é•¿\"] = 4,\n        rewards: Annotated[Optional[np.ndarray], \"å¯é€‰çš„å¥–åŠ±çŸ©é˜µ\"] = None,\n    ):\n        super().__init__()\n        self.size = size\n        \n        # è®¾ç½®å¥–åŠ±çŸ©é˜µ\n        if rewards is None:\n            self.rewards = np.array([\n                [0, 0, -1, -1],\n                [0, -1, -1, 1],\n                [0, -1, 0, 0],\n                [-1, 0, 0, -1]\n            ])\n        else:\n            self.rewards = rewards\n        \n        # ç›®æ ‡ä½ç½®å’Œéšœç¢ç‰©ä½ç½®\n        self.target_pos = np.argwhere(self.rewards == 1)[0]\n        self.obstacle_mask = (self.rewards == -1)\n        \n        # åŠ¨ä½œç©ºé—´ï¼š0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Discrete(size * size)\n        \n        # åŠ¨ä½œæ˜ å°„\n        self._action_to_direction = {\n            0: np.array([-1, 0]),  # ä¸Š\n            1: np.array([1, 0]),   # ä¸‹\n            2: np.array([0, -1]),  # å·¦\n            3: np.array([0, 1]),   # å³\n        }\n        \n        self._agent_location = np.array([0, 0])\n    \n    def _pos_to_state(\n        self,\n        pos: Annotated[np.ndarray, \"äºŒç»´ä½ç½®åæ ‡\"],\n    ) -> Annotated[int, \"å¯¹åº”çš„ä¸€ç»´çŠ¶æ€ç´¢å¼•\"]:\n        \"\"\"å°†äºŒç»´ä½ç½®è½¬æ¢ä¸ºä¸€ç»´çŠ¶æ€ç´¢å¼•\"\"\"\n        return pos[0] * self.size + pos[1]\n    \n    def _state_to_pos(\n        self,\n        state: Annotated[int, \"ä¸€ç»´çŠ¶æ€ç´¢å¼•\"],\n    ) -> Annotated[np.ndarray, \"å¯¹åº”çš„äºŒç»´ä½ç½®åæ ‡\"]:\n        \"\"\"å°†ä¸€ç»´çŠ¶æ€ç´¢å¼•è½¬æ¢ä¸ºäºŒç»´ä½ç½®\"\"\"\n        return np.array([state // self.size, state % self.size])\n    \n    def reset(\n        self,\n        start_pos: Annotated[Optional[np.ndarray], \"å¯é€‰çš„èµ·å§‹ä½ç½®åæ ‡\"] = None,\n    ) -> Annotated[Tuple[int, Dict[str, Any]], \"åˆå§‹çŠ¶æ€ç´¢å¼•åŠé™„åŠ ä¿¡æ¯å­—å…¸\"]:\n        \"\"\"é‡ç½®ç¯å¢ƒï¼Œå¯ä»¥æŒ‡å®šæˆ–éšæœºé€‰æ‹©èµ·å§‹ä½ç½®\"\"\"\n        if start_pos is not None:\n            self._agent_location = np.array(start_pos)\n        else:\n            # éšæœºé€‰æ‹©ä¸€ä¸ªéç»ˆæ­¢ã€ééšœç¢çš„èµ·å§‹ä½ç½®\n            while True:\n                pos = np.random.randint(0, self.size, size=2)\n                if not np.array_equal(pos, self.target_pos) and not self.obstacle_mask[pos[0], pos[1]]:\n                    self._agent_location = pos\n                    break\n        \n        state = self._pos_to_state(self._agent_location)\n        return state, {}\n    \n    def step(\n        self,\n        action: Annotated[int, \"è¦æ‰§è¡Œçš„åŠ¨ä½œç¼–å·\"],\n    ) -> Annotated[Tuple[int, float, bool, bool, Dict[str, Any]], \"æ–°çŠ¶æ€ã€å¥–åŠ±ã€ç»ˆæ­¢æ ‡å¿—ã€æˆªæ–­æ ‡å¿—åŠé¢å¤–ä¿¡æ¯\"]:\n        \"\"\"æ‰§è¡ŒåŠ¨ä½œ\"\"\"\n        direction = self._action_to_direction[action]\n        new_location = self._agent_location + direction\n        \n        if (0 <= new_location[0] < self.size and 0 <= new_location[1] < self.size):\n            self._agent_location = new_location\n        \n        reward = self.rewards[self._agent_location[0], self._agent_location[1]]\n        terminated = np.array_equal(self._agent_location, self.target_pos)\n        state = self._pos_to_state(self._agent_location)\n        return state, reward, terminated, False, {}\n    \n    def generate_episode(\n        self,\n        policy: Annotated[np.ndarray, \"ç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"],\n        max_steps: Annotated[int, \"å•ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°\"] = 100,\n    ) -> Annotated[List[Tuple[int, int, float]], \"çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±åºåˆ—\"]:\n        \"\"\"ä»éšæœºçŠ¶æ€å¼€å§‹ï¼Œæ ¹æ®ç­–ç•¥ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›åˆ\"\"\"\n        state, _ = self.reset() # éšæœºèµ·ç‚¹\n        episode = []\n        \n        for _ in range(max_steps):\n            action = np.random.choice(self.action_space.n, p=policy[state])\n            next_state, reward, terminated, _, _ = self.step(action)\n            episode.append((state, action, reward))\n            if terminated:\n                break\n            state = next_state\n        \n        return episode\n\n# åˆ›å»ºç¯å¢ƒå®ä¾‹\nenv = GridWorldEpsilonGreedy(size=4)\nprint(f\"âœ… GridWorldEpsilonGreedy ç¯å¢ƒåˆ›å»ºå®Œæˆ\")\nprint(f\"   - ç½‘æ ¼å¤§å°: {env.size}Ã—{env.size}\")\nprint(f\"   - åŠ¨ä½œç©ºé—´: {env.action_space} (0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³)\")\nprint(f\"   - ç›®æ ‡ä½ç½®: {env.target_pos}\")\nprint(f\"\\n   å¥–åŠ±çŸ©é˜µ:\")\nprint(env.rewards)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰æ­¥ï¼šç†è§£ MC Îµ-è´ªå¿ƒç®—æ³•\n",
    "\n",
    "### ğŸ§® Îµ-è´ªå¿ƒç­–ç•¥\n",
    "\n",
    "ä¸ºäº†ç¡®ä¿åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æœ‰è¶³å¤Ÿçš„æ¢ç´¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ **Îµ-è´ªå¿ƒç­–ç•¥**ã€‚å¯¹äºä»»æ„çŠ¶æ€ $s$ï¼Œç­–ç•¥ $\\pi$ å®šä¹‰å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \n",
    "\\begin{cases} \n",
    "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \n",
    "\\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a \\neq \\arg\\max_{a'} Q(s,a') \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $1 - \\epsilon$ æ˜¯â€œåˆ©ç”¨â€çš„æ¦‚ç‡ï¼Œå³é€‰æ‹©å½“å‰ä¼°è®¡çš„æœ€å¥½åŠ¨ä½œã€‚\n",
    "- $\\epsilon$ æ˜¯â€œæ¢ç´¢â€çš„æ¦‚ç‡ï¼Œå³éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚\n",
    "- $|\\mathcal{A}(s)|$ æ˜¯çŠ¶æ€ $s$ ä¸‹çš„åŠ¨ä½œæ•°é‡ã€‚\n",
    "\n",
    "### ğŸ”„ On-Policy MC Control ç®—æ³•æµç¨‹\n",
    "\n",
    "**åˆå§‹åŒ–**ï¼š\n",
    "- éšæœºåˆå§‹åŒ– $Q(s,a)$ã€‚\n",
    "- åˆå§‹åŒ– `returns(s,a)` ä¸ºç©ºåˆ—è¡¨ã€‚\n",
    "- æ ¹æ®åˆå§‹ Q å€¼åˆ›å»ºä¸€ä¸ª Îµ-è´ªå¿ƒç­–ç•¥ $\\pi$ã€‚\n",
    "\n",
    "**å¾ªç¯ï¼ˆç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§å›åˆæ•°ï¼‰ï¼š**\n",
    "\n",
    "1. **ç”Ÿæˆå›åˆ**ï¼šä½¿ç”¨å½“å‰ç­–ç•¥ $\\pi$ ä»ä¸€ä¸ªéšæœºèµ·ç‚¹ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›åˆã€‚\n",
    "   - `(S_0, A_0, R_1), (S_1, A_1, R_2), ..., (S_{T-1}, A_{T-1}, R_T)`\n",
    "\n",
    "2. **è®¡ç®—å›æŠ¥**ï¼šå¯¹äºå›åˆä¸­æ¯ä¸ªæ—¶é—´æ­¥ $t=0, 1, ..., T-1$ï¼š\n",
    "   - è®¡ç®—ä»è¯¥æ­¥å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥ $G_t = R_{t+1} + \\gamma R_{t+2} + ...$\n",
    "\n",
    "3. **æ›´æ–° Q å€¼**ï¼šå¯¹äºå›åˆä¸­å‡ºç°çš„æ¯ä¸€ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ $(S_t, A_t)$ï¼š\n",
    "   - å°†å›æŠ¥ $G_t$ æ·»åŠ åˆ° `returns(S_t, A_t)` åˆ—è¡¨ä¸­ã€‚\n",
    "   - æ›´æ–° $Q(S_t, A_t)$ ä¸º `returns(S_t, A_t)` çš„å¹³å‡å€¼ã€‚\n",
    "\n",
    "4. **æ›´æ–°ç­–ç•¥**ï¼šæ ¹æ®æ›´æ–°åçš„ Q å€¼ï¼Œæ”¹è¿› Îµ-è´ªå¿ƒç­–ç•¥ $\\pi$ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››æ­¥ï¼šå®šä¹‰ç®—æ³•å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç®—æ³•å‚æ•°é…ç½®å®Œæˆ\n",
      "   - ç½‘æ ¼å¤§å°: 4Ã—4\n",
      "   - æŠ˜æ‰£å› å­ Î³: 0.9\n",
      "   - Epsilon Îµ: 0.1\n",
      "   - æ€»å›åˆæ•°: 5000\n",
      "   - åŠ¨ä½œç©ºé—´: ['â†‘', 'â†“', 'â†', 'â†’']\n"
     ]
    }
   ],
   "source": [
    "# --- ç®—æ³•å‚æ•° ---\n",
    "GRID_SIZE = 4           # ç½‘æ ¼å¤§å°\n",
    "GAMMA = 0.9             # æŠ˜æ‰£å› å­\n",
    "EPSILON = 0.1           # Îµ-è´ªå¿ƒç­–ç•¥ä¸­çš„ Îµ å€¼\n",
    "NUM_EPISODES = 5000     # æ€»å…±è¦è¿è¡Œçš„å›åˆæ•°\n",
    "MAX_STEPS = 100         # æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°\n",
    "\n",
    "# åŠ¨ä½œåç§°ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰\n",
    "ACTION_NAMES = ['â†‘', 'â†“', 'â†', 'â†’']\n",
    "\n",
    "print(f\"âœ… ç®—æ³•å‚æ•°é…ç½®å®Œæˆ\")\n",
    "print(f\"   - ç½‘æ ¼å¤§å°: {GRID_SIZE}Ã—{GRID_SIZE}\")\n",
    "print(f\"   - æŠ˜æ‰£å› å­ Î³: {GAMMA}\")\n",
    "print(f\"   - Epsilon Îµ: {EPSILON}\")\n",
    "print(f\"   - æ€»å›åˆæ•°: {NUM_EPISODES}\")\n",
    "print(f\"   - åŠ¨ä½œç©ºé—´: {ACTION_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”æ­¥ï¼šå®ç° MC Îµ-è´ªå¿ƒæ ¸å¿ƒç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_epsilon_greedy_policy(\n    Q: Annotated[np.ndarray, \"çŠ¶æ€-åŠ¨ä½œä»·å€¼è¡¨\"],\n    epsilon: Annotated[float, \"Îµ-è´ªå¿ƒç­–ç•¥ä¸­çš„æ¢ç´¢æ¦‚ç‡\"],\n    num_actions: Annotated[int, \"åŠ¨ä½œç©ºé—´çš„å¤§å°\"],\n) -> Annotated[np.ndarray, \"Îµ-è´ªå¿ƒç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"]:\n    \"\"\"æ ¹æ® Q å€¼åˆ›å»ºä¸€ä¸ª Îµ-è´ªå¿ƒç­–ç•¥\"\"\"\n    num_states = Q.shape[0]\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])\n        policy[state, best_action] += (1.0 - epsilon)\n    \n    return policy\n\ndef get_policy_matrix(\n    policy: Annotated[np.ndarray, \"ç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"],\n    grid_size: Annotated[int, \"ç½‘æ ¼çš„è¾¹é•¿\"],\n) -> Annotated[np.ndarray, \"ç”¨äºå¯è§†åŒ–çš„ç­–ç•¥ç®­å¤´çŸ©é˜µ\"]:\n    \"\"\"å°†ç­–ç•¥è½¬æ¢ä¸ºç®­å¤´çŸ©é˜µç”¨äºå¯è§†åŒ–\"\"\"\n    policy_arrows = np.empty((grid_size, grid_size), dtype=object)\n    \n    for state in range(grid_size * grid_size):\n        row = state // grid_size\n        col = state % grid_size\n        best_action = np.argmax(policy[state])\n        policy_arrows[row, col] = ACTION_NAMES[best_action]\n    \n    return policy_arrows\n\nprint(\"âœ… MC Îµ-è´ªå¿ƒæ ¸å¿ƒå‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­æ­¥ï¼šå®ç°å¯è§†åŒ–å‡½æ•°\n",
    "\n",
    "å¯è§†åŒ–å‡½æ•°ä¸ `MC Basic` åŸºæœ¬ç›¸åŒï¼Œä½†æ ‡é¢˜å’Œè¿­ä»£è®¡æ•°æ–¹å¼æœ‰æ‰€è°ƒæ•´ï¼Œä»¥åæ˜ æ˜¯åŸºäºå›åˆæ•°è¿›è¡Œæ›´æ–°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def render_mc_epsilon_greedy_to_array(\n    rewards: Annotated[np.ndarray, \"å¥–åŠ±çŸ©é˜µ\"],\n    Q: Annotated[np.ndarray, \"çŠ¶æ€-åŠ¨ä½œä»·å€¼è¡¨\"],\n    policy_arrows: Annotated[np.ndarray, \"ç­–ç•¥ç®­å¤´çŸ©é˜µ\"],\n    episode_num: Annotated[int, \"å½“å‰å›åˆæ•°\"],\n) -> Annotated[np.ndarray, \"ç”¨äºç”ŸæˆåŠ¨ç”»çš„ RGB æ•°ç»„\"]:\n    \"\"\"\n    å°†å½“å‰ MC Îµ-è´ªå¿ƒçŠ¶æ€æ¸²æŸ“ä¸º RGB æ•°ç»„ï¼ˆç”¨äº GIF ç”Ÿæˆï¼‰\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=80)\n    \n    # é…è‰²æ–¹æ¡ˆ\n    reward_cmap = LinearSegmentedColormap.from_list('reward', ['red', 'white', 'green'])\n    value_cmap = 'viridis'\n    \n    # 1. ç»˜åˆ¶å¥–åŠ±çŸ©é˜µ\n    ax1 = axes[0]\n    im1 = ax1.imshow(rewards, cmap=reward_cmap, vmin=-1, vmax=1)\n    ax1.set_title(f'Reward Matrix', fontsize=14, fontweight='bold')\n    ax1.set_xticks(range(GRID_SIZE))\n    ax1.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax1.text(j, i, f'{rewards[i, j]:.0f}',\n                    ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n    \n    plt.colorbar(im1, ax=ax1, fraction=0.046)\n    \n    # 2. ç»˜åˆ¶ Q å€¼çŸ©é˜µï¼ˆæ˜¾ç¤ºæœ€å¤§ Q å€¼ï¼‰\n    ax2 = axes[1]\n    Q_max = Q.reshape(GRID_SIZE, GRID_SIZE, 4).max(axis=2)\n    im2 = ax2.imshow(Q_max, cmap=value_cmap)\n    ax2.set_title(f'Q-Value Matrix (max) - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax2.set_xticks(range(GRID_SIZE))\n    ax2.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax2.text(j, i, f'{Q_max[i, j]:.2f}',\n                    ha=\"center\", va=\"center\", \n                    color=\"white\" if Q_max[i, j] < (Q_max.max()/2 if Q_max.max() > 0 else 0.5) else \"black\",\n                    fontsize=10)\n    \n    plt.colorbar(im2, ax=ax2, fraction=0.046)\n    \n    # 3. ç»˜åˆ¶ç­–ç•¥çŸ©é˜µ\n    ax3 = axes[2]\n    policy_display = np.zeros((GRID_SIZE, GRID_SIZE))\n    im3 = ax3.imshow(policy_display, cmap='gray', vmin=0, vmax=1, alpha=0.1)\n    ax3.set_title(f'Policy Matrix - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax3.set_xticks(range(GRID_SIZE))\n    ax3.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            if policy_arrows[i, j]:\n                ax3.text(j, i, policy_arrows[i, j],\n                        ha=\"center\", va=\"center\", \n                        color=\"blue\", fontsize=24, fontweight='bold')\n    \n    # æ•´ä½“æ ‡é¢˜\n    fig.suptitle(f'MC Îµ-Greedy Algorithm: Episode {episode_num}', \n                 fontsize=16, fontweight='bold', y=0.98)\n    \n    plt.tight_layout()\n    \n    # è½¬æ¢ä¸º RGB æ•°ç»„\n    fig.canvas.draw()\n    buf = fig.canvas.buffer_rgba()\n    rgb_array = np.asarray(buf)\n    rgb_array = rgb_array[:, :, :3]  # RGBA to RGB\n    plt.close(fig)\n    \n    return rgb_array\n\nprint(\"âœ… å¯è§†åŒ–å‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸ƒæ­¥ï¼šæ‰§è¡Œ MC Îµ-è´ªå¿ƒç®—æ³•å¹¶ç”Ÿæˆ GIF åŠ¨ç”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_mc_epsilon_greedy(\n    gif_filename: Annotated[str, \"è¾“å‡º GIF æ–‡ä»¶å\"] = 'mc_epsilon_greedy.gif',\n    fps: Annotated[int, \"ç”ŸæˆåŠ¨ç”»çš„å¸§ç‡\"] = 10,\n) -> Annotated[Tuple[np.ndarray, np.ndarray], \"æœ€ç»ˆ Q å€¼å’Œç­–ç•¥çŸ©é˜µ\"]:\n    \"\"\"\n    æ‰§è¡Œ MC Îµ-è´ªå¿ƒç®—æ³•å¹¶ç”Ÿæˆ GIF åŠ¨ç”»\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"ğŸš€ MC Îµ-è´ªå¿ƒç®—æ³•å¯åŠ¨\".center(70))\n    print(\"=\" * 70)\n    \n    # åˆå§‹åŒ–\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    returns = defaultdict(list)\n    \n    frames = []\n    frame_interval = NUM_EPISODES // 100  # æ¯éš”å¤šå°‘å›åˆä¿å­˜ä¸€å¸§\n    \n    # MC Îµ-è´ªå¿ƒä¸»å¾ªç¯\n    for i in range(1, NUM_EPISODES + 1):\n        # 1. åˆ›å»º/æ›´æ–° Îµ-è´ªå¿ƒç­–ç•¥\n        policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n        \n        # 2. ç”Ÿæˆä¸€ä¸ªå›åˆ\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # 3. è®¡ç®—å›æŠ¥å¹¶æ›´æ–° Q å€¼\n        G = 0\n        visited_sa_pairs = set()\n        \n        # ä»åå‘å‰éå†å›åˆ\n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # é¦–æ¬¡è®¿é—® (First-visit) MC\n            if sa_pair not in visited_sa_pairs:\n                returns[sa_pair].append(G)\n                Q[state, action] = np.mean(returns[sa_pair])\n                visited_sa_pairs.add(sa_pair)\n        \n        # 4. å®šæœŸç”Ÿæˆå¹¶ä¿å­˜å¸§\n        if i % frame_interval == 0 or i == 1:\n            policy_arrows = get_policy_matrix(policy, GRID_SIZE)\n            print(f\"ç”Ÿæˆç¬¬ {i} å›åˆçš„å¸§...\")\n            frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, i))\n    \n    print(f\"\\n{'=' * 70}\")\n    print(\"ğŸ‰ MC Îµ-è´ªå¿ƒç®—æ³•å®Œæˆï¼\".center(70))\n    print(f\"{'=' * 70}\")\n    print(f\"âœ… æ€»è®¡å›åˆæ•°: {NUM_EPISODES}\")\n    \n    # åœ¨ç»“å°¾å¤šæ·»åŠ å‡ å¸§ä»¥ä¾¿è§‚å¯Ÿæœ€ç»ˆç»“æœ\n    policy_arrows = get_policy_matrix(create_epsilon_greedy_policy(Q, 0, num_actions), GRID_SIZE) # æœ€ç»ˆè´ªå©ªç­–ç•¥\n    for _ in range(10):\n        frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, NUM_EPISODES))\n    \n    # ä¿å­˜ GIF\n    print(f\"\\næ­£åœ¨ä¿å­˜ GIFï¼ˆå…± {len(frames)} å¸§ï¼‰...\")\n    imageio.mimsave(gif_filename, frames, fps=fps, loop=0)\n    \n    print(f\"\\nâœ… GIF åŠ¨ç”»å·²ä¿å­˜: {gif_filename}\")\n    print(f\"   - æ€»å¸§æ•°: {len(frames)}\")\n    print(f\"   - å¸§ç‡: {fps} fps\")\n    print(f\"   - æ€»å›åˆæ•°: {NUM_EPISODES}\")\n    \n    return Q, policy\n\n# æ‰§è¡Œç®—æ³•\nQ_final, policy_final = run_mc_epsilon_greedy(\n    gif_filename='mc_epsilon_greedy.gif',\n    fps=20\n)\n\n# æ˜¾ç¤º GIF\nprint(f\"\\næ­£åœ¨æ˜¾ç¤º GIF...\")\ndisplay(IPImage(filename='mc_epsilon_greedy.gif'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š æ€»ç»“ä¸åˆ†æ\n",
    "\n",
    "### ğŸ¯ ä½ å­¦åˆ°äº†ä»€ä¹ˆ\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªæ•™ç¨‹ï¼Œä½ å®ç°äº†æ›´é€šç”¨çš„ MC Îµ-è´ªå¿ƒç®—æ³•ï¼Œå¹¶å…‹æœäº† MC Basic çš„å±€é™æ€§ï¼š\n",
    "\n",
    "1. **Îµ-è´ªå¿ƒç­–ç•¥çš„é‡è¦æ€§**\n",
    "   - å®ƒæ˜¯ä¿è¯æŒç»­æ¢ç´¢çš„å…³é”®ï¼Œé¿å…äº†ç®—æ³•è¿‡æ—©é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚\n",
    "   - ä½¿å¾—ç®—æ³•ä¸å†éœ€è¦â€œæ¢ç´¢æ€§èµ·ç‚¹â€è¿™ä¸€ä¸åˆ‡å®é™…çš„å‡è®¾ã€‚\n",
    "\n",
    "2. **On-Policy å­¦ä¹ **\n",
    "   - å­¦ä¹ çš„ç­–ç•¥å’Œç”¨äºç”Ÿæˆæ•°æ®çš„ç­–ç•¥æ˜¯åŒä¸€ä¸ªã€‚\n",
    "   - ç®—æ³•åœ¨â€œè¾¹æ¢ç´¢è¾¹å­¦ä¹ â€çš„è¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–è‡ªèº«ã€‚\n",
    "\n",
    "3. **ä¸ MC Basic çš„å¯¹æ¯”**\n",
    "   - **MC Basic**ï¼šæ¦‚å¿µç®€å•ï¼Œåˆ†ä¸ºç‹¬ç«‹çš„è¯„ä¼°å’Œæ”¹è¿›ä¸¤æ­¥ï¼Œä½†ä¾èµ–æ¢ç´¢æ€§èµ·ç‚¹ã€‚\n",
    "   - **MC Îµ-è´ªå¿ƒ**ï¼šæ›´å®ç”¨ï¼Œå°†æ¢ç´¢èå…¥ç­–ç•¥ä¸­ï¼Œé€šè¿‡å¤§é‡å›åˆé€æ­¥æ”¶æ•›ã€‚\n",
    "\n",
    "### ğŸ” è§‚å¯Ÿç»“æœ\n",
    "\n",
    "ä»ç”Ÿæˆçš„ GIF åŠ¨ç”»ä¸­ï¼Œä½ å¯ä»¥è§‚å¯Ÿåˆ°ï¼š\n",
    "\n",
    "- **Q å€¼çš„æŒç»­æ›´æ–°**ï¼šQ å€¼éšç€æ¯ä¸ªå›åˆçš„ç»éªŒè€Œä¸æ–­æ³¢åŠ¨å’Œä¼˜åŒ–ã€‚\n",
    "- **æ¢ç´¢çš„å½±å“**ï¼šå³ä½¿åœ¨å­¦ä¹ åæœŸï¼Œæ™ºèƒ½ä½“å¶å°”ä¹Ÿä¼šé€‰æ‹©éæœ€ä¼˜è·¯å¾„ï¼Œè¿™å°±æ˜¯ Îµ-è´ªå¿ƒç­–ç•¥åœ¨èµ·ä½œç”¨ã€‚\n",
    "- **ç­–ç•¥çš„é€æ¸æ”¶æ•›**ï¼šå°½ç®¡æœ‰æ¢ç´¢ï¼Œä½†æœ€ä¼˜åŠ¨ä½œçš„æ¦‚ç‡å§‹ç»ˆæ˜¯æœ€é«˜çš„ï¼Œå› æ­¤ç­–ç•¥çš„ä¸»ä½“æ–¹å‘ä¼šé€æ¸ç¨³å®šå¹¶æŒ‡å‘ç›®æ ‡ã€‚\n",
    "\n",
    "### ğŸ“ ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "MC æ–¹æ³•éœ€è¦ç­‰å¾…ä¸€ä¸ªå®Œæ•´çš„å›åˆç»“æŸåæ‰èƒ½è¿›è¡Œå­¦ä¹ ï¼Œè¿™åœ¨æŸäº›ä»»åŠ¡ä¸­æ•ˆç‡è¾ƒä½ã€‚ä¸‹ä¸€æ­¥ï¼Œä½ å°†å­¦ä¹ æ›´é«˜æ•ˆçš„æ— æ¨¡å‹æ–¹æ³•ï¼š\n",
    "\n",
    "1. **æ—¶åºå·®åˆ†å­¦ä¹  (Temporal-Difference, TD)**\n",
    "   - æ— éœ€ç­‰å¾…å›åˆç»“æŸï¼Œæ¯èµ°ä¸€æ­¥å°±å¯ä»¥å­¦ä¹ ã€‚\n",
    "   - ç»“åˆäº†è’™ç‰¹å¡æ´›ï¼ˆä»ç»éªŒä¸­å­¦ä¹ ï¼‰å’ŒåŠ¨æ€è§„åˆ’ï¼ˆè‡ªä¸¾ï¼‰çš„ä¼˜ç‚¹ã€‚\n",
    "   - æ ¸å¿ƒç®—æ³•ï¼š**SARSA (On-policy)** å’Œ **Q-learning (Off-policy)**ã€‚\n",
    "\n",
    "2. **å‡½æ•°é€¼è¿‘**\n",
    "   - å½“çŠ¶æ€ç©ºé—´è¿‡å¤§æ—¶ï¼Œç”¨è¡¨æ ¼å­˜å‚¨ Q å€¼ä¸å†å¯è¡Œã€‚\n",
    "   - ä½¿ç”¨ç¥ç»ç½‘ç»œç­‰æ¨¡å‹æ¥è¿‘ä¼¼ Q å‡½æ•°ï¼Œå³æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¼€ç«¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œä½ æŒæ¡äº† MC Îµ-è´ªå¿ƒç®—æ³•ï¼Œå‘æ›´é«˜çº§çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯åˆè¿ˆè¿›äº†ä¸€å¤§æ­¥ï¼** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}