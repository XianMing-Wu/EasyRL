{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Îµ-è´ªå¿ƒç®—æ³•æ•™ç¨‹ï¼šæ›´å®ç”¨çš„è’™ç‰¹å¡æ´›æ–¹æ³•\n",
    "\n",
    "## ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦ MC Îµ-è´ªå¿ƒï¼Ÿ\n",
    "\n",
    "åœ¨ `MC Basic` ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è’™ç‰¹å¡æ´›æ–¹æ³•çš„åŸºæœ¬åŸç†ã€‚ä½†å®ƒæœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ï¼š**æ¢ç´¢æ€§èµ·ç‚¹ (Exploring Starts)**ã€‚è¿™æ„å‘³ç€ä¸ºäº†è¯„ä¼°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œæˆ‘ä»¬éœ€è¦ä»æ¯ä¸€ä¸ªå¯èƒ½çš„ (s,a) å¯¹å¼€å§‹ç”Ÿæˆå›åˆã€‚\n",
    "\n",
    "åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè¿™ä¸ªå‡è®¾å¾€å¾€éš¾ä»¥æ»¡è¶³ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•éšæ„è®¾ç½®æœºå™¨äººçš„åˆå§‹çŠ¶æ€å’Œåˆå§‹åŠ¨ä½œã€‚\n",
    "\n",
    "**MC Îµ-è´ªå¿ƒç®—æ³•**é€šè¿‡å¼•å…¥â€œæ¢ç´¢â€æœºåˆ¶è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½¿å…¶æ›´åŠ å®ç”¨ï¼š\n",
    "\n",
    "- **æ— éœ€æ¢ç´¢æ€§èµ·ç‚¹**ï¼šé€šè¿‡ Îµ-è´ªå¿ƒç­–ç•¥åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¿›è¡Œæ¢ç´¢ã€‚\n",
    "- **å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨**ï¼šä»¥ 1-Îµ çš„æ¦‚ç‡é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ï¼Œä»¥ Îµ çš„æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ã€‚\n",
    "- **åœ¨çº¿ç­–ç•¥ (On-policy)**ï¼šç”¨äºç”Ÿæˆæ•°æ®çš„ç­–ç•¥å’Œè¢«è¯„ä¼°æ”¹è¿›çš„ç­–ç•¥æ˜¯åŒä¸€ä¸ªã€‚\n",
    "- **ä¿è¯æ”¶æ•›**ï¼šåœ¨ä¸€å®šæ¡ä»¶ä¸‹ï¼Œå®ƒèƒ½æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚\n",
    "\n",
    "## ğŸ¯ æœ¬æ•™ç¨‹ç›®æ ‡\n",
    "\n",
    "æˆ‘ä»¬å°†ä»é›¶å¼€å§‹å®ç° MC Îµ-è´ªå¿ƒç®—æ³•ï¼Œå¹¶åœ¨åŒä¸€ä¸ª 4Ã—4 ç½‘æ ¼ä¸–ç•Œä¸­è¿›è¡Œå¯è§†åŒ–ï¼š\n",
    "\n",
    "- **æ ¸å¿ƒå†…å®¹**ï¼š\n",
    "  - ç†è§£ Îµ-è´ªå¿ƒç­–ç•¥å¦‚ä½•å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚\n",
    "  - å®ç°åŸºäºå›åˆé‡‡æ ·çš„ on-policy è’™ç‰¹å¡æ´›æ§åˆ¶ã€‚\n",
    "  - å¯è§†åŒ– Q å€¼å’Œç­–ç•¥åœ¨æ¢ç´¢ä¸­çš„æ”¶æ•›è¿‡ç¨‹ã€‚\n",
    "\n",
    "- **å­¦ä¹ è·¯å¾„**ï¼š\n",
    "  1. ç†è§£ Îµ-è´ªå¿ƒç­–ç•¥ã€‚\n",
    "  2. æ„å»ºæ”¯æŒéšæœºèµ·å§‹å›åˆçš„ç¯å¢ƒã€‚\n",
    "  3. å®ç° MC Îµ-è´ªå¿ƒæ ¸å¿ƒç®—æ³•ã€‚\n",
    "  4. å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹ï¼ˆGIF åŠ¨ç”»ï¼‰ã€‚\n",
    "  5. åˆ†æä¸ MC Basic çš„å¼‚åŒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "è®©æˆ‘ä»¬å¼€å§‹æ¢ç´¢è¿™ç§æ›´å¼ºå¤§çš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€æ­¥ï¼šå®‰è£…å’Œå¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom typing import Optional, Tuple, Dict, Any, List, Annotated\nfrom collections import defaultdict\n\n# è®¾ç½® matplotlib åç«¯ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ pyplot ä¹‹å‰ï¼‰\nimport matplotlib\nmatplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆäº‘ç«¯ç¯å¢ƒ\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom IPython.display import display, clear_output, Image as IPImage\nimport imageio\nfrom io import BytesIO\n\n# é…ç½® matplotlib ä¸­æ–‡æ˜¾ç¤º\nplt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(f\"âœ… Gymnasium ç‰ˆæœ¬: {gym.__version__}\")\nprint(f\"âœ… NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"âœ… Matplotlib ç‰ˆæœ¬: {matplotlib.__version__}\")\nprint(f\"âœ… Matplotlib åç«¯: {matplotlib.get_backend()}\")\nprint(f\"âœ… ä¸­æ–‡å­—ä½“é…ç½®å®Œæˆ\")\nprint(f\"âœ… imageio å·²å¯¼å…¥ï¼Œæ”¯æŒ GIF åŠ¨ç”»ç”Ÿæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ”¯æŒ MC å­¦ä¹ çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\n",
    "\n",
    "ä¸ºäº†é€‚åº” Îµ-è´ªå¿ƒç®—æ³•ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç¯å¢ƒåšä¸€äº›å¾®è°ƒã€‚ä¸»è¦åŒºåˆ«åœ¨äºå›åˆçš„ç”Ÿæˆæ–¹å¼ï¼š\n",
    "\n",
    "- **éšæœºèµ·ç‚¹**ï¼šå›åˆå¯ä»¥ä»ä»»æ„éç»ˆæ­¢çŠ¶æ€éšæœºå¼€å§‹ã€‚\n",
    "- **ç­–ç•¥é©±åŠ¨**ï¼šæ•´ä¸ªå›åˆéƒ½ç”±ä¸€ä¸ªç»™å®šçš„ç­–ç•¥ï¼ˆå³ Îµ-è´ªå¿ƒç­–ç•¥ï¼‰é©±åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GridWorldEpsilonGreedy(gym.Env):\n    \"\"\"æ”¯æŒ MC Îµ-è´ªå¿ƒå­¦ä¹ çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\"\"\"\n    \n    def __init__(\n        self,\n        size: Annotated[int, \"ç½‘æ ¼çš„è¾¹é•¿\"] = 4,\n        rewards: Annotated[Optional[np.ndarray], \"å¯é€‰çš„å¥–åŠ±çŸ©é˜µ\"] = None,\n    ):\n        super().__init__()\n        self.size = size\n        \n        # è®¾ç½®å¥–åŠ±çŸ©é˜µ\n        if rewards is None:\n            self.rewards = np.array([\n                [0, 0, -1, -1],\n                [0, -1, -1, 1],\n                [0, -1, 0, 0],\n                [-1, 0, 0, -1]\n            ])\n        else:\n            self.rewards = rewards\n        \n        # ç›®æ ‡ä½ç½®å’Œéšœç¢ç‰©ä½ç½®\n        self.target_pos = np.argwhere(self.rewards == 1)[0]\n        self.obstacle_mask = (self.rewards == -1)\n        \n        # åŠ¨ä½œç©ºé—´ï¼š0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Discrete(size * size)\n        \n        # åŠ¨ä½œæ˜ å°„\n        self._action_to_direction = {\n            0: np.array([-1, 0]),  # ä¸Š\n            1: np.array([1, 0]),   # ä¸‹\n            2: np.array([0, -1]),  # å·¦\n            3: np.array([0, 1]),   # å³\n        }\n        \n        self._agent_location = np.array([0, 0])\n    \n    def _pos_to_state(\n        self,\n        pos: Annotated[np.ndarray, \"äºŒç»´ä½ç½®åæ ‡\"],\n    ) -> Annotated[int, \"å¯¹åº”çš„ä¸€ç»´çŠ¶æ€ç´¢å¼•\"]:\n        \"\"\"å°†äºŒç»´ä½ç½®è½¬æ¢ä¸ºä¸€ç»´çŠ¶æ€ç´¢å¼•\"\"\"\n        return pos[0] * self.size + pos[1]\n    \n    def _state_to_pos(\n        self,\n        state: Annotated[int, \"ä¸€ç»´çŠ¶æ€ç´¢å¼•\"],\n    ) -> Annotated[np.ndarray, \"å¯¹åº”çš„äºŒç»´ä½ç½®åæ ‡\"]:\n        \"\"\"å°†ä¸€ç»´çŠ¶æ€ç´¢å¼•è½¬æ¢ä¸ºäºŒç»´ä½ç½®\"\"\"\n        return np.array([state // self.size, state % self.size])\n    \n    def reset(\n        self,\n        start_pos: Annotated[Optional[np.ndarray], \"å¯é€‰çš„èµ·å§‹ä½ç½®åæ ‡\"] = None,\n    ) -> Annotated[Tuple[int, Dict[str, Any]], \"åˆå§‹çŠ¶æ€ç´¢å¼•åŠé™„åŠ ä¿¡æ¯å­—å…¸\"]:\n        \"\"\"é‡ç½®ç¯å¢ƒï¼Œå¯ä»¥æŒ‡å®šæˆ–éšæœºé€‰æ‹©èµ·å§‹ä½ç½®\"\"\"\n        if start_pos is not None:\n            self._agent_location = np.array(start_pos)\n        else:\n            # éšæœºé€‰æ‹©ä¸€ä¸ªéç»ˆæ­¢ã€ééšœç¢çš„èµ·å§‹ä½ç½®\n            while True:\n                pos = np.random.randint(0, self.size, size=2)\n                if not np.array_equal(pos, self.target_pos) and not self.obstacle_mask[pos[0], pos[1]]:\n                    self._agent_location = pos\n                    break\n        \n        state = self._pos_to_state(self._agent_location)\n        return state, {}\n    \n    def step(\n        self,\n        action: Annotated[int, \"è¦æ‰§è¡Œçš„åŠ¨ä½œç¼–å·\"],\n    ) -> Annotated[Tuple[int, float, bool, bool, Dict[str, Any]], \"æ–°çŠ¶æ€ã€å¥–åŠ±ã€ç»ˆæ­¢æ ‡å¿—ã€æˆªæ–­æ ‡å¿—åŠé¢å¤–ä¿¡æ¯\"]:\n        \"\"\"æ‰§è¡ŒåŠ¨ä½œ\"\"\"\n        direction = self._action_to_direction[action]\n        new_location = self._agent_location + direction\n        \n        if (0 <= new_location[0] < self.size and 0 <= new_location[1] < self.size):\n            self._agent_location = new_location\n        \n        reward = self.rewards[self._agent_location[0], self._agent_location[1]]\n        terminated = np.array_equal(self._agent_location, self.target_pos)\n        state = self._pos_to_state(self._agent_location)\n        return state, reward, terminated, False, {}\n    \n    def generate_episode(\n        self,\n        policy: Annotated[np.ndarray, \"ç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"],\n        max_steps: Annotated[int, \"å•ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°\"] = 100,\n    ) -> Annotated[List[Tuple[int, int, float]], \"çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±åºåˆ—\"]:\n        \"\"\"ä»éšæœºçŠ¶æ€å¼€å§‹ï¼Œæ ¹æ®ç­–ç•¥ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›åˆ\"\"\"\n        state, _ = self.reset() # éšæœºèµ·ç‚¹\n        episode = []\n        \n        for _ in range(max_steps):\n            action = np.random.choice(self.action_space.n, p=policy[state])\n            next_state, reward, terminated, _, _ = self.step(action)\n            episode.append((state, action, reward))\n            if terminated:\n                break\n            state = next_state\n        \n        return episode\n\n# åˆ›å»ºç¯å¢ƒå®ä¾‹\nenv = GridWorldEpsilonGreedy(size=4)\nprint(f\"âœ… GridWorldEpsilonGreedy ç¯å¢ƒåˆ›å»ºå®Œæˆ\")\nprint(f\"   - ç½‘æ ¼å¤§å°: {env.size}Ã—{env.size}\")\nprint(f\"   - åŠ¨ä½œç©ºé—´: {env.action_space} (0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³)\")\nprint(f\"   - ç›®æ ‡ä½ç½®: {env.target_pos}\")\nprint(f\"\\n   å¥–åŠ±çŸ©é˜µ:\")\nprint(env.rewards)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰æ­¥ï¼šç†è§£ MC Îµ-è´ªå¿ƒç®—æ³•\n",
    "\n",
    "### ğŸ§® Îµ-è´ªå¿ƒç­–ç•¥\n",
    "\n",
    "ä¸ºäº†ç¡®ä¿åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æœ‰è¶³å¤Ÿçš„æ¢ç´¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ **Îµ-è´ªå¿ƒç­–ç•¥**ã€‚å¯¹äºä»»æ„çŠ¶æ€ $s$ï¼Œç­–ç•¥ $\\pi$ å®šä¹‰å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \n",
    "\\begin{cases} \n",
    "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \n",
    "\\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a \\neq \\arg\\max_{a'} Q(s,a') \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $1 - \\epsilon$ æ˜¯â€œåˆ©ç”¨â€çš„æ¦‚ç‡ï¼Œå³é€‰æ‹©å½“å‰ä¼°è®¡çš„æœ€å¥½åŠ¨ä½œã€‚\n",
    "- $\\epsilon$ æ˜¯â€œæ¢ç´¢â€çš„æ¦‚ç‡ï¼Œå³éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚\n",
    "- $|\\mathcal{A}(s)|$ æ˜¯çŠ¶æ€ $s$ ä¸‹çš„åŠ¨ä½œæ•°é‡ã€‚\n",
    "\n",
    "### ğŸ”„ On-Policy MC Control ç®—æ³•æµç¨‹\n",
    "\n",
    "**åˆå§‹åŒ–**ï¼š\n",
    "- éšæœºåˆå§‹åŒ– $Q(s,a)$ã€‚\n",
    "- åˆå§‹åŒ– `returns(s,a)` ä¸ºç©ºåˆ—è¡¨ã€‚\n",
    "- æ ¹æ®åˆå§‹ Q å€¼åˆ›å»ºä¸€ä¸ª Îµ-è´ªå¿ƒç­–ç•¥ $\\pi$ã€‚\n",
    "\n",
    "**å¾ªç¯ï¼ˆç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§å›åˆæ•°ï¼‰ï¼š**\n",
    "\n",
    "1. **ç”Ÿæˆå›åˆ**ï¼šä½¿ç”¨å½“å‰ç­–ç•¥ $\\pi$ ä»ä¸€ä¸ªéšæœºèµ·ç‚¹ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›åˆã€‚\n",
    "   - `(S_0, A_0, R_1), (S_1, A_1, R_2), ..., (S_{T-1}, A_{T-1}, R_T)`\n",
    "\n",
    "2. **è®¡ç®—å›æŠ¥**ï¼šå¯¹äºå›åˆä¸­æ¯ä¸ªæ—¶é—´æ­¥ $t=0, 1, ..., T-1$ï¼š\n",
    "   - è®¡ç®—ä»è¯¥æ­¥å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥ $G_t = R_{t+1} + \\gamma R_{t+2} + ...$\n",
    "\n",
    "3. **æ›´æ–° Q å€¼**ï¼šå¯¹äºå›åˆä¸­å‡ºç°çš„æ¯ä¸€ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ $(S_t, A_t)$ï¼š\n",
    "   - å°†å›æŠ¥ $G_t$ æ·»åŠ åˆ° `returns(S_t, A_t)` åˆ—è¡¨ä¸­ã€‚\n",
    "   - æ›´æ–° $Q(S_t, A_t)$ ä¸º `returns(S_t, A_t)` çš„å¹³å‡å€¼ã€‚\n",
    "\n",
    "4. **æ›´æ–°ç­–ç•¥**ï¼šæ ¹æ®æ›´æ–°åçš„ Q å€¼ï¼Œæ”¹è¿› Îµ-è´ªå¿ƒç­–ç•¥ $\\pi$ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››æ­¥ï¼šå®šä¹‰ç®—æ³•å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç®—æ³•å‚æ•°é…ç½®å®Œæˆ\n",
      "   - ç½‘æ ¼å¤§å°: 4Ã—4\n",
      "   - æŠ˜æ‰£å› å­ Î³: 0.9\n",
      "   - Epsilon Îµ: 0.1\n",
      "   - æ€»å›åˆæ•°: 5000\n",
      "   - åŠ¨ä½œç©ºé—´: ['â†‘', 'â†“', 'â†', 'â†’']\n"
     ]
    }
   ],
   "source": [
    "# --- ç®—æ³•å‚æ•° ---\n",
    "GRID_SIZE = 4           # ç½‘æ ¼å¤§å°\n",
    "GAMMA = 0.9             # æŠ˜æ‰£å› å­\n",
    "EPSILON = 0.1           # Îµ-è´ªå¿ƒç­–ç•¥ä¸­çš„ Îµ å€¼\n",
    "NUM_EPISODES = 5000     # æ€»å…±è¦è¿è¡Œçš„å›åˆæ•°\n",
    "MAX_STEPS = 100         # æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°\n",
    "\n",
    "# åŠ¨ä½œåç§°ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰\n",
    "ACTION_NAMES = ['â†‘', 'â†“', 'â†', 'â†’']\n",
    "\n",
    "print(f\"âœ… ç®—æ³•å‚æ•°é…ç½®å®Œæˆ\")\n",
    "print(f\"   - ç½‘æ ¼å¤§å°: {GRID_SIZE}Ã—{GRID_SIZE}\")\n",
    "print(f\"   - æŠ˜æ‰£å› å­ Î³: {GAMMA}\")\n",
    "print(f\"   - Epsilon Îµ: {EPSILON}\")\n",
    "print(f\"   - æ€»å›åˆæ•°: {NUM_EPISODES}\")\n",
    "print(f\"   - åŠ¨ä½œç©ºé—´: {ACTION_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”æ­¥ï¼šå®ç° MC Îµ-è´ªå¿ƒæ ¸å¿ƒç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_epsilon_greedy_policy(\n    Q: Annotated[np.ndarray, \"çŠ¶æ€-åŠ¨ä½œä»·å€¼è¡¨\"],\n    epsilon: Annotated[float, \"Îµ-è´ªå¿ƒç­–ç•¥ä¸­çš„æ¢ç´¢æ¦‚ç‡\"],\n    num_actions: Annotated[int, \"åŠ¨ä½œç©ºé—´çš„å¤§å°\"],\n) -> Annotated[np.ndarray, \"Îµ-è´ªå¿ƒç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"]:\n    \"\"\"æ ¹æ® Q å€¼åˆ›å»ºä¸€ä¸ª Îµ-è´ªå¿ƒç­–ç•¥\"\"\"\n    num_states = Q.shape[0]\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])\n        policy[state, best_action] += (1.0 - epsilon)\n    \n    return policy\n\ndef get_policy_matrix(\n    policy: Annotated[np.ndarray, \"ç­–ç•¥æ¦‚ç‡çŸ©é˜µ\"],\n    grid_size: Annotated[int, \"ç½‘æ ¼çš„è¾¹é•¿\"],\n) -> Annotated[np.ndarray, \"ç”¨äºå¯è§†åŒ–çš„ç­–ç•¥ç®­å¤´çŸ©é˜µ\"]:\n    \"\"\"å°†ç­–ç•¥è½¬æ¢ä¸ºç®­å¤´çŸ©é˜µç”¨äºå¯è§†åŒ–\"\"\"\n    policy_arrows = np.empty((grid_size, grid_size), dtype=object)\n    \n    for state in range(grid_size * grid_size):\n        row = state // grid_size\n        col = state % grid_size\n        best_action = np.argmax(policy[state])\n        policy_arrows[row, col] = ACTION_NAMES[best_action]\n    \n    return policy_arrows\n\nprint(\"âœ… MC Îµ-è´ªå¿ƒæ ¸å¿ƒå‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­æ­¥ï¼šå®ç°å¯è§†åŒ–å‡½æ•°\n",
    "\n",
    "å¯è§†åŒ–å‡½æ•°ä¸ `MC Basic` åŸºæœ¬ç›¸åŒï¼Œä½†æ ‡é¢˜å’Œè¿­ä»£è®¡æ•°æ–¹å¼æœ‰æ‰€è°ƒæ•´ï¼Œä»¥åæ˜ æ˜¯åŸºäºå›åˆæ•°è¿›è¡Œæ›´æ–°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def render_mc_epsilon_greedy_to_array(\n    rewards: Annotated[np.ndarray, \"å¥–åŠ±çŸ©é˜µ\"],\n    Q: Annotated[np.ndarray, \"çŠ¶æ€-åŠ¨ä½œä»·å€¼è¡¨\"],\n    policy_arrows: Annotated[np.ndarray, \"ç­–ç•¥ç®­å¤´çŸ©é˜µ\"],\n    episode_num: Annotated[int, \"å½“å‰å›åˆæ•°\"],\n) -> Annotated[np.ndarray, \"ç”¨äºç”ŸæˆåŠ¨ç”»çš„ RGB æ•°ç»„\"]:\n    \"\"\"\n    å°†å½“å‰ MC Îµ-è´ªå¿ƒçŠ¶æ€æ¸²æŸ“ä¸º RGB æ•°ç»„ï¼ˆç”¨äº GIF ç”Ÿæˆï¼‰\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=80)\n    \n    # é…è‰²æ–¹æ¡ˆ\n    reward_cmap = LinearSegmentedColormap.from_list('reward', ['red', 'white', 'green'])\n    value_cmap = 'viridis'\n    \n    # 1. ç»˜åˆ¶å¥–åŠ±çŸ©é˜µ\n    ax1 = axes[0]\n    im1 = ax1.imshow(rewards, cmap=reward_cmap, vmin=-1, vmax=1)\n    ax1.set_title(f'Reward Matrix', fontsize=14, fontweight='bold')\n    ax1.set_xticks(range(GRID_SIZE))\n    ax1.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax1.text(j, i, f'{rewards[i, j]:.0f}',\n                    ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n    \n    plt.colorbar(im1, ax=ax1, fraction=0.046)\n    \n    # 2. ç»˜åˆ¶ Q å€¼çŸ©é˜µï¼ˆæ˜¾ç¤ºæœ€å¤§ Q å€¼ï¼‰\n    ax2 = axes[1]\n    Q_max = Q.reshape(GRID_SIZE, GRID_SIZE, 4).max(axis=2)\n    im2 = ax2.imshow(Q_max, cmap=value_cmap)\n    ax2.set_title(f'Q-Value Matrix (max) - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax2.set_xticks(range(GRID_SIZE))\n    ax2.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax2.text(j, i, f'{Q_max[i, j]:.2f}',\n                    ha=\"center\", va=\"center\", \n                    color=\"white\" if Q_max[i, j] < (Q_max.max()/2 if Q_max.max() > 0 else 0.5) else \"black\",\n                    fontsize=10)\n    \n    plt.colorbar(im2, ax=ax2, fraction=0.046)\n    \n    # 3. ç»˜åˆ¶ç­–ç•¥çŸ©é˜µ\n    ax3 = axes[2]\n    policy_display = np.zeros((GRID_SIZE, GRID_SIZE))\n    im3 = ax3.imshow(policy_display, cmap='gray', vmin=0, vmax=1, alpha=0.1)\n    ax3.set_title(f'Policy Matrix - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax3.set_xticks(range(GRID_SIZE))\n    ax3.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            if policy_arrows[i, j]:\n                ax3.text(j, i, policy_arrows[i, j],\n                        ha=\"center\", va=\"center\", \n                        color=\"blue\", fontsize=24, fontweight='bold')\n    \n    # æ•´ä½“æ ‡é¢˜\n    fig.suptitle(f'MC Îµ-Greedy Algorithm: Episode {episode_num}', \n                 fontsize=16, fontweight='bold', y=0.98)\n    \n    plt.tight_layout()\n    \n    # è½¬æ¢ä¸º RGB æ•°ç»„\n    fig.canvas.draw()\n    buf = fig.canvas.buffer_rgba()\n    rgb_array = np.asarray(buf)\n    rgb_array = rgb_array[:, :, :3]  # RGBA to RGB\n    plt.close(fig)\n    \n    return rgb_array\n\nprint(\"âœ… å¯è§†åŒ–å‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸ƒæ­¥ï¼šæ‰§è¡Œ MC Îµ-è´ªå¿ƒç®—æ³•å¹¶ç”Ÿæˆ GIF åŠ¨ç”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_mc_epsilon_greedy(\n    gif_filename: Annotated[str, \"è¾“å‡º GIF æ–‡ä»¶å\"] = 'mc_epsilon_greedy.gif',\n    fps: Annotated[int, \"ç”ŸæˆåŠ¨ç”»çš„å¸§ç‡\"] = 10,\n) -> Annotated[Tuple[np.ndarray, np.ndarray], \"æœ€ç»ˆ Q å€¼å’Œç­–ç•¥çŸ©é˜µ\"]:\n    \"\"\"\n    æ‰§è¡Œ MC Îµ-è´ªå¿ƒç®—æ³•å¹¶ç”Ÿæˆ GIF åŠ¨ç”»\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"ğŸš€ MC Îµ-è´ªå¿ƒç®—æ³•å¯åŠ¨\".center(70))\n    print(\"=\" * 70)\n    \n    # åˆå§‹åŒ–\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    returns = defaultdict(list)\n    \n    frames = []\n    frame_interval = NUM_EPISODES // 100  # æ¯éš”å¤šå°‘å›åˆä¿å­˜ä¸€å¸§\n    \n    # MC Îµ-è´ªå¿ƒä¸»å¾ªç¯\n    for i in range(1, NUM_EPISODES + 1):\n        # 1. åˆ›å»º/æ›´æ–° Îµ-è´ªå¿ƒç­–ç•¥\n        policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n        \n        # 2. ç”Ÿæˆä¸€ä¸ªå›åˆ\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # 3. è®¡ç®—å›æŠ¥å¹¶æ›´æ–° Q å€¼\n        G = 0\n        visited_sa_pairs = set()\n        \n        # ä»åå‘å‰éå†å›åˆ\n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # é¦–æ¬¡è®¿é—® (First-visit) MC\n            if sa_pair not in visited_sa_pairs:\n                returns[sa_pair].append(G)\n                Q[state, action] = np.mean(returns[sa_pair])\n                visited_sa_pairs.add(sa_pair)\n        \n        # 4. å®šæœŸç”Ÿæˆå¹¶ä¿å­˜å¸§\n        if i % frame_interval == 0 or i == 1:\n            policy_arrows = get_policy_matrix(policy, GRID_SIZE)\n            print(f\"ç”Ÿæˆç¬¬ {i} å›åˆçš„å¸§...\")\n            frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, i))\n    \n    print(f\"\\n{'=' * 70}\")\n    print(\"ğŸ‰ MC Îµ-è´ªå¿ƒç®—æ³•å®Œæˆï¼\".center(70))\n    print(f\"{'=' * 70}\")\n    print(f\"âœ… æ€»è®¡å›åˆæ•°: {NUM_EPISODES}\")\n    \n    # åœ¨ç»“å°¾å¤šæ·»åŠ å‡ å¸§ä»¥ä¾¿è§‚å¯Ÿæœ€ç»ˆç»“æœ\n    policy_arrows = get_policy_matrix(create_epsilon_greedy_policy(Q, 0, num_actions), GRID_SIZE) # æœ€ç»ˆè´ªå©ªç­–ç•¥\n    for _ in range(10):\n        frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, NUM_EPISODES))\n    \n    # ä¿å­˜ GIF\n    print(f\"\\næ­£åœ¨ä¿å­˜ GIFï¼ˆå…± {len(frames)} å¸§ï¼‰...\")\n    imageio.mimsave(gif_filename, frames, fps=fps, loop=0)\n    \n    print(f\"\\nâœ… GIF åŠ¨ç”»å·²ä¿å­˜: {gif_filename}\")\n    print(f\"   - æ€»å¸§æ•°: {len(frames)}\")\n    print(f\"   - å¸§ç‡: {fps} fps\")\n    print(f\"   - æ€»å›åˆæ•°: {NUM_EPISODES}\")\n    \n    return Q, policy\n\n# æ‰§è¡Œç®—æ³•\nQ_final, policy_final = run_mc_epsilon_greedy(\n    gif_filename='mc_epsilon_greedy.gif',\n    fps=20\n)\n\n# æ˜¾ç¤º GIF\nprint(f\"\\næ­£åœ¨æ˜¾ç¤º GIF...\")\ndisplay(IPImage(filename='mc_epsilon_greedy.gif'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ“ ç®—æ³•ä¼ªä»£ç ä¸å®ç°å¯¹ç…§\n\n## ğŸ” ç†è®ºç®—æ³•ï¼ˆå›¾19ï¼šåŸºäºÎµ-è´ªå¿ƒæ¢ç´¢çš„MCæ–¹æ³•ï¼‰\n\nä¸‹é¢çš„ä¼ªä»£ç å±•ç¤ºäº†æ•™æä¸­æè¿°çš„ç®—æ³•ç»“æ„ï¼š\n\n```\nç®—æ³•ï¼šOn-policy MC Control (åŸºäºÎµ-è´ªå¿ƒç­–ç•¥)\n\nå‚æ•°ï¼š\n  - Îµ: æ¢ç´¢æ¦‚ç‡ï¼ˆå°çš„æ­£æ•°ï¼‰\n\nåˆå§‹åŒ–ï¼š\n  - å¯¹æ‰€æœ‰ s âˆˆ S, a âˆˆ A(s)ï¼š\n    * Q(s,a) â† ä»»æ„å€¼\n    * Returns(s,a) â† ç©ºåˆ—è¡¨\n  - Ï€ â† ç›¸å¯¹äºQçš„Îµ-è´ªå¿ƒç­–ç•¥\n    \nå¾ªç¯ï¼ˆå¯¹æ¯ä¸ªå›åˆï¼‰ï¼š\n  (a) ä½¿ç”¨ç­–ç•¥Ï€ç”Ÿæˆä¸€ä¸ªå›åˆï¼šSâ‚€,Aâ‚€,Râ‚,Sâ‚,Aâ‚,Râ‚‚,...,S_T-1,A_T-1,R_T\n  (b) G â† 0\n  (c) å¾ªç¯ï¼ˆå¯¹å›åˆä¸­æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä»T-1åˆ°0ï¼‰ï¼š\n      * G â† Î³G + R_{t+1}ã€æŠ˜æ‰£å›æŠ¥é€’æ¨è®¡ç®—ã€‘\n      * é™¤é (S_t, A_t) å‡ºç°åœ¨ Sâ‚€,Aâ‚€,...,S_{t-1},A_{t-1} ä¸­ï¼šã€First-visitæ£€æŸ¥ã€‘\n        - å°† G è¿½åŠ åˆ° Returns(S_t, A_t)\n        - Q(S_t, A_t) â† average(Returns(S_t, A_t))\n        - A* â† argmax_a Q(S_t, a)ã€æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‘\n        - å¯¹æ‰€æœ‰ a âˆˆ A(S_t)ï¼šã€æ›´æ–°Îµ-è´ªå¿ƒç­–ç•¥ã€‘\n          å¦‚æœ a = A*:\n            Ï€(a|S_t) â† 1 - Îµ + Îµ/|A(S_t)|\n          å¦åˆ™:\n            Ï€(a|S_t) â† Îµ/|A(S_t)|\n```\n\n---\n\n## ğŸ’» æœ¬ç¬”è®°çš„å®ç°å¯¹ç…§\n\n### ğŸ“¦ **åˆå§‹åŒ–éƒ¨åˆ†** â†’ `run_mc_epsilon_greedy()` å‡½æ•°å¼€å¤´\n\n**ä¼ªä»£ç **ï¼š\n```\nQ(s,a) â† ä»»æ„å€¼\nReturns(s,a) â† ç©ºåˆ—è¡¨\nÏ€ â† ç›¸å¯¹äºQçš„Îµ-è´ªå¿ƒç­–ç•¥\n```\n\n**å®ç°ä»£ç **ï¼š\n```python\n# run_mc_epsilon_greedy() å‡½æ•°ä¸­ï¼š\nQ = np.zeros((num_states, num_actions))    # Q(s,a)åˆå§‹åŒ–ä¸º0\nreturns = defaultdict(list)                # Returns(s,a)ç©ºåˆ—è¡¨ï¼ˆåŠ¨æ€åˆ›å»ºï¼‰\n# Ï€ç­–ç•¥åœ¨æ¯ä¸ªå›åˆå¼€å§‹æ—¶åˆ›å»º\n```\n\n---\n\n### ğŸ”„ **ä¸»å¾ªç¯** â†’ `run_mc_epsilon_greedy()` ä¸­çš„ for å¾ªç¯\n\n**ä¼ªä»£ç **ï¼š\n```\nå¾ªç¯ï¼ˆå¯¹æ¯ä¸ªå›åˆï¼‰\n```\n\n**å®ç°ä»£ç **ï¼š\n```python\nfor i in range(1, NUM_EPISODES + 1):\n    # æ­¥éª¤(a): ç”Ÿæˆå›åˆ\n    policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n    episode = env.generate_episode(policy, MAX_STEPS)\n    \n    # æ­¥éª¤(b-c): æ›´æ–°Qå€¼å’Œç­–ç•¥\n    # ...ï¼ˆè§ä¸‹æ–‡è¯¦ç»†å¯¹åº”ï¼‰\n```\n\n> ğŸ’¡ **å…³é”®åŒºåˆ«**ï¼šä¸MC Basicä¸åŒï¼Œè¿™é‡Œ**æ¯ä¸ªå›åˆ**éƒ½æ›´æ–°Qå€¼ï¼Œè€Œä¸æ˜¯è¿­ä»£æ‰€æœ‰(s,a)å¯¹ã€‚\n\n---\n\n### ğŸ¯ **æ­¥éª¤(a)ï¼šç”Ÿæˆå›åˆ** â†’ `create_epsilon_greedy_policy()` + `env.generate_episode()`\n\n**ä¼ªä»£ç **ï¼š\n```\nä½¿ç”¨ç­–ç•¥Ï€ç”Ÿæˆä¸€ä¸ªå›åˆï¼šSâ‚€,Aâ‚€,Râ‚,...,S_T\n```\n\n**å®ç°ä»£ç **ï¼š\n\n#### ç¬¬1æ­¥ï¼šåˆ›å»ºÎµ-è´ªå¿ƒç­–ç•¥\n\n```python\ndef create_epsilon_greedy_policy(Q, epsilon, num_actions):\n    num_states = Q.shape[0]\n    # åˆå§‹åŒ–æ‰€æœ‰åŠ¨ä½œçš„åŸºç¡€æ¦‚ç‡ä¸º Îµ/|A|\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])       # A* = argmax_a Q(s,a)\n        policy[state, best_action] += (1.0 - epsilon)  # Ï€(A*|s) = 1-Îµ+Îµ/|A|\n    \n    return policy\n```\n\n> ğŸ”‘ **Îµ-è´ªå¿ƒç­–ç•¥å…¬å¼çš„å®ç°**ï¼š\n> - æ‰€æœ‰åŠ¨ä½œå…ˆè®¾ä¸º `Îµ/|A|`ï¼ˆæ¢ç´¢éƒ¨åˆ†ï¼‰\n> - æœ€ä½³åŠ¨ä½œå†åŠ ä¸Š `1-Îµ`ï¼ˆåˆ©ç”¨éƒ¨åˆ†ï¼‰\n> - ç»“æœï¼šæœ€ä½³åŠ¨ä½œæ¦‚ç‡ = `1-Îµ+Îµ/|A|`ï¼Œå…¶ä»–åŠ¨ä½œ = `Îµ/|A|`\n\n#### ç¬¬2æ­¥ï¼šç”Ÿæˆå›åˆ\n\n```python\n# GridWorldEpsilonGreedy.generate_episode() æ–¹æ³•ï¼š\ndef generate_episode(self, policy, max_steps):\n    state, _ = self.reset()  # ğŸ”‘ éšæœºèµ·ç‚¹ï¼ˆéæ¢ç´¢æ€§èµ·ç‚¹ï¼‰\n    episode = []\n    \n    for _ in range(max_steps):\n        # æ ¹æ®Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n        action = np.random.choice(self.action_space.n, p=policy[state])\n        next_state, reward, terminated, _, _ = self.step(action)\n        episode.append((state, action, reward))\n        if terminated:\n            break\n        state = next_state\n    \n    return episode\n```\n\n> âš¡ **ä¸MC Basicçš„æ ¸å¿ƒåŒºåˆ«**ï¼š\n> - âŒ **MC Basic**ï¼šä»æŒ‡å®šçš„(s,a)å¼€å§‹ï¼ˆæ¢ç´¢æ€§èµ·ç‚¹ï¼‰\n> - âœ… **MC Îµ-è´ªå¿ƒ**ï¼šä»éšæœºçŠ¶æ€å¼€å§‹ï¼Œæ•´ä¸ªå›åˆç”±Îµ-è´ªå¿ƒç­–ç•¥é©±åŠ¨\n\n---\n\n### ğŸ§® **æ­¥éª¤(b-c)ï¼šè®¡ç®—å›æŠ¥å¹¶æ›´æ–°Qå€¼** â†’ ä¸»å¾ªç¯ä¸­çš„ä»£ç \n\n**ä¼ªä»£ç **ï¼š\n```\nG â† 0\nå¾ªç¯ï¼ˆä»T-1åˆ°0ï¼‰ï¼š\n  G â† Î³G + R_{t+1}\n  é™¤é (S_t, A_t) å·²å‡ºç°ï¼š\n    è¿½åŠ  G åˆ° Returns(S_t, A_t)\n    Q(S_t, A_t) â† average(Returns(S_t, A_t))\n```\n\n**å®ç°ä»£ç **ï¼š\n```python\n# åœ¨ run_mc_epsilon_greedy() çš„ä¸»å¾ªç¯ä¸­ï¼š\nG = 0\nvisited_sa_pairs = set()  # è·Ÿè¸ªå·²è®¿é—®çš„(s,a)å¯¹ã€First-visitã€‘\n\n# ä»åå‘å‰éå†å›åˆ\nfor state, action, reward in reversed(episode):\n    G = reward + GAMMA * G  # æŠ˜æ‰£å›æŠ¥é€’æ¨è®¡ç®—\n    sa_pair = (state, action)\n    \n    # First-visit MCï¼šåªå¯¹é¦–æ¬¡å‡ºç°çš„(s,a)æ›´æ–°\n    if sa_pair not in visited_sa_pairs:\n        returns[sa_pair].append(G)                  # è¿½åŠ å›æŠ¥\n        Q[state, action] = np.mean(returns[sa_pair]) # æ›´æ–°Qå€¼ä¸ºå¹³å‡\n        visited_sa_pairs.add(sa_pair)               # æ ‡è®°å·²è®¿é—®\n```\n\n> ğŸ“Š **First-visit MCçš„å®ç°**ï¼š\n> - ä½¿ç”¨ `visited_sa_pairs` é›†åˆè·Ÿè¸ªå·²æ›´æ–°çš„(s,a)å¯¹\n> - åªå¯¹**é¦–æ¬¡å‡ºç°**çš„(s,a)è¿›è¡Œæ›´æ–°\n> - é¿å…åŒä¸€ä¸ª(s,a)åœ¨ä¸€ä¸ªå›åˆä¸­è¢«å¤šæ¬¡è®¡æ•°\n\n---\n\n### ğŸ”„ **æ­¥éª¤(c)ï¼šç­–ç•¥æ›´æ–°** â†’ åœ¨ä¸»å¾ªç¯å¼€å§‹æ—¶è‡ªåŠ¨å®Œæˆ\n\n**ä¼ªä»£ç **ï¼š\n```\nA* â† argmax_a Q(S_t, a)\nå¯¹æ‰€æœ‰ a âˆˆ A(S_t)ï¼š\n  å¦‚æœ a = A*: Ï€(a|S_t) â† 1 - Îµ + Îµ/|A|\n  å¦åˆ™: Ï€(a|S_t) â† Îµ/|A|\n```\n\n**å®ç°ä»£ç **ï¼š\n```python\n# åœ¨æ¯ä¸ªå›åˆå¼€å§‹æ—¶ï¼š\npolicy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n```\n\n> ğŸ’¡ **ç­–ç•¥æ›´æ–°çš„éšå¼å®ç°**ï¼š\n> - ä¸éœ€è¦æ˜¾å¼åœ°åœ¨å›åˆç»“æŸåæ›´æ–°ç­–ç•¥\n> - å› ä¸º `create_epsilon_greedy_policy()` æ€»æ˜¯åŸºäº**æœ€æ–°çš„Qå€¼**åˆ›å»ºç­–ç•¥\n> - æ¯ä¸ªå›åˆå¼€å§‹æ—¶é‡æ–°åˆ›å»ºç­–ç•¥ï¼Œè‡ªåŠ¨åæ˜ Qå€¼çš„å˜åŒ–\n\n---\n\n## ğŸ”‘ å…³é”®ç†è§£ç‚¹\n\n### 1ï¸âƒ£ **Îµ-è´ªå¿ƒç­–ç•¥å¦‚ä½•æ¶ˆé™¤æ¢ç´¢æ€§èµ·ç‚¹éœ€æ±‚ï¼Ÿ**\n\n**MC Basicçš„é—®é¢˜**ï¼š\n- éœ€è¦ä»æ¯ä¸ª(s,a)å¼€å§‹ç”Ÿæˆå›åˆï¼ˆæ¢ç´¢æ€§èµ·ç‚¹ï¼‰\n- å®é™…åº”ç”¨ä¸­å¾€å¾€æ— æ³•æ§åˆ¶èµ·ç‚¹\n\n**MC Îµ-è´ªå¿ƒçš„è§£å†³æ–¹æ¡ˆ**ï¼š\n```python\n# ç­–ç•¥æœ¬èº«å°±åŒ…å«æ¢ç´¢\npolicy[state, best_action] = 1 - Îµ + Îµ/num_actions  # åˆ©ç”¨ï¼ˆä¸»è¦ï¼‰\npolicy[state, other_action] = Îµ/num_actions         # æ¢ç´¢ï¼ˆæ¬¡è¦ï¼‰\n```\n\n- âœ… å³ä½¿ä»ä»»æ„èµ·ç‚¹å¼€å§‹ï¼ŒÎµ-è´ªå¿ƒç­–ç•¥ä¹Ÿä¼šä»¥å°æ¦‚ç‡å°è¯•æ‰€æœ‰åŠ¨ä½œ\n- âœ… é•¿æœŸæ¥çœ‹ï¼Œæ‰€æœ‰(s,a)å¯¹éƒ½ä¼šè¢«è®¿é—®åˆ°\n- âœ… æ— éœ€äººä¸ºæ§åˆ¶èµ·ç‚¹\n\n### 2ï¸âƒ£ **On-Policy å­¦ä¹ çš„ç‰¹ç‚¹**\n\n**å®šä¹‰**ï¼šç”¨äºç”Ÿæˆæ•°æ®çš„ç­–ç•¥ = è¢«è¯„ä¼°å’Œæ”¹è¿›çš„ç­–ç•¥\n\n**å®ç°ä½“ç°**ï¼š\n```python\n# åŒä¸€ä¸ªÎµ-è´ªå¿ƒç­–ç•¥Ï€æ—¢ç”¨äºï¼š\npolicy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)  # 1. ç”Ÿæˆç»éªŒ\nepisode = env.generate_episode(policy, MAX_STEPS)               # 2. è¯„ä¼°æ”¹è¿›\n```\n\n**ä¸MC Basicå¯¹æ¯”**ï¼š\n- MC Basicï¼šè¯„ä¼°å½“å‰ç­–ç•¥ï¼Œæ”¹è¿›ä¸ºè´ªå©ªç­–ç•¥ï¼ˆå¯ä»¥çœ‹ä½œoff-policyï¼‰\n- MC Îµ-è´ªå¿ƒï¼šå§‹ç»ˆè¯„ä¼°å’Œæ”¹è¿›åŒä¸€ä¸ªÎµ-è´ªå¿ƒç­–ç•¥ï¼ˆon-policyï¼‰\n\n### 3ï¸âƒ£ **ä¸ºä»€ä¹ˆéœ€è¦å¤§é‡å›åˆï¼Ÿ**\n\n```python\nNUM_EPISODES = 5000  # MC Îµ-è´ªå¿ƒéœ€è¦5000ä¸ªå›åˆ\n```\n\n**åŸå› åˆ†æ**ï¼š\n1. **éšæœºèµ·ç‚¹**ï¼šæ¯ä¸ªå›åˆåªè¦†ç›–ä¸€éƒ¨åˆ†(s,a)å¯¹\n2. **æ¢ç´¢æ¦‚ç‡å°**ï¼šÎµ=0.1æ„å‘³ç€90%æ—¶é—´åœ¨åˆ©ç”¨ï¼Œåªæœ‰10%åœ¨æ¢ç´¢\n3. **æ”¶æ•›éœ€è¦**ï¼šæ¯ä¸ª(s,a)éœ€è¦è¶³å¤Ÿå¤šçš„æ ·æœ¬æ‰èƒ½å‡†ç¡®ä¼°è®¡Qå€¼\n\n**ä¸MC Basicå¯¹æ¯”**ï¼š\n```python\n# MC Basicåªéœ€è¦20æ¬¡è¿­ä»£ï¼Œä½†æ¯æ¬¡è¿­ä»£ï¼š\nfor state in range(16):        # 16ä¸ªçŠ¶æ€\n    for action in range(4):    # 4ä¸ªåŠ¨ä½œ\n        for _ in range(10):    # æ¯ä¸ª(s,a)é‡‡æ ·10æ¬¡\n# æ€»è®¡ï¼š20 Ã— 16 Ã— 4 Ã— 10 = 12800 ä¸ªå›åˆï¼\n```\n\n---\n\n## ğŸ†š MC Basic vs MC Îµ-è´ªå¿ƒå¯¹æ¯”\n\n| ç»´åº¦ | MC Basicï¼ˆå›¾18ï¼‰ | MC Îµ-è´ªå¿ƒï¼ˆå›¾19ï¼‰ |\n|------|-----------------|-------------------|\n| **æ¢ç´¢æœºåˆ¶** | æ¢ç´¢æ€§èµ·ç‚¹ï¼ˆéå†æ‰€æœ‰(s,a)ï¼‰ | Îµ-è´ªå¿ƒç­–ç•¥ï¼ˆå†…ç½®æ¢ç´¢ï¼‰ |\n| **å›åˆèµ·ç‚¹** | æŒ‡å®šçš„(s,a)å¯¹ | éšæœºçŠ¶æ€ |\n| **ç­–ç•¥ç±»å‹** | ç¡®å®šæ€§è´ªå©ªç­–ç•¥ | éšæœºÎµ-è´ªå¿ƒç­–ç•¥ |\n| **å­¦ä¹ æ–¹å¼** | Off-policyï¼ˆè¯„ä¼°vsæ”¹è¿›ç­–ç•¥ä¸åŒï¼‰ | On-policyï¼ˆåŒä¸€ç­–ç•¥ï¼‰ |\n| **å®é™…å¯è¡Œæ€§** | éœ€è¦æ§åˆ¶ç¯å¢ƒèµ·ç‚¹ï¼ˆä¸å®ç”¨ï¼‰ | æ— éœ€æ§åˆ¶èµ·ç‚¹ï¼ˆå®ç”¨ï¼‰ |\n| **æ”¶æ•›ä¿è¯** | æ¢ç´¢æ€§èµ·ç‚¹å‡è®¾ä¸‹ä¿è¯ | Îµ-softç­–ç•¥ä¸‹ä¿è¯ |\n| **ä¸»å¾ªç¯ç»“æ„** | å¤–å±‚ï¼šè¿­ä»£ï¼›å†…å±‚ï¼šæ‰€æœ‰(s,a) | å•å±‚ï¼šå›åˆæ•° |\n\n---\n\n## ğŸ“ ç®—æ³•ç²¾é«“æ€»ç»“\n\n### Îµ-è´ªå¿ƒçš„æ•°å­¦ç¾æ„Ÿ\n\n$$\n\\pi(a|s) = \n\\begin{cases} \n1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max Q(s,a) \\\\ \n\\frac{\\epsilon}{|A|} & \\text{otherwise}\n\\end{cases}\n$$\n\n**ä»£ç å®ç°**ï¼š\n```python\npolicy = np.ones((num_states, num_actions)) * epsilon / num_actions  # åŸºç¡€æ¢ç´¢\npolicy[state, best_action] += (1.0 - epsilon)                        # é¢å¤–åˆ©ç”¨\n```\n\n**ç›´è§‰ç†è§£**ï¼š\n- æ‰€æœ‰åŠ¨ä½œå…ˆåˆ†äº« `Îµ` çš„æ€»æ¦‚ç‡ï¼ˆå…¬å¹³æ¢ç´¢ï¼‰\n- æœ€ä½³åŠ¨ä½œå†é¢å¤–è·å¾— `1-Îµ` çš„æ¦‚ç‡ï¼ˆé‡ç‚¹åˆ©ç”¨ï¼‰\n- ç»“æœï¼šæ¢ç´¢ä¸åˆ©ç”¨çš„å®Œç¾å¹³è¡¡\n\n### First-visit MCçš„å¿…è¦æ€§\n\n```python\nif sa_pair not in visited_sa_pairs:  # ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ£€æŸ¥ï¼Ÿ\n```\n\n**åŸå› **ï¼š\n- ä¸€ä¸ªå›åˆä¸­ï¼ŒåŒä¸€ä¸ª(s,a)å¯èƒ½å‡ºç°å¤šæ¬¡ï¼ˆç‰¹åˆ«æ˜¯åœ¨æœ‰å¾ªç¯çš„ç¯å¢ƒä¸­ï¼‰\n- å¦‚æœæ¯æ¬¡å‡ºç°éƒ½æ›´æ–°ï¼Œä¼šå¯¼è‡´æ ·æœ¬ä¸ç‹¬ç«‹\n- First-visitç¡®ä¿æ¯ä¸ªå›åˆå¯¹æ¯ä¸ª(s,a)åªè´¡çŒ®ä¸€ä¸ªæ ·æœ¬\n\n---\n\n## â“ æ€è€ƒé¢˜\n\nç°åœ¨ä½ ç†è§£äº†MC Îµ-è´ªå¿ƒçš„å®Œæ•´å®ç°ï¼Œæ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š\n\n1. **Îµçš„é€‰æ‹©**ï¼šå¦‚æœÎµ=0ä¼šæ€æ ·ï¼Ÿå¦‚æœÎµ=1å‘¢ï¼Ÿ\n   - æç¤ºï¼šæ€è€ƒæ¢ç´¢ä¸åˆ©ç”¨çš„æç«¯æƒ…å†µ\n\n2. **è¡°å‡Îµ**ï¼šèƒ½å¦åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é€æ¸å‡å°Îµï¼Ÿè¿™æ ·åšçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n   - æç¤ºï¼šæ—©æœŸéœ€è¦å¤šæ¢ç´¢ï¼ŒåæœŸéœ€è¦å¤šåˆ©ç”¨\n\n3. **ä¸MC Basicçš„æ•ˆç‡å¯¹æ¯”**ï¼šå“ªä¸ªæ–¹æ³•åœ¨å®é™…ä¸­æ›´é«˜æ•ˆï¼Ÿ\n   - æç¤ºï¼šè€ƒè™‘ç¯å¢ƒçº¦æŸå’Œé‡‡æ ·æˆæœ¬\n\n4. **æ”¶æ•›æ€§**ï¼šä¸ºä»€ä¹ˆÎµ-è´ªå¿ƒç­–ç•¥èƒ½ä¿è¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Ÿ\n   - æç¤ºï¼šGLIEï¼ˆGreedy in the Limit with Infinite Explorationï¼‰æ¡ä»¶\n\n---\n\nâ¸ï¸ **æ­å–œï¼ä½ ç°åœ¨å®Œå…¨ç†è§£äº†MC Îµ-è´ªå¿ƒç®—æ³•çš„ç†è®ºä¸å®ç°ã€‚æ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ æ›´é«˜æ•ˆçš„æ—¶åºå·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ æ–¹æ³•ï¼** ğŸ‰"
  },
  {
   "cell_type": "markdown",
   "source": "# ğŸ”¢ Qå€¼æ›´æ–°çš„ä¸¤ç§ç­‰ä»·æ–¹æ³•\n\n## âš ï¸ é‡è¦è¯´æ˜ï¼šå›¾ç‰‡ç®—æ³• vs æœ¬ç¬”è®°å®ç°\n\nä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œ**å›¾ç‰‡ä¸­çš„ç®—æ³•ä½¿ç”¨äº†ä¸åŒçš„Qå€¼æ›´æ–°æ–¹å¼**ã€‚è®©æˆ‘ä»¬è¯¦ç»†å¯¹æ¯”è¿™ä¸¤ç§æ–¹æ³•ã€‚\n\n---\n\n## ğŸ“Š å›¾ç‰‡ä¸­çš„ç®—æ³•ï¼šå¢é‡æ›´æ–°ï¼ˆIncremental Updateï¼‰\n\n### ä¼ªä»£ç \n```\nåˆå§‹åŒ–ï¼š\n  Q(s,a) â† 0ï¼Œå¯¹æ‰€æœ‰ s,a\n  N(s,a) â† 0ï¼Œå¯¹æ‰€æœ‰ s,a  # è®¿é—®æ¬¡æ•°è®¡æ•°å™¨\n  Îµ â† 1, k â† 1\n  Ï€_k â† Îµ-è´ªå¿ƒ(Q)\n\nå¾ªç¯ï¼ˆå¯¹æ¯ä¸ªå›åˆ kï¼‰ï¼š\n  å¯¹ç¬¬kä¸ªå›åˆé‡‡æ ·ï¼šSâ‚€,Aâ‚€,Râ‚,...,S_T\n  å¯¹å›åˆä¸­æ¯ä¸ªæ—¶é—´æ­¥ tï¼š\n    è®¡ç®— G_t â† R_{t+1} + Î³R_{t+2} + ...\n    N(S_t, A_t) â† N(S_t, A_t) + 1           # å¢åŠ è®¿é—®æ¬¡æ•°\n    Q(S_t, A_t) â† Q(S_t, A_t) + (1/N(S_t, A_t)) * (G_t - Q(S_t, A_t))  # å¢é‡æ›´æ–°\n  k â† k + 1\n  Îµ â† 1/k  # Îµè¡°å‡\n  Ï€_k â† Îµ-è´ªå¿ƒ(Q)\n```\n\n### ğŸ”‘ å…³é”®è¦ç´ \n\n#### 1. **N(s,a)ï¼šè®¿é—®æ¬¡æ•°è®¡æ•°å™¨**\n```python\nN = np.zeros((num_states, num_actions))  # åˆå§‹åŒ–è®¡æ•°å™¨\n\n# æ¯æ¬¡æ›´æ–°æ—¶ï¼š\nN[state, action] += 1  # å¢åŠ è®¿é—®æ¬¡æ•°\n```\n\n**ä½œç”¨**ï¼šè·Ÿè¸ªæ¯ä¸ª(s,a)å¯¹è¢«è®¿é—®äº†å¤šå°‘æ¬¡ï¼Œç”¨äºè®¡ç®—å¢é‡æ›´æ–°çš„æ­¥é•¿ã€‚\n\n#### 2. **å¢é‡æ›´æ–°å…¬å¼**\n$$\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} \\cdot (G_t - Q(S_t, A_t))\n$$\n\n**å…¬å¼è§£æ**ï¼š\n- `Q(S_t, A_t)`ï¼šå½“å‰Qå€¼ä¼°è®¡\n- `G_t`ï¼šæ–°è§‚æµ‹åˆ°çš„å›æŠ¥\n- `G_t - Q(S_t, A_t)`ï¼š**TDè¯¯å·®**ï¼ˆæ–°æ ·æœ¬ä¸å½“å‰ä¼°è®¡çš„å·®å¼‚ï¼‰\n- `1/N(S_t, A_t)`ï¼š**å­¦ä¹ ç‡**ï¼ˆæ­¥é•¿ï¼‰ï¼Œéšç€æ ·æœ¬å¢å¤šè€Œå‡å°\n\n**ç›´è§‰ç†è§£**ï¼š\n```\næ–°ä¼°è®¡ = æ—§ä¼°è®¡ + å­¦ä¹ ç‡ Ã— (æ–°æ ·æœ¬ - æ—§ä¼°è®¡)\n       = æ—§ä¼°è®¡ + å­¦ä¹ ç‡ Ã— é¢„æµ‹è¯¯å·®\n```\n\nå¦‚æœæ–°æ ·æœ¬ > æ—§ä¼°è®¡ï¼ŒQå€¼å¢åŠ ï¼›å¦‚æœæ–°æ ·æœ¬ < æ—§ä¼°è®¡ï¼ŒQå€¼å‡å°ã€‚\n\n---\n\n## ğŸ’» æœ¬ç¬”è®°çš„å®ç°ï¼šæ‰¹é‡å¹³å‡ï¼ˆBatch Averageï¼‰\n\n### å®ç°ä»£ç \n\n```python\n# åˆå§‹åŒ–\nreturns = defaultdict(list)  # å­˜å‚¨æ‰€æœ‰å›æŠ¥æ ·æœ¬\n\n# åœ¨ä¸»å¾ªç¯ä¸­ï¼š\nfor state, action, reward in reversed(episode):\n    G = reward + GAMMA * G\n    sa_pair = (state, action)\n    \n    if sa_pair not in visited_sa_pairs:\n        returns[sa_pair].append(G)                  # ğŸ“ å­˜å‚¨å›æŠ¥\n        Q[state, action] = np.mean(returns[sa_pair]) # ğŸ§® è®¡ç®—å¹³å‡\n        visited_sa_pairs.add(sa_pair)\n```\n\n### ğŸ”‘ å…³é”®ç‰¹ç‚¹\n\n1. **å­˜å‚¨æ‰€æœ‰æ ·æœ¬**ï¼š`returns[(s,a)]` åˆ—è¡¨ä¿å­˜æ‰€æœ‰å†å²å›æŠ¥\n2. **é‡æ–°è®¡ç®—å¹³å‡**ï¼šæ¯æ¬¡éƒ½ç”¨ `np.mean()` è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼\n\n---\n\n## ğŸ¯ æ•°å­¦ç­‰ä»·æ€§è¯æ˜\n\n### å®šç†ï¼šå¢é‡æ›´æ–° â‰¡ æ‰¹é‡å¹³å‡\n\n**è®¾å‰nä¸ªæ ·æœ¬çš„å¹³å‡å€¼ä¸º $Q_n$**ï¼Œå½“æ”¶åˆ°ç¬¬ $n+1$ ä¸ªæ–°æ ·æœ¬ $G_{n+1}$ æ—¶ï¼š\n\n#### æ‰¹é‡å¹³å‡æ–¹æ³•ï¼š\n$$\nQ_{n+1} = \\frac{1}{n+1} \\sum_{i=1}^{n+1} G_i\n$$\n\n#### å¢é‡æ›´æ–°æ–¹æ³•ï¼š\nä»æ‰¹é‡å¹³å‡å…¬å¼æ¨å¯¼ï¼š\n\n$$\n\\begin{align}\nQ_{n+1} &= \\frac{1}{n+1} \\sum_{i=1}^{n+1} G_i \\\\\n&= \\frac{1}{n+1} \\left( \\sum_{i=1}^{n} G_i + G_{n+1} \\right) \\\\\n&= \\frac{1}{n+1} \\left( n \\cdot Q_n + G_{n+1} \\right) \\\\\n&= \\frac{n}{n+1} Q_n + \\frac{1}{n+1} G_{n+1} \\\\\n&= Q_n + \\frac{1}{n+1} (G_{n+1} - Q_n) \\quad \\checkmark\n\\end{align}\n$$\n\n**ç»“è®º**ï¼šä¸¤ç§æ–¹æ³•åœ¨æ•°å­¦ä¸Š**å®Œå…¨ç­‰ä»·**ï¼\n\n---\n\n## ğŸ” å…·ä½“ä¾‹å­ï¼šæ•°å€¼éªŒè¯\n\n### åœºæ™¯è®¾ç½®\nå‡è®¾çŠ¶æ€-åŠ¨ä½œå¯¹ $(s_0, a_0)$ åœ¨3ä¸ªå›åˆä¸­çš„å›æŠ¥åˆ†åˆ«ä¸ºï¼š\n- ç¬¬1ä¸ªå›åˆï¼š$G_1 = 5.0$\n- ç¬¬2ä¸ªå›åˆï¼š$G_2 = 8.0$  \n- ç¬¬3ä¸ªå›åˆï¼š$G_3 = 6.0$\n\n### æ–¹æ³•1ï¼šæ‰¹é‡å¹³å‡\n\n```python\nreturns = [5.0, 8.0, 6.0]\nQ = np.mean(returns) = (5.0 + 8.0 + 6.0) / 3 = 6.333...\n```\n\n### æ–¹æ³•2ï¼šå¢é‡æ›´æ–°\n\n```python\n# åˆå§‹åŒ–\nQ = 0.0\nN = 0\n\n# ç¬¬1ä¸ªæ ·æœ¬ Gâ‚ = 5.0\nN = 1\nQ = Q + (1/N) * (5.0 - Q)\n  = 0.0 + 1.0 * (5.0 - 0.0)\n  = 5.0\n\n# ç¬¬2ä¸ªæ ·æœ¬ Gâ‚‚ = 8.0\nN = 2\nQ = Q + (1/N) * (8.0 - Q)\n  = 5.0 + 0.5 * (8.0 - 5.0)\n  = 5.0 + 1.5\n  = 6.5\n\n# ç¬¬3ä¸ªæ ·æœ¬ Gâ‚ƒ = 6.0\nN = 3\nQ = Q + (1/N) * (6.0 - Q)\n  = 6.5 + (1/3) * (6.0 - 6.5)\n  = 6.5 - 0.1666...\n  = 6.333...  âœ… ç›¸åŒï¼\n```\n\n**éªŒè¯æˆåŠŸ**ï¼šä¸¤ç§æ–¹æ³•å¾—åˆ°ç›¸åŒç»“æœ `6.333...`\n\n---\n\n## âš–ï¸ ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”\n\n| ç»´åº¦ | æ‰¹é‡å¹³å‡ï¼ˆæœ¬ç¬”è®°ï¼‰ | å¢é‡æ›´æ–°ï¼ˆå›¾ç‰‡ç®—æ³•ï¼‰ |\n|------|-------------------|---------------------|\n| **å†…å­˜æ¶ˆè€—** | O(æ€»æ ·æœ¬æ•°) | O(1) |\n| **è®¡ç®—å¤æ‚åº¦** | O(N) æ¯æ¬¡æ›´æ–° | O(1) æ¯æ¬¡æ›´æ–° |\n| **å®ç°éš¾åº¦** | ç®€å•ï¼ˆPythonä¸€è¡Œï¼‰ | ä¸­ç­‰ï¼ˆéœ€è¦ç»´æŠ¤Nï¼‰ |\n| **æ•°å­¦ç­‰ä»·æ€§** | âœ… ç­‰ä»· | âœ… ç­‰ä»· |\n| **é€‚åˆåœºæ™¯** | æ•™å­¦ã€è°ƒè¯•ã€å°è§„æ¨¡é—®é¢˜ | å®é™…åº”ç”¨ã€å¤§è§„æ¨¡é—®é¢˜ |\n| **æ‰©å±•æ€§** | ä¸é€‚åˆåœ¨çº¿å­¦ä¹  | é€‚åˆåœ¨çº¿å­¦ä¹  |\n\n---\n\n## ğŸ“ ä¸ºä»€ä¹ˆæœ¬ç¬”è®°é€‰æ‹©æ‰¹é‡å¹³å‡ï¼Ÿ\n\n### âœ… æ•™å­¦ä¼˜åŠ¿\n1. **æ¦‚å¿µæ¸…æ™°**ï¼šç›´æ¥å¯¹åº”\"å¹³å‡å›æŠ¥\"çš„å®šä¹‰\n2. **æ˜“äºç†è§£**ï¼š`np.mean(returns)` ä¸€ç›®äº†ç„¶\n3. **ä¾¿äºè°ƒè¯•**ï¼šå¯ä»¥æŸ¥çœ‹æ‰€æœ‰å†å²æ ·æœ¬\n4. **ä»£ç ç®€æ´**ï¼šæ— éœ€é¢å¤–çš„è®¡æ•°å™¨å˜é‡\n\n### ğŸ“š æ•™å­¦æµç¨‹çš„ä¼˜å…ˆçº§\n```\næ¦‚å¿µç†è§£ > å†…å­˜æ•ˆç‡\nç›´è§‚æ€§ > è®¡ç®—é€Ÿåº¦\n```\n\nåœ¨æ•™å­¦ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ä¼˜å…ˆè®©ä½ ç†è§£\"Qå€¼æ˜¯å›æŠ¥çš„å¹³å‡å€¼\"è¿™ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼Œè€Œä¸æ˜¯ä¸€å¼€å§‹å°±å¼•å…¥å¢é‡æ›´æ–°çš„æŠ€å·§ã€‚\n\n---\n\n## ğŸš€ å¢é‡æ›´æ–°çš„å®é™…åº”ç”¨ä»·å€¼\n\n### ä¸ºä»€ä¹ˆå®é™…åº”ç”¨ä¸­æ›´å¸¸ç”¨å¢é‡æ›´æ–°ï¼Ÿ\n\n#### 1. **å†…å­˜æ•ˆç‡**\n```python\n# æ‰¹é‡å¹³å‡ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰æ ·æœ¬\nreturns = defaultdict(list)  # å¯èƒ½å­˜å‚¨æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬\n\n# å¢é‡æ›´æ–°ï¼šåªéœ€è¦O(1)å†…å­˜\nQ = np.zeros((num_states, num_actions))\nN = np.zeros((num_states, num_actions))\n```\n\n#### 2. **è®¡ç®—æ•ˆç‡**\n```python\n# æ‰¹é‡å¹³å‡ï¼šæ¯æ¬¡O(N)è®¡ç®—\nQ[s, a] = np.mean(returns[(s, a)])  # éå†æ‰€æœ‰æ ·æœ¬\n\n# å¢é‡æ›´æ–°ï¼šæ¯æ¬¡O(1)è®¡ç®—\nQ[s, a] += (1/N[s, a]) * (G - Q[s, a])  # å¸¸æ•°æ—¶é—´\n```\n\n#### 3. **åœ¨çº¿å­¦ä¹ **\n- å¢é‡æ›´æ–°å¯ä»¥å®æ—¶æ›´æ–°ï¼Œä¸éœ€è¦ä¿å­˜å†å²æ•°æ®\n- é€‚åˆæ°¸ä¸åœæ­¢çš„è¿ç»­å­¦ä¹ åœºæ™¯ï¼ˆå¦‚æœºå™¨äººã€æ¸¸æˆAIï¼‰\n\n---\n\n## ğŸ’¡ å¢é‡æ›´æ–°çš„ä»£ç å®ç°ç¤ºä¾‹\n\nå¦‚æœä½ æƒ³æŒ‰ç…§å›¾ç‰‡ä¸­çš„ç®—æ³•å®ç°ï¼Œå¯ä»¥è¿™æ ·ä¿®æ”¹ï¼š\n\n```python\ndef run_mc_epsilon_greedy_incremental(gif_filename='mc_incremental.gif', fps=10):\n    \"\"\"ä½¿ç”¨å¢é‡æ›´æ–°çš„MC Îµ-è´ªå¿ƒç®—æ³•\"\"\"\n    \n    # åˆå§‹åŒ–\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    N = np.zeros((num_states, num_actions))  # ğŸ”‘ è®¿é—®æ¬¡æ•°è®¡æ•°å™¨\n    \n    # ä¸»å¾ªç¯\n    for k in range(1, NUM_EPISODES + 1):\n        # åŠ¨æ€è°ƒæ•´ Îµï¼ˆå¦‚å›¾ç‰‡ç®—æ³•ï¼‰\n        epsilon = 1.0 / k  # Îµè¡°å‡\n        \n        # åˆ›å»º Îµ-è´ªå¿ƒç­–ç•¥\n        policy = create_epsilon_greedy_policy(Q, epsilon, num_actions)\n        \n        # ç”Ÿæˆå›åˆ\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # è®¡ç®—å›æŠ¥å¹¶å¢é‡æ›´æ–° Q å€¼\n        G = 0\n        visited_sa_pairs = set()\n        \n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # First-visit æ£€æŸ¥\n            if sa_pair not in visited_sa_pairs:\n                N[state, action] += 1  # ğŸ”‘ å¢åŠ è®¿é—®æ¬¡æ•°\n                \n                # ğŸ”‘ å¢é‡æ›´æ–°å…¬å¼\n                alpha = 1.0 / N[state, action]  # å­¦ä¹ ç‡ = 1/N\n                Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])\n                \n                visited_sa_pairs.add(sa_pair)\n    \n    return Q\n\n# âœ… è¿™ä¸ªç‰ˆæœ¬ä¸å›¾ç‰‡ç®—æ³•å®Œå…¨ä¸€è‡´ï¼\n```\n\n### ğŸ”‘ å…³é”®ä¿®æ”¹ç‚¹\n\n1. **æ·»åŠ  N è®¡æ•°å™¨**ï¼š`N = np.zeros((num_states, num_actions))`\n2. **å¢é‡æ›´æ–°å…¬å¼**ï¼š`Q += (1/N) * (G - Q)`\n3. **Îµè¡°å‡**ï¼š`epsilon = 1.0 / k`ï¼ˆå¯é€‰ï¼Œå›¾ç‰‡ä¸­æœ‰ï¼‰\n\n---\n\n## ğŸ“‹ å®Œæ•´å¯¹ç…§è¡¨ï¼šå›¾ç‰‡ç®—æ³• vs æœ¬ç¬”è®°å®ç°\n\n| å›¾ç‰‡ç®—æ³•æ­¥éª¤ | å›¾ç‰‡ä¼ªä»£ç  | æœ¬ç¬”è®°å®ç° | æ•°å­¦ç­‰ä»·æ€§ |\n|-------------|-----------|-----------|----------|\n| åˆå§‹åŒ–Q | `Q(s,a) â† 0` | `Q = np.zeros(...)` | âœ… |\n| åˆå§‹åŒ–è®¡æ•°å™¨ | `N(s,a) â† 0` | `returns = defaultdict(list)` | âœ…ï¼ˆéšå¼ï¼‰ |\n| ç”Ÿæˆå›åˆ | é‡‡æ ·å›åˆ | `env.generate_episode()` | âœ… |\n| è®¡ç®—å›æŠ¥ | `G_t â† R_{t+1} + Î³G_{t+1}` | `G = reward + GAMMA * G` | âœ… |\n| æ›´æ–°è®¿é—®æ¬¡æ•° | `N(s,a) â† N(s,a) + 1` | `len(returns[(s,a)])` | âœ…ï¼ˆéšå¼ï¼‰ |\n| æ›´æ–°Qå€¼ | `Q += (1/N)*(G-Q)` | `Q = mean(returns)` | âœ… ç­‰ä»·ï¼ |\n| ç­–ç•¥æ›´æ–° | `Ï€ â† Îµ-è´ªå¿ƒ(Q)` | `create_epsilon_greedy_policy(Q)` | âœ… |\n\n---\n\n## ğŸ¯ æ€»ç»“ä¸å»ºè®®\n\n### âœ… ä½ ç°åœ¨åº”è¯¥ç†è§£çš„å…³é”®ç‚¹\n\n1. **å›¾ç‰‡ç®—æ³•ç”¨å¢é‡æ›´æ–°ï¼Œæœ¬ç¬”è®°ç”¨æ‰¹é‡å¹³å‡**\n2. **ä¸¤ç§æ–¹æ³•æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·**\n3. **æ‰¹é‡å¹³å‡æ›´ç›´è§‚ï¼Œå¢é‡æ›´æ–°æ›´é«˜æ•ˆ**\n4. **æœ¬ç¬”è®°ä¼˜å…ˆæ•™å­¦æ¸…æ™°æ€§ï¼Œå®é™…åº”ç”¨ä¼˜å…ˆæ•ˆç‡**\n\n### ğŸš€ å­¦ä¹ è·¯å¾„å»ºè®®\n\n1. **å½“å‰é˜¶æ®µ**ï¼šä½¿ç”¨æ‰¹é‡å¹³å‡ï¼ˆæœ¬ç¬”è®°å®ç°ï¼‰\n   - ç†è§£\"Qå€¼ = å›æŠ¥å¹³å‡å€¼\"è¿™ä¸ªæ ¸å¿ƒæ¦‚å¿µ\n   - ä¾¿äºè°ƒè¯•å’Œå¯è§†åŒ–\n\n2. **è¿›é˜¶é˜¶æ®µ**ï¼šå­¦ä¹ å¢é‡æ›´æ–°\n   - ç†è§£å¢é‡å…¬å¼çš„æ¨å¯¼\n   - é€‚åº”æ›´é«˜æ•ˆçš„å®ç°æ–¹å¼\n\n3. **å®æˆ˜é˜¶æ®µ**ï¼šä½¿ç”¨å¢é‡æ›´æ–°\n   - å¤„ç†å¤§è§„æ¨¡é—®é¢˜\n   - åœ¨çº¿å­¦ä¹ åœºæ™¯\n\n### ğŸ’¡ ä¸‹ä¸€æ­¥\n\n- å¦‚æœä½ æƒ³å®è·µå¢é‡æ›´æ–°ï¼Œå¯ä»¥å¤åˆ¶ä¸Šé¢çš„ `run_mc_epsilon_greedy_incremental()` å‡½æ•°\n- è¿è¡Œå¹¶å¯¹æ¯”ä¸¤ç§å®ç°çš„ç»“æœï¼ˆåº”è¯¥å®Œå…¨ç›¸åŒï¼ï¼‰\n- æ€è€ƒï¼šå¦‚æœç”¨å¸¸æ•°å­¦ä¹ ç‡ `Î± = 0.1` ä»£æ›¿ `1/N`ï¼Œä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n\n---\n\nâ¸ï¸ **ç†è§£äº†Qå€¼æ›´æ–°çš„ä¸¤ç§æ–¹æ³•åï¼Œä½ å°±å®Œå…¨æŒæ¡äº†å›¾ç‰‡ç®—æ³•ä¸æœ¬ç¬”è®°å®ç°çš„å¯¹åº”å…³ç³»ï¼**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}