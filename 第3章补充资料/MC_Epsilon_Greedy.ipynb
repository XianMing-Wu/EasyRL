{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC ε-贪心算法教程：更实用的蒙特卡洛方法\n",
    "\n",
    "## 📖 为什么需要 MC ε-贪心？\n",
    "\n",
    "在 `MC Basic` 算法中，我们学习了蒙特卡洛方法的基本原理。但它有一个很强的假设：**探索性起点 (Exploring Starts)**。这意味着为了评估所有状态-动作对，我们需要从每一个可能的 (s,a) 对开始生成回合。\n",
    "\n",
    "在现实世界中，这个假设往往难以满足。例如，在机器人控制任务中，我们可能无法随意设置机器人的初始状态和初始动作。\n",
    "\n",
    "**MC ε-贪心算法**通过引入“探索”机制解决了这个问题，使其更加实用：\n",
    "\n",
    "- **无需探索性起点**：通过 ε-贪心策略在学习过程中进行探索。\n",
    "- **平衡探索与利用**：以 1-ε 的概率选择当前最优动作（利用），以 ε 的概率随机选择一个动作（探索）。\n",
    "- **在线策略 (On-policy)**：用于生成数据的策略和被评估改进的策略是同一个。\n",
    "- **保证收敛**：在一定条件下，它能收敛到最优策略。\n",
    "\n",
    "## 🎯 本教程目标\n",
    "\n",
    "我们将从零开始实现 MC ε-贪心算法，并在同一个 4×4 网格世界中进行可视化：\n",
    "\n",
    "- **核心内容**：\n",
    "  - 理解 ε-贪心策略如何平衡探索与利用。\n",
    "  - 实现基于回合采样的 on-policy 蒙特卡洛控制。\n",
    "  - 可视化 Q 值和策略在探索中的收敛过程。\n",
    "\n",
    "- **学习路径**：\n",
    "  1. 理解 ε-贪心策略。\n",
    "  2. 构建支持随机起始回合的环境。\n",
    "  3. 实现 MC ε-贪心核心算法。\n",
    "  4. 可视化学习过程（GIF 动画）。\n",
    "  5. 分析与 MC Basic 的异同。\n",
    "\n",
    "---\n",
    "\n",
    "让我们开始探索这种更强大的无模型学习方法！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：安装和导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 导入必要的库\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom typing import Optional, Tuple, Dict, Any, List, Annotated\nfrom collections import defaultdict\n\n# 设置 matplotlib 后端（必须在导入 pyplot 之前）\nimport matplotlib\nmatplotlib.use('Agg')  # 使用非交互式后端，适合云端环境\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom IPython.display import display, clear_output, Image as IPImage\nimport imageio\nfrom io import BytesIO\n\n# 配置 matplotlib 中文显示\nplt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(f\"✅ Gymnasium 版本: {gym.__version__}\")\nprint(f\"✅ NumPy 版本: {np.__version__}\")\nprint(f\"✅ Matplotlib 版本: {matplotlib.__version__}\")\nprint(f\"✅ Matplotlib 后端: {matplotlib.get_backend()}\")\nprint(f\"✅ 中文字体配置完成\")\nprint(f\"✅ imageio 已导入，支持 GIF 动画生成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：创建支持 MC 学习的网格世界环境\n",
    "\n",
    "为了适应 ε-贪心算法，我们需要对环境做一些微调。主要区别在于回合的生成方式：\n",
    "\n",
    "- **随机起点**：回合可以从任意非终止状态随机开始。\n",
    "- **策略驱动**：整个回合都由一个给定的策略（即 ε-贪心策略）驱动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GridWorldEpsilonGreedy(gym.Env):\n    \"\"\"支持 MC ε-贪心学习的网格世界环境\"\"\"\n    \n    def __init__(\n        self,\n        size: Annotated[int, \"网格的边长\"] = 4,\n        rewards: Annotated[Optional[np.ndarray], \"可选的奖励矩阵\"] = None,\n    ):\n        super().__init__()\n        self.size = size\n        \n        # 设置奖励矩阵\n        if rewards is None:\n            self.rewards = np.array([\n                [0, 0, -1, -1],\n                [0, -1, -1, 1],\n                [0, -1, 0, 0],\n                [-1, 0, 0, -1]\n            ])\n        else:\n            self.rewards = rewards\n        \n        # 目标位置和障碍物位置\n        self.target_pos = np.argwhere(self.rewards == 1)[0]\n        self.obstacle_mask = (self.rewards == -1)\n        \n        # 动作空间：0=上, 1=下, 2=左, 3=右\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Discrete(size * size)\n        \n        # 动作映射\n        self._action_to_direction = {\n            0: np.array([-1, 0]),  # 上\n            1: np.array([1, 0]),   # 下\n            2: np.array([0, -1]),  # 左\n            3: np.array([0, 1]),   # 右\n        }\n        \n        self._agent_location = np.array([0, 0])\n    \n    def _pos_to_state(\n        self,\n        pos: Annotated[np.ndarray, \"二维位置坐标\"],\n    ) -> Annotated[int, \"对应的一维状态索引\"]:\n        \"\"\"将二维位置转换为一维状态索引\"\"\"\n        return pos[0] * self.size + pos[1]\n    \n    def _state_to_pos(\n        self,\n        state: Annotated[int, \"一维状态索引\"],\n    ) -> Annotated[np.ndarray, \"对应的二维位置坐标\"]:\n        \"\"\"将一维状态索引转换为二维位置\"\"\"\n        return np.array([state // self.size, state % self.size])\n    \n    def reset(\n        self,\n        start_pos: Annotated[Optional[np.ndarray], \"可选的起始位置坐标\"] = None,\n    ) -> Annotated[Tuple[int, Dict[str, Any]], \"初始状态索引及附加信息字典\"]:\n        \"\"\"重置环境，可以指定或随机选择起始位置\"\"\"\n        if start_pos is not None:\n            self._agent_location = np.array(start_pos)\n        else:\n            # 随机选择一个非终止、非障碍的起始位置\n            while True:\n                pos = np.random.randint(0, self.size, size=2)\n                if not np.array_equal(pos, self.target_pos) and not self.obstacle_mask[pos[0], pos[1]]:\n                    self._agent_location = pos\n                    break\n        \n        state = self._pos_to_state(self._agent_location)\n        return state, {}\n    \n    def step(\n        self,\n        action: Annotated[int, \"要执行的动作编号\"],\n    ) -> Annotated[Tuple[int, float, bool, bool, Dict[str, Any]], \"新状态、奖励、终止标志、截断标志及额外信息\"]:\n        \"\"\"执行动作\"\"\"\n        direction = self._action_to_direction[action]\n        new_location = self._agent_location + direction\n        \n        if (0 <= new_location[0] < self.size and 0 <= new_location[1] < self.size):\n            self._agent_location = new_location\n        \n        reward = self.rewards[self._agent_location[0], self._agent_location[1]]\n        terminated = np.array_equal(self._agent_location, self.target_pos)\n        state = self._pos_to_state(self._agent_location)\n        return state, reward, terminated, False, {}\n    \n    def generate_episode(\n        self,\n        policy: Annotated[np.ndarray, \"策略概率矩阵\"],\n        max_steps: Annotated[int, \"单个回合的最大步数\"] = 100,\n    ) -> Annotated[List[Tuple[int, int, float]], \"状态-动作-奖励序列\"]:\n        \"\"\"从随机状态开始，根据策略生成一个完整的回合\"\"\"\n        state, _ = self.reset() # 随机起点\n        episode = []\n        \n        for _ in range(max_steps):\n            action = np.random.choice(self.action_space.n, p=policy[state])\n            next_state, reward, terminated, _, _ = self.step(action)\n            episode.append((state, action, reward))\n            if terminated:\n                break\n            state = next_state\n        \n        return episode\n\n# 创建环境实例\nenv = GridWorldEpsilonGreedy(size=4)\nprint(f\"✅ GridWorldEpsilonGreedy 环境创建完成\")\nprint(f\"   - 网格大小: {env.size}×{env.size}\")\nprint(f\"   - 动作空间: {env.action_space} (0=上, 1=下, 2=左, 3=右)\")\nprint(f\"   - 目标位置: {env.target_pos}\")\nprint(f\"\\n   奖励矩阵:\")\nprint(env.rewards)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：理解 MC ε-贪心算法\n",
    "\n",
    "### 🧮 ε-贪心策略\n",
    "\n",
    "为了确保在学习过程中有足够的探索，我们使用 **ε-贪心策略**。对于任意状态 $s$，策略 $\\pi$ 定义如下：\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \n",
    "\\begin{cases} \n",
    "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \n",
    "\\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a \\neq \\arg\\max_{a'} Q(s,a') \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $1 - \\epsilon$ 是“利用”的概率，即选择当前估计的最好动作。\n",
    "- $\\epsilon$ 是“探索”的概率，即随机选择一个动作。\n",
    "- $|\\mathcal{A}(s)|$ 是状态 $s$ 下的动作数量。\n",
    "\n",
    "### 🔄 On-Policy MC Control 算法流程\n",
    "\n",
    "**初始化**：\n",
    "- 随机初始化 $Q(s,a)$。\n",
    "- 初始化 `returns(s,a)` 为空列表。\n",
    "- 根据初始 Q 值创建一个 ε-贪心策略 $\\pi$。\n",
    "\n",
    "**循环（直到收敛或达到最大回合数）：**\n",
    "\n",
    "1. **生成回合**：使用当前策略 $\\pi$ 从一个随机起点生成一个完整的回合。\n",
    "   - `(S_0, A_0, R_1), (S_1, A_1, R_2), ..., (S_{T-1}, A_{T-1}, R_T)`\n",
    "\n",
    "2. **计算回报**：对于回合中每个时间步 $t=0, 1, ..., T-1$：\n",
    "   - 计算从该步开始的折扣回报 $G_t = R_{t+1} + \\gamma R_{t+2} + ...$\n",
    "\n",
    "3. **更新 Q 值**：对于回合中出现的每一个状态-动作对 $(S_t, A_t)$：\n",
    "   - 将回报 $G_t$ 添加到 `returns(S_t, A_t)` 列表中。\n",
    "   - 更新 $Q(S_t, A_t)$ 为 `returns(S_t, A_t)` 的平均值。\n",
    "\n",
    "4. **更新策略**：根据更新后的 Q 值，改进 ε-贪心策略 $\\pi$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：定义算法参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 算法参数配置完成\n",
      "   - 网格大小: 4×4\n",
      "   - 折扣因子 γ: 0.9\n",
      "   - Epsilon ε: 0.1\n",
      "   - 总回合数: 5000\n",
      "   - 动作空间: ['↑', '↓', '←', '→']\n"
     ]
    }
   ],
   "source": [
    "# --- 算法参数 ---\n",
    "GRID_SIZE = 4           # 网格大小\n",
    "GAMMA = 0.9             # 折扣因子\n",
    "EPSILON = 0.1           # ε-贪心策略中的 ε 值\n",
    "NUM_EPISODES = 5000     # 总共要运行的回合数\n",
    "MAX_STEPS = 100         # 每个回合的最大步数\n",
    "\n",
    "# 动作名称（用于可视化）\n",
    "ACTION_NAMES = ['↑', '↓', '←', '→']\n",
    "\n",
    "print(f\"✅ 算法参数配置完成\")\n",
    "print(f\"   - 网格大小: {GRID_SIZE}×{GRID_SIZE}\")\n",
    "print(f\"   - 折扣因子 γ: {GAMMA}\")\n",
    "print(f\"   - Epsilon ε: {EPSILON}\")\n",
    "print(f\"   - 总回合数: {NUM_EPISODES}\")\n",
    "print(f\"   - 动作空间: {ACTION_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：实现 MC ε-贪心核心算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_epsilon_greedy_policy(\n    Q: Annotated[np.ndarray, \"状态-动作价值表\"],\n    epsilon: Annotated[float, \"ε-贪心策略中的探索概率\"],\n    num_actions: Annotated[int, \"动作空间的大小\"],\n) -> Annotated[np.ndarray, \"ε-贪心策略概率矩阵\"]:\n    \"\"\"根据 Q 值创建一个 ε-贪心策略\"\"\"\n    num_states = Q.shape[0]\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])\n        policy[state, best_action] += (1.0 - epsilon)\n    \n    return policy\n\ndef get_policy_matrix(\n    policy: Annotated[np.ndarray, \"策略概率矩阵\"],\n    grid_size: Annotated[int, \"网格的边长\"],\n) -> Annotated[np.ndarray, \"用于可视化的策略箭头矩阵\"]:\n    \"\"\"将策略转换为箭头矩阵用于可视化\"\"\"\n    policy_arrows = np.empty((grid_size, grid_size), dtype=object)\n    \n    for state in range(grid_size * grid_size):\n        row = state // grid_size\n        col = state % grid_size\n        best_action = np.argmax(policy[state])\n        policy_arrows[row, col] = ACTION_NAMES[best_action]\n    \n    return policy_arrows\n\nprint(\"✅ MC ε-贪心核心函数定义完成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：实现可视化函数\n",
    "\n",
    "可视化函数与 `MC Basic` 基本相同，但标题和迭代计数方式有所调整，以反映是基于回合数进行更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def render_mc_epsilon_greedy_to_array(\n    rewards: Annotated[np.ndarray, \"奖励矩阵\"],\n    Q: Annotated[np.ndarray, \"状态-动作价值表\"],\n    policy_arrows: Annotated[np.ndarray, \"策略箭头矩阵\"],\n    episode_num: Annotated[int, \"当前回合数\"],\n) -> Annotated[np.ndarray, \"用于生成动画的 RGB 数组\"]:\n    \"\"\"\n    将当前 MC ε-贪心状态渲染为 RGB 数组（用于 GIF 生成）\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=80)\n    \n    # 配色方案\n    reward_cmap = LinearSegmentedColormap.from_list('reward', ['red', 'white', 'green'])\n    value_cmap = 'viridis'\n    \n    # 1. 绘制奖励矩阵\n    ax1 = axes[0]\n    im1 = ax1.imshow(rewards, cmap=reward_cmap, vmin=-1, vmax=1)\n    ax1.set_title(f'Reward Matrix', fontsize=14, fontweight='bold')\n    ax1.set_xticks(range(GRID_SIZE))\n    ax1.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax1.text(j, i, f'{rewards[i, j]:.0f}',\n                    ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n    \n    plt.colorbar(im1, ax=ax1, fraction=0.046)\n    \n    # 2. 绘制 Q 值矩阵（显示最大 Q 值）\n    ax2 = axes[1]\n    Q_max = Q.reshape(GRID_SIZE, GRID_SIZE, 4).max(axis=2)\n    im2 = ax2.imshow(Q_max, cmap=value_cmap)\n    ax2.set_title(f'Q-Value Matrix (max) - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax2.set_xticks(range(GRID_SIZE))\n    ax2.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax2.text(j, i, f'{Q_max[i, j]:.2f}',\n                    ha=\"center\", va=\"center\", \n                    color=\"white\" if Q_max[i, j] < (Q_max.max()/2 if Q_max.max() > 0 else 0.5) else \"black\",\n                    fontsize=10)\n    \n    plt.colorbar(im2, ax=ax2, fraction=0.046)\n    \n    # 3. 绘制策略矩阵\n    ax3 = axes[2]\n    policy_display = np.zeros((GRID_SIZE, GRID_SIZE))\n    im3 = ax3.imshow(policy_display, cmap='gray', vmin=0, vmax=1, alpha=0.1)\n    ax3.set_title(f'Policy Matrix - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax3.set_xticks(range(GRID_SIZE))\n    ax3.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            if policy_arrows[i, j]:\n                ax3.text(j, i, policy_arrows[i, j],\n                        ha=\"center\", va=\"center\", \n                        color=\"blue\", fontsize=24, fontweight='bold')\n    \n    # 整体标题\n    fig.suptitle(f'MC ε-Greedy Algorithm: Episode {episode_num}', \n                 fontsize=16, fontweight='bold', y=0.98)\n    \n    plt.tight_layout()\n    \n    # 转换为 RGB 数组\n    fig.canvas.draw()\n    buf = fig.canvas.buffer_rgba()\n    rgb_array = np.asarray(buf)\n    rgb_array = rgb_array[:, :, :3]  # RGBA to RGB\n    plt.close(fig)\n    \n    return rgb_array\n\nprint(\"✅ 可视化函数定义完成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七步：执行 MC ε-贪心算法并生成 GIF 动画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_mc_epsilon_greedy(\n    gif_filename: Annotated[str, \"输出 GIF 文件名\"] = 'mc_epsilon_greedy.gif',\n    fps: Annotated[int, \"生成动画的帧率\"] = 10,\n) -> Annotated[Tuple[np.ndarray, np.ndarray], \"最终 Q 值和策略矩阵\"]:\n    \"\"\"\n    执行 MC ε-贪心算法并生成 GIF 动画\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"🚀 MC ε-贪心算法启动\".center(70))\n    print(\"=\" * 70)\n    \n    # 初始化\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    returns = defaultdict(list)\n    \n    frames = []\n    frame_interval = NUM_EPISODES // 100  # 每隔多少回合保存一帧\n    \n    # MC ε-贪心主循环\n    for i in range(1, NUM_EPISODES + 1):\n        # 1. 创建/更新 ε-贪心策略\n        policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n        \n        # 2. 生成一个回合\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # 3. 计算回报并更新 Q 值\n        G = 0\n        visited_sa_pairs = set()\n        \n        # 从后向前遍历回合\n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # 首次访问 (First-visit) MC\n            if sa_pair not in visited_sa_pairs:\n                returns[sa_pair].append(G)\n                Q[state, action] = np.mean(returns[sa_pair])\n                visited_sa_pairs.add(sa_pair)\n        \n        # 4. 定期生成并保存帧\n        if i % frame_interval == 0 or i == 1:\n            policy_arrows = get_policy_matrix(policy, GRID_SIZE)\n            print(f\"生成第 {i} 回合的帧...\")\n            frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, i))\n    \n    print(f\"\\n{'=' * 70}\")\n    print(\"🎉 MC ε-贪心算法完成！\".center(70))\n    print(f\"{'=' * 70}\")\n    print(f\"✅ 总计回合数: {NUM_EPISODES}\")\n    \n    # 在结尾多添加几帧以便观察最终结果\n    policy_arrows = get_policy_matrix(create_epsilon_greedy_policy(Q, 0, num_actions), GRID_SIZE) # 最终贪婪策略\n    for _ in range(10):\n        frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, NUM_EPISODES))\n    \n    # 保存 GIF\n    print(f\"\\n正在保存 GIF（共 {len(frames)} 帧）...\")\n    imageio.mimsave(gif_filename, frames, fps=fps, loop=0)\n    \n    print(f\"\\n✅ GIF 动画已保存: {gif_filename}\")\n    print(f\"   - 总帧数: {len(frames)}\")\n    print(f\"   - 帧率: {fps} fps\")\n    print(f\"   - 总回合数: {NUM_EPISODES}\")\n    \n    return Q, policy\n\n# 执行算法\nQ_final, policy_final = run_mc_epsilon_greedy(\n    gif_filename='mc_epsilon_greedy.gif',\n    fps=20\n)\n\n# 显示 GIF\nprint(f\"\\n正在显示 GIF...\")\ndisplay(IPImage(filename='mc_epsilon_greedy.gif'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 总结与分析\n",
    "\n",
    "### 🎯 你学到了什么\n",
    "\n",
    "通过这个教程，你实现了更通用的 MC ε-贪心算法，并克服了 MC Basic 的局限性：\n",
    "\n",
    "1. **ε-贪心策略的重要性**\n",
    "   - 它是保证持续探索的关键，避免了算法过早陷入局部最优。\n",
    "   - 使得算法不再需要“探索性起点”这一不切实际的假设。\n",
    "\n",
    "2. **On-Policy 学习**\n",
    "   - 学习的策略和用于生成数据的策略是同一个。\n",
    "   - 算法在“边探索边学习”的过程中不断优化自身。\n",
    "\n",
    "3. **与 MC Basic 的对比**\n",
    "   - **MC Basic**：概念简单，分为独立的评估和改进两步，但依赖探索性起点。\n",
    "   - **MC ε-贪心**：更实用，将探索融入策略中，通过大量回合逐步收敛。\n",
    "\n",
    "### 🔍 观察结果\n",
    "\n",
    "从生成的 GIF 动画中，你可以观察到：\n",
    "\n",
    "- **Q 值的持续更新**：Q 值随着每个回合的经验而不断波动和优化。\n",
    "- **探索的影响**：即使在学习后期，智能体偶尔也会选择非最优路径，这就是 ε-贪心策略在起作用。\n",
    "- **策略的逐渐收敛**：尽管有探索，但最优动作的概率始终是最高的，因此策略的主体方向会逐渐稳定并指向目标。\n",
    "\n",
    "### 🎓 下一步学习\n",
    "\n",
    "MC 方法需要等待一个完整的回合结束后才能进行学习，这在某些任务中效率较低。下一步，你将学习更高效的无模型方法：\n",
    "\n",
    "1. **时序差分学习 (Temporal-Difference, TD)**\n",
    "   - 无需等待回合结束，每走一步就可以学习。\n",
    "   - 结合了蒙特卡洛（从经验中学习）和动态规划（自举）的优点。\n",
    "   - 核心算法：**SARSA (On-policy)** 和 **Q-learning (Off-policy)**。\n",
    "\n",
    "2. **函数逼近**\n",
    "   - 当状态空间过大时，用表格存储 Q 值不再可行。\n",
    "   - 使用神经网络等模型来近似 Q 函数，即深度强化学习的开端。\n",
    "\n",
    "---\n",
    "\n",
    "**恭喜你掌握了 MC ε-贪心算法，向更高级的强化学习技术又迈进了一大步！** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}