{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC ε-贪心算法教程：更实用的蒙特卡洛方法\n",
    "\n",
    "## 📖 为什么需要 MC ε-贪心？\n",
    "\n",
    "在 `MC Basic` 算法中，我们学习了蒙特卡洛方法的基本原理。但它有一个很强的假设：**探索性起点 (Exploring Starts)**。这意味着为了评估所有状态-动作对，我们需要从每一个可能的 (s,a) 对开始生成回合。\n",
    "\n",
    "在现实世界中，这个假设往往难以满足。例如，在机器人控制任务中，我们可能无法随意设置机器人的初始状态和初始动作。\n",
    "\n",
    "**MC ε-贪心算法**通过引入“探索”机制解决了这个问题，使其更加实用：\n",
    "\n",
    "- **无需探索性起点**：通过 ε-贪心策略在学习过程中进行探索。\n",
    "- **平衡探索与利用**：以 1-ε 的概率选择当前最优动作（利用），以 ε 的概率随机选择一个动作（探索）。\n",
    "- **在线策略 (On-policy)**：用于生成数据的策略和被评估改进的策略是同一个。\n",
    "- **保证收敛**：在一定条件下，它能收敛到最优策略。\n",
    "\n",
    "## 🎯 本教程目标\n",
    "\n",
    "我们将从零开始实现 MC ε-贪心算法，并在同一个 4×4 网格世界中进行可视化：\n",
    "\n",
    "- **核心内容**：\n",
    "  - 理解 ε-贪心策略如何平衡探索与利用。\n",
    "  - 实现基于回合采样的 on-policy 蒙特卡洛控制。\n",
    "  - 可视化 Q 值和策略在探索中的收敛过程。\n",
    "\n",
    "- **学习路径**：\n",
    "  1. 理解 ε-贪心策略。\n",
    "  2. 构建支持随机起始回合的环境。\n",
    "  3. 实现 MC ε-贪心核心算法。\n",
    "  4. 可视化学习过程（GIF 动画）。\n",
    "  5. 分析与 MC Basic 的异同。\n",
    "\n",
    "---\n",
    "\n",
    "让我们开始探索这种更强大的无模型学习方法！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：安装和导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 导入必要的库\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom typing import Optional, Tuple, Dict, Any, List, Annotated\nfrom collections import defaultdict\n\n# 设置 matplotlib 后端（必须在导入 pyplot 之前）\nimport matplotlib\nmatplotlib.use('Agg')  # 使用非交互式后端，适合云端环境\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom IPython.display import display, clear_output, Image as IPImage\nimport imageio\nfrom io import BytesIO\n\n# 配置 matplotlib 中文显示\nplt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(f\"✅ Gymnasium 版本: {gym.__version__}\")\nprint(f\"✅ NumPy 版本: {np.__version__}\")\nprint(f\"✅ Matplotlib 版本: {matplotlib.__version__}\")\nprint(f\"✅ Matplotlib 后端: {matplotlib.get_backend()}\")\nprint(f\"✅ 中文字体配置完成\")\nprint(f\"✅ imageio 已导入，支持 GIF 动画生成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：创建支持 MC 学习的网格世界环境\n",
    "\n",
    "为了适应 ε-贪心算法，我们需要对环境做一些微调。主要区别在于回合的生成方式：\n",
    "\n",
    "- **随机起点**：回合可以从任意非终止状态随机开始。\n",
    "- **策略驱动**：整个回合都由一个给定的策略（即 ε-贪心策略）驱动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GridWorldEpsilonGreedy(gym.Env):\n    \"\"\"支持 MC ε-贪心学习的网格世界环境\"\"\"\n    \n    def __init__(\n        self,\n        size: Annotated[int, \"网格的边长\"] = 4,\n        rewards: Annotated[Optional[np.ndarray], \"可选的奖励矩阵\"] = None,\n    ):\n        super().__init__()\n        self.size = size\n        \n        # 设置奖励矩阵\n        if rewards is None:\n            self.rewards = np.array([\n                [0, 0, -1, -1],\n                [0, -1, -1, 1],\n                [0, -1, 0, 0],\n                [-1, 0, 0, -1]\n            ])\n        else:\n            self.rewards = rewards\n        \n        # 目标位置和障碍物位置\n        self.target_pos = np.argwhere(self.rewards == 1)[0]\n        self.obstacle_mask = (self.rewards == -1)\n        \n        # 动作空间：0=上, 1=下, 2=左, 3=右\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Discrete(size * size)\n        \n        # 动作映射\n        self._action_to_direction = {\n            0: np.array([-1, 0]),  # 上\n            1: np.array([1, 0]),   # 下\n            2: np.array([0, -1]),  # 左\n            3: np.array([0, 1]),   # 右\n        }\n        \n        self._agent_location = np.array([0, 0])\n    \n    def _pos_to_state(\n        self,\n        pos: Annotated[np.ndarray, \"二维位置坐标\"],\n    ) -> Annotated[int, \"对应的一维状态索引\"]:\n        \"\"\"将二维位置转换为一维状态索引\"\"\"\n        return pos[0] * self.size + pos[1]\n    \n    def _state_to_pos(\n        self,\n        state: Annotated[int, \"一维状态索引\"],\n    ) -> Annotated[np.ndarray, \"对应的二维位置坐标\"]:\n        \"\"\"将一维状态索引转换为二维位置\"\"\"\n        return np.array([state // self.size, state % self.size])\n    \n    def reset(\n        self,\n        start_pos: Annotated[Optional[np.ndarray], \"可选的起始位置坐标\"] = None,\n    ) -> Annotated[Tuple[int, Dict[str, Any]], \"初始状态索引及附加信息字典\"]:\n        \"\"\"重置环境，可以指定或随机选择起始位置\"\"\"\n        if start_pos is not None:\n            self._agent_location = np.array(start_pos)\n        else:\n            # 随机选择一个非终止、非障碍的起始位置\n            while True:\n                pos = np.random.randint(0, self.size, size=2)\n                if not np.array_equal(pos, self.target_pos) and not self.obstacle_mask[pos[0], pos[1]]:\n                    self._agent_location = pos\n                    break\n        \n        state = self._pos_to_state(self._agent_location)\n        return state, {}\n    \n    def step(\n        self,\n        action: Annotated[int, \"要执行的动作编号\"],\n    ) -> Annotated[Tuple[int, float, bool, bool, Dict[str, Any]], \"新状态、奖励、终止标志、截断标志及额外信息\"]:\n        \"\"\"执行动作\"\"\"\n        direction = self._action_to_direction[action]\n        new_location = self._agent_location + direction\n        \n        if (0 <= new_location[0] < self.size and 0 <= new_location[1] < self.size):\n            self._agent_location = new_location\n        \n        reward = self.rewards[self._agent_location[0], self._agent_location[1]]\n        terminated = np.array_equal(self._agent_location, self.target_pos)\n        state = self._pos_to_state(self._agent_location)\n        return state, reward, terminated, False, {}\n    \n    def generate_episode(\n        self,\n        policy: Annotated[np.ndarray, \"策略概率矩阵\"],\n        max_steps: Annotated[int, \"单个回合的最大步数\"] = 100,\n    ) -> Annotated[List[Tuple[int, int, float]], \"状态-动作-奖励序列\"]:\n        \"\"\"从随机状态开始，根据策略生成一个完整的回合\"\"\"\n        state, _ = self.reset() # 随机起点\n        episode = []\n        \n        for _ in range(max_steps):\n            action = np.random.choice(self.action_space.n, p=policy[state])\n            next_state, reward, terminated, _, _ = self.step(action)\n            episode.append((state, action, reward))\n            if terminated:\n                break\n            state = next_state\n        \n        return episode\n\n# 创建环境实例\nenv = GridWorldEpsilonGreedy(size=4)\nprint(f\"✅ GridWorldEpsilonGreedy 环境创建完成\")\nprint(f\"   - 网格大小: {env.size}×{env.size}\")\nprint(f\"   - 动作空间: {env.action_space} (0=上, 1=下, 2=左, 3=右)\")\nprint(f\"   - 目标位置: {env.target_pos}\")\nprint(f\"\\n   奖励矩阵:\")\nprint(env.rewards)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：理解 MC ε-贪心算法\n",
    "\n",
    "### 🧮 ε-贪心策略\n",
    "\n",
    "为了确保在学习过程中有足够的探索，我们使用 **ε-贪心策略**。对于任意状态 $s$，策略 $\\pi$ 定义如下：\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \n",
    "\\begin{cases} \n",
    "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \n",
    "\\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a \\neq \\arg\\max_{a'} Q(s,a') \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $1 - \\epsilon$ 是“利用”的概率，即选择当前估计的最好动作。\n",
    "- $\\epsilon$ 是“探索”的概率，即随机选择一个动作。\n",
    "- $|\\mathcal{A}(s)|$ 是状态 $s$ 下的动作数量。\n",
    "\n",
    "### 🔄 On-Policy MC Control 算法流程\n",
    "\n",
    "**初始化**：\n",
    "- 随机初始化 $Q(s,a)$。\n",
    "- 初始化 `returns(s,a)` 为空列表。\n",
    "- 根据初始 Q 值创建一个 ε-贪心策略 $\\pi$。\n",
    "\n",
    "**循环（直到收敛或达到最大回合数）：**\n",
    "\n",
    "1. **生成回合**：使用当前策略 $\\pi$ 从一个随机起点生成一个完整的回合。\n",
    "   - `(S_0, A_0, R_1), (S_1, A_1, R_2), ..., (S_{T-1}, A_{T-1}, R_T)`\n",
    "\n",
    "2. **计算回报**：对于回合中每个时间步 $t=0, 1, ..., T-1$：\n",
    "   - 计算从该步开始的折扣回报 $G_t = R_{t+1} + \\gamma R_{t+2} + ...$\n",
    "\n",
    "3. **更新 Q 值**：对于回合中出现的每一个状态-动作对 $(S_t, A_t)$：\n",
    "   - 将回报 $G_t$ 添加到 `returns(S_t, A_t)` 列表中。\n",
    "   - 更新 $Q(S_t, A_t)$ 为 `returns(S_t, A_t)` 的平均值。\n",
    "\n",
    "4. **更新策略**：根据更新后的 Q 值，改进 ε-贪心策略 $\\pi$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：定义算法参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 算法参数配置完成\n",
      "   - 网格大小: 4×4\n",
      "   - 折扣因子 γ: 0.9\n",
      "   - Epsilon ε: 0.1\n",
      "   - 总回合数: 5000\n",
      "   - 动作空间: ['↑', '↓', '←', '→']\n"
     ]
    }
   ],
   "source": [
    "# --- 算法参数 ---\n",
    "GRID_SIZE = 4           # 网格大小\n",
    "GAMMA = 0.9             # 折扣因子\n",
    "EPSILON = 0.1           # ε-贪心策略中的 ε 值\n",
    "NUM_EPISODES = 5000     # 总共要运行的回合数\n",
    "MAX_STEPS = 100         # 每个回合的最大步数\n",
    "\n",
    "# 动作名称（用于可视化）\n",
    "ACTION_NAMES = ['↑', '↓', '←', '→']\n",
    "\n",
    "print(f\"✅ 算法参数配置完成\")\n",
    "print(f\"   - 网格大小: {GRID_SIZE}×{GRID_SIZE}\")\n",
    "print(f\"   - 折扣因子 γ: {GAMMA}\")\n",
    "print(f\"   - Epsilon ε: {EPSILON}\")\n",
    "print(f\"   - 总回合数: {NUM_EPISODES}\")\n",
    "print(f\"   - 动作空间: {ACTION_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：实现 MC ε-贪心核心算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_epsilon_greedy_policy(\n    Q: Annotated[np.ndarray, \"状态-动作价值表\"],\n    epsilon: Annotated[float, \"ε-贪心策略中的探索概率\"],\n    num_actions: Annotated[int, \"动作空间的大小\"],\n) -> Annotated[np.ndarray, \"ε-贪心策略概率矩阵\"]:\n    \"\"\"根据 Q 值创建一个 ε-贪心策略\"\"\"\n    num_states = Q.shape[0]\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])\n        policy[state, best_action] += (1.0 - epsilon)\n    \n    return policy\n\ndef get_policy_matrix(\n    policy: Annotated[np.ndarray, \"策略概率矩阵\"],\n    grid_size: Annotated[int, \"网格的边长\"],\n) -> Annotated[np.ndarray, \"用于可视化的策略箭头矩阵\"]:\n    \"\"\"将策略转换为箭头矩阵用于可视化\"\"\"\n    policy_arrows = np.empty((grid_size, grid_size), dtype=object)\n    \n    for state in range(grid_size * grid_size):\n        row = state // grid_size\n        col = state % grid_size\n        best_action = np.argmax(policy[state])\n        policy_arrows[row, col] = ACTION_NAMES[best_action]\n    \n    return policy_arrows\n\nprint(\"✅ MC ε-贪心核心函数定义完成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：实现可视化函数\n",
    "\n",
    "可视化函数与 `MC Basic` 基本相同，但标题和迭代计数方式有所调整，以反映是基于回合数进行更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def render_mc_epsilon_greedy_to_array(\n    rewards: Annotated[np.ndarray, \"奖励矩阵\"],\n    Q: Annotated[np.ndarray, \"状态-动作价值表\"],\n    policy_arrows: Annotated[np.ndarray, \"策略箭头矩阵\"],\n    episode_num: Annotated[int, \"当前回合数\"],\n) -> Annotated[np.ndarray, \"用于生成动画的 RGB 数组\"]:\n    \"\"\"\n    将当前 MC ε-贪心状态渲染为 RGB 数组（用于 GIF 生成）\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=80)\n    \n    # 配色方案\n    reward_cmap = LinearSegmentedColormap.from_list('reward', ['red', 'white', 'green'])\n    value_cmap = 'viridis'\n    \n    # 1. 绘制奖励矩阵\n    ax1 = axes[0]\n    im1 = ax1.imshow(rewards, cmap=reward_cmap, vmin=-1, vmax=1)\n    ax1.set_title(f'Reward Matrix', fontsize=14, fontweight='bold')\n    ax1.set_xticks(range(GRID_SIZE))\n    ax1.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax1.text(j, i, f'{rewards[i, j]:.0f}',\n                    ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n    \n    plt.colorbar(im1, ax=ax1, fraction=0.046)\n    \n    # 2. 绘制 Q 值矩阵（显示最大 Q 值）\n    ax2 = axes[1]\n    Q_max = Q.reshape(GRID_SIZE, GRID_SIZE, 4).max(axis=2)\n    im2 = ax2.imshow(Q_max, cmap=value_cmap)\n    ax2.set_title(f'Q-Value Matrix (max) - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax2.set_xticks(range(GRID_SIZE))\n    ax2.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            ax2.text(j, i, f'{Q_max[i, j]:.2f}',\n                    ha=\"center\", va=\"center\", \n                    color=\"white\" if Q_max[i, j] < (Q_max.max()/2 if Q_max.max() > 0 else 0.5) else \"black\",\n                    fontsize=10)\n    \n    plt.colorbar(im2, ax=ax2, fraction=0.046)\n    \n    # 3. 绘制策略矩阵\n    ax3 = axes[2]\n    policy_display = np.zeros((GRID_SIZE, GRID_SIZE))\n    im3 = ax3.imshow(policy_display, cmap='gray', vmin=0, vmax=1, alpha=0.1)\n    ax3.set_title(f'Policy Matrix - Episode {episode_num}', fontsize=14, fontweight='bold')\n    ax3.set_xticks(range(GRID_SIZE))\n    ax3.set_yticks(range(GRID_SIZE))\n    \n    for i in range(GRID_SIZE):\n        for j in range(GRID_SIZE):\n            if policy_arrows[i, j]:\n                ax3.text(j, i, policy_arrows[i, j],\n                        ha=\"center\", va=\"center\", \n                        color=\"blue\", fontsize=24, fontweight='bold')\n    \n    # 整体标题\n    fig.suptitle(f'MC ε-Greedy Algorithm: Episode {episode_num}', \n                 fontsize=16, fontweight='bold', y=0.98)\n    \n    plt.tight_layout()\n    \n    # 转换为 RGB 数组\n    fig.canvas.draw()\n    buf = fig.canvas.buffer_rgba()\n    rgb_array = np.asarray(buf)\n    rgb_array = rgb_array[:, :, :3]  # RGBA to RGB\n    plt.close(fig)\n    \n    return rgb_array\n\nprint(\"✅ 可视化函数定义完成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七步：执行 MC ε-贪心算法并生成 GIF 动画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_mc_epsilon_greedy(\n    gif_filename: Annotated[str, \"输出 GIF 文件名\"] = 'mc_epsilon_greedy.gif',\n    fps: Annotated[int, \"生成动画的帧率\"] = 10,\n) -> Annotated[Tuple[np.ndarray, np.ndarray], \"最终 Q 值和策略矩阵\"]:\n    \"\"\"\n    执行 MC ε-贪心算法并生成 GIF 动画\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"🚀 MC ε-贪心算法启动\".center(70))\n    print(\"=\" * 70)\n    \n    # 初始化\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    returns = defaultdict(list)\n    \n    frames = []\n    frame_interval = NUM_EPISODES // 100  # 每隔多少回合保存一帧\n    \n    # MC ε-贪心主循环\n    for i in range(1, NUM_EPISODES + 1):\n        # 1. 创建/更新 ε-贪心策略\n        policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n        \n        # 2. 生成一个回合\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # 3. 计算回报并更新 Q 值\n        G = 0\n        visited_sa_pairs = set()\n        \n        # 从后向前遍历回合\n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # 首次访问 (First-visit) MC\n            if sa_pair not in visited_sa_pairs:\n                returns[sa_pair].append(G)\n                Q[state, action] = np.mean(returns[sa_pair])\n                visited_sa_pairs.add(sa_pair)\n        \n        # 4. 定期生成并保存帧\n        if i % frame_interval == 0 or i == 1:\n            policy_arrows = get_policy_matrix(policy, GRID_SIZE)\n            print(f\"生成第 {i} 回合的帧...\")\n            frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, i))\n    \n    print(f\"\\n{'=' * 70}\")\n    print(\"🎉 MC ε-贪心算法完成！\".center(70))\n    print(f\"{'=' * 70}\")\n    print(f\"✅ 总计回合数: {NUM_EPISODES}\")\n    \n    # 在结尾多添加几帧以便观察最终结果\n    policy_arrows = get_policy_matrix(create_epsilon_greedy_policy(Q, 0, num_actions), GRID_SIZE) # 最终贪婪策略\n    for _ in range(10):\n        frames.append(render_mc_epsilon_greedy_to_array(env.rewards, Q, policy_arrows, NUM_EPISODES))\n    \n    # 保存 GIF\n    print(f\"\\n正在保存 GIF（共 {len(frames)} 帧）...\")\n    imageio.mimsave(gif_filename, frames, fps=fps, loop=0)\n    \n    print(f\"\\n✅ GIF 动画已保存: {gif_filename}\")\n    print(f\"   - 总帧数: {len(frames)}\")\n    print(f\"   - 帧率: {fps} fps\")\n    print(f\"   - 总回合数: {NUM_EPISODES}\")\n    \n    return Q, policy\n\n# 执行算法\nQ_final, policy_final = run_mc_epsilon_greedy(\n    gif_filename='mc_epsilon_greedy.gif',\n    fps=20\n)\n\n# 显示 GIF\nprint(f\"\\n正在显示 GIF...\")\ndisplay(IPImage(filename='mc_epsilon_greedy.gif'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 📐 算法伪代码与实现对照\n\n## 🔍 理论算法（图19：基于ε-贪心探索的MC方法）\n\n下面的伪代码展示了教材中描述的算法结构：\n\n```\n算法：On-policy MC Control (基于ε-贪心策略)\n\n参数：\n  - ε: 探索概率（小的正数）\n\n初始化：\n  - 对所有 s ∈ S, a ∈ A(s)：\n    * Q(s,a) ← 任意值\n    * Returns(s,a) ← 空列表\n  - π ← 相对于Q的ε-贪心策略\n    \n循环（对每个回合）：\n  (a) 使用策略π生成一个回合：S₀,A₀,R₁,S₁,A₁,R₂,...,S_T-1,A_T-1,R_T\n  (b) G ← 0\n  (c) 循环（对回合中每个时间步，从T-1到0）：\n      * G ← γG + R_{t+1}【折扣回报递推计算】\n      * 除非 (S_t, A_t) 出现在 S₀,A₀,...,S_{t-1},A_{t-1} 中：【First-visit检查】\n        - 将 G 追加到 Returns(S_t, A_t)\n        - Q(S_t, A_t) ← average(Returns(S_t, A_t))\n        - A* ← argmax_a Q(S_t, a)【找到最佳动作】\n        - 对所有 a ∈ A(S_t)：【更新ε-贪心策略】\n          如果 a = A*:\n            π(a|S_t) ← 1 - ε + ε/|A(S_t)|\n          否则:\n            π(a|S_t) ← ε/|A(S_t)|\n```\n\n---\n\n## 💻 本笔记的实现对照\n\n### 📦 **初始化部分** → `run_mc_epsilon_greedy()` 函数开头\n\n**伪代码**：\n```\nQ(s,a) ← 任意值\nReturns(s,a) ← 空列表\nπ ← 相对于Q的ε-贪心策略\n```\n\n**实现代码**：\n```python\n# run_mc_epsilon_greedy() 函数中：\nQ = np.zeros((num_states, num_actions))    # Q(s,a)初始化为0\nreturns = defaultdict(list)                # Returns(s,a)空列表（动态创建）\n# π策略在每个回合开始时创建\n```\n\n---\n\n### 🔄 **主循环** → `run_mc_epsilon_greedy()` 中的 for 循环\n\n**伪代码**：\n```\n循环（对每个回合）\n```\n\n**实现代码**：\n```python\nfor i in range(1, NUM_EPISODES + 1):\n    # 步骤(a): 生成回合\n    policy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n    episode = env.generate_episode(policy, MAX_STEPS)\n    \n    # 步骤(b-c): 更新Q值和策略\n    # ...（见下文详细对应）\n```\n\n> 💡 **关键区别**：与MC Basic不同，这里**每个回合**都更新Q值，而不是迭代所有(s,a)对。\n\n---\n\n### 🎯 **步骤(a)：生成回合** → `create_epsilon_greedy_policy()` + `env.generate_episode()`\n\n**伪代码**：\n```\n使用策略π生成一个回合：S₀,A₀,R₁,...,S_T\n```\n\n**实现代码**：\n\n#### 第1步：创建ε-贪心策略\n\n```python\ndef create_epsilon_greedy_policy(Q, epsilon, num_actions):\n    num_states = Q.shape[0]\n    # 初始化所有动作的基础概率为 ε/|A|\n    policy = np.ones((num_states, num_actions)) * epsilon / num_actions\n    \n    for state in range(num_states):\n        best_action = np.argmax(Q[state])       # A* = argmax_a Q(s,a)\n        policy[state, best_action] += (1.0 - epsilon)  # π(A*|s) = 1-ε+ε/|A|\n    \n    return policy\n```\n\n> 🔑 **ε-贪心策略公式的实现**：\n> - 所有动作先设为 `ε/|A|`（探索部分）\n> - 最佳动作再加上 `1-ε`（利用部分）\n> - 结果：最佳动作概率 = `1-ε+ε/|A|`，其他动作 = `ε/|A|`\n\n#### 第2步：生成回合\n\n```python\n# GridWorldEpsilonGreedy.generate_episode() 方法：\ndef generate_episode(self, policy, max_steps):\n    state, _ = self.reset()  # 🔑 随机起点（非探索性起点）\n    episode = []\n    \n    for _ in range(max_steps):\n        # 根据ε-贪心策略选择动作\n        action = np.random.choice(self.action_space.n, p=policy[state])\n        next_state, reward, terminated, _, _ = self.step(action)\n        episode.append((state, action, reward))\n        if terminated:\n            break\n        state = next_state\n    \n    return episode\n```\n\n> ⚡ **与MC Basic的核心区别**：\n> - ❌ **MC Basic**：从指定的(s,a)开始（探索性起点）\n> - ✅ **MC ε-贪心**：从随机状态开始，整个回合由ε-贪心策略驱动\n\n---\n\n### 🧮 **步骤(b-c)：计算回报并更新Q值** → 主循环中的代码\n\n**伪代码**：\n```\nG ← 0\n循环（从T-1到0）：\n  G ← γG + R_{t+1}\n  除非 (S_t, A_t) 已出现：\n    追加 G 到 Returns(S_t, A_t)\n    Q(S_t, A_t) ← average(Returns(S_t, A_t))\n```\n\n**实现代码**：\n```python\n# 在 run_mc_epsilon_greedy() 的主循环中：\nG = 0\nvisited_sa_pairs = set()  # 跟踪已访问的(s,a)对【First-visit】\n\n# 从后向前遍历回合\nfor state, action, reward in reversed(episode):\n    G = reward + GAMMA * G  # 折扣回报递推计算\n    sa_pair = (state, action)\n    \n    # First-visit MC：只对首次出现的(s,a)更新\n    if sa_pair not in visited_sa_pairs:\n        returns[sa_pair].append(G)                  # 追加回报\n        Q[state, action] = np.mean(returns[sa_pair]) # 更新Q值为平均\n        visited_sa_pairs.add(sa_pair)               # 标记已访问\n```\n\n> 📊 **First-visit MC的实现**：\n> - 使用 `visited_sa_pairs` 集合跟踪已更新的(s,a)对\n> - 只对**首次出现**的(s,a)进行更新\n> - 避免同一个(s,a)在一个回合中被多次计数\n\n---\n\n### 🔄 **步骤(c)：策略更新** → 在主循环开始时自动完成\n\n**伪代码**：\n```\nA* ← argmax_a Q(S_t, a)\n对所有 a ∈ A(S_t)：\n  如果 a = A*: π(a|S_t) ← 1 - ε + ε/|A|\n  否则: π(a|S_t) ← ε/|A|\n```\n\n**实现代码**：\n```python\n# 在每个回合开始时：\npolicy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)\n```\n\n> 💡 **策略更新的隐式实现**：\n> - 不需要显式地在回合结束后更新策略\n> - 因为 `create_epsilon_greedy_policy()` 总是基于**最新的Q值**创建策略\n> - 每个回合开始时重新创建策略，自动反映Q值的变化\n\n---\n\n## 🔑 关键理解点\n\n### 1️⃣ **ε-贪心策略如何消除探索性起点需求？**\n\n**MC Basic的问题**：\n- 需要从每个(s,a)开始生成回合（探索性起点）\n- 实际应用中往往无法控制起点\n\n**MC ε-贪心的解决方案**：\n```python\n# 策略本身就包含探索\npolicy[state, best_action] = 1 - ε + ε/num_actions  # 利用（主要）\npolicy[state, other_action] = ε/num_actions         # 探索（次要）\n```\n\n- ✅ 即使从任意起点开始，ε-贪心策略也会以小概率尝试所有动作\n- ✅ 长期来看，所有(s,a)对都会被访问到\n- ✅ 无需人为控制起点\n\n### 2️⃣ **On-Policy 学习的特点**\n\n**定义**：用于生成数据的策略 = 被评估和改进的策略\n\n**实现体现**：\n```python\n# 同一个ε-贪心策略π既用于：\npolicy = create_epsilon_greedy_policy(Q, EPSILON, num_actions)  # 1. 生成经验\nepisode = env.generate_episode(policy, MAX_STEPS)               # 2. 评估改进\n```\n\n**与MC Basic对比**：\n- MC Basic：评估当前策略，改进为贪婪策略（可以看作off-policy）\n- MC ε-贪心：始终评估和改进同一个ε-贪心策略（on-policy）\n\n### 3️⃣ **为什么需要大量回合？**\n\n```python\nNUM_EPISODES = 5000  # MC ε-贪心需要5000个回合\n```\n\n**原因分析**：\n1. **随机起点**：每个回合只覆盖一部分(s,a)对\n2. **探索概率小**：ε=0.1意味着90%时间在利用，只有10%在探索\n3. **收敛需要**：每个(s,a)需要足够多的样本才能准确估计Q值\n\n**与MC Basic对比**：\n```python\n# MC Basic只需要20次迭代，但每次迭代：\nfor state in range(16):        # 16个状态\n    for action in range(4):    # 4个动作\n        for _ in range(10):    # 每个(s,a)采样10次\n# 总计：20 × 16 × 4 × 10 = 12800 个回合！\n```\n\n---\n\n## 🆚 MC Basic vs MC ε-贪心对比\n\n| 维度 | MC Basic（图18） | MC ε-贪心（图19） |\n|------|-----------------|-------------------|\n| **探索机制** | 探索性起点（遍历所有(s,a)） | ε-贪心策略（内置探索） |\n| **回合起点** | 指定的(s,a)对 | 随机状态 |\n| **策略类型** | 确定性贪婪策略 | 随机ε-贪心策略 |\n| **学习方式** | Off-policy（评估vs改进策略不同） | On-policy（同一策略） |\n| **实际可行性** | 需要控制环境起点（不实用） | 无需控制起点（实用） |\n| **收敛保证** | 探索性起点假设下保证 | ε-soft策略下保证 |\n| **主循环结构** | 外层：迭代；内层：所有(s,a) | 单层：回合数 |\n\n---\n\n## 🎓 算法精髓总结\n\n### ε-贪心的数学美感\n\n$$\n\\pi(a|s) = \n\\begin{cases} \n1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max Q(s,a) \\\\ \n\\frac{\\epsilon}{|A|} & \\text{otherwise}\n\\end{cases}\n$$\n\n**代码实现**：\n```python\npolicy = np.ones((num_states, num_actions)) * epsilon / num_actions  # 基础探索\npolicy[state, best_action] += (1.0 - epsilon)                        # 额外利用\n```\n\n**直觉理解**：\n- 所有动作先分享 `ε` 的总概率（公平探索）\n- 最佳动作再额外获得 `1-ε` 的概率（重点利用）\n- 结果：探索与利用的完美平衡\n\n### First-visit MC的必要性\n\n```python\nif sa_pair not in visited_sa_pairs:  # 为什么需要这个检查？\n```\n\n**原因**：\n- 一个回合中，同一个(s,a)可能出现多次（特别是在有循环的环境中）\n- 如果每次出现都更新，会导致样本不独立\n- First-visit确保每个回合对每个(s,a)只贡献一个样本\n\n---\n\n## ❓ 思考题\n\n现在你理解了MC ε-贪心的完整实现，思考以下问题：\n\n1. **ε的选择**：如果ε=0会怎样？如果ε=1呢？\n   - 提示：思考探索与利用的极端情况\n\n2. **衰减ε**：能否在学习过程中逐渐减小ε？这样做的优缺点是什么？\n   - 提示：早期需要多探索，后期需要多利用\n\n3. **与MC Basic的效率对比**：哪个方法在实际中更高效？\n   - 提示：考虑环境约束和采样成本\n\n4. **收敛性**：为什么ε-贪心策略能保证找到最优策略？\n   - 提示：GLIE（Greedy in the Limit with Infinite Exploration）条件\n\n---\n\n⏸️ **恭喜！你现在完全理解了MC ε-贪心算法的理论与实现。接下来可以学习更高效的时序差分（TD）学习方法！** 🎉"
  },
  {
   "cell_type": "markdown",
   "source": "# 🔢 Q值更新的两种等价方法\n\n## ⚠️ 重要说明：图片算法 vs 本笔记实现\n\n你可能注意到，**图片中的算法使用了不同的Q值更新方式**。让我们详细对比这两种方法。\n\n---\n\n## 📊 图片中的算法：增量更新（Incremental Update）\n\n### 伪代码\n```\n初始化：\n  Q(s,a) ← 0，对所有 s,a\n  N(s,a) ← 0，对所有 s,a  # 访问次数计数器\n  ε ← 1, k ← 1\n  π_k ← ε-贪心(Q)\n\n循环（对每个回合 k）：\n  对第k个回合采样：S₀,A₀,R₁,...,S_T\n  对回合中每个时间步 t：\n    计算 G_t ← R_{t+1} + γR_{t+2} + ...\n    N(S_t, A_t) ← N(S_t, A_t) + 1           # 增加访问次数\n    Q(S_t, A_t) ← Q(S_t, A_t) + (1/N(S_t, A_t)) * (G_t - Q(S_t, A_t))  # 增量更新\n  k ← k + 1\n  ε ← 1/k  # ε衰减\n  π_k ← ε-贪心(Q)\n```\n\n### 🔑 关键要素\n\n#### 1. **N(s,a)：访问次数计数器**\n```python\nN = np.zeros((num_states, num_actions))  # 初始化计数器\n\n# 每次更新时：\nN[state, action] += 1  # 增加访问次数\n```\n\n**作用**：跟踪每个(s,a)对被访问了多少次，用于计算增量更新的步长。\n\n#### 2. **增量更新公式**\n$$\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} \\cdot (G_t - Q(S_t, A_t))\n$$\n\n**公式解析**：\n- `Q(S_t, A_t)`：当前Q值估计\n- `G_t`：新观测到的回报\n- `G_t - Q(S_t, A_t)`：**TD误差**（新样本与当前估计的差异）\n- `1/N(S_t, A_t)`：**学习率**（步长），随着样本增多而减小\n\n**直觉理解**：\n```\n新估计 = 旧估计 + 学习率 × (新样本 - 旧估计)\n       = 旧估计 + 学习率 × 预测误差\n```\n\n如果新样本 > 旧估计，Q值增加；如果新样本 < 旧估计，Q值减小。\n\n---\n\n## 💻 本笔记的实现：批量平均（Batch Average）\n\n### 实现代码\n\n```python\n# 初始化\nreturns = defaultdict(list)  # 存储所有回报样本\n\n# 在主循环中：\nfor state, action, reward in reversed(episode):\n    G = reward + GAMMA * G\n    sa_pair = (state, action)\n    \n    if sa_pair not in visited_sa_pairs:\n        returns[sa_pair].append(G)                  # 📝 存储回报\n        Q[state, action] = np.mean(returns[sa_pair]) # 🧮 计算平均\n        visited_sa_pairs.add(sa_pair)\n```\n\n### 🔑 关键特点\n\n1. **存储所有样本**：`returns[(s,a)]` 列表保存所有历史回报\n2. **重新计算平均**：每次都用 `np.mean()` 计算所有样本的平均值\n\n---\n\n## 🎯 数学等价性证明\n\n### 定理：增量更新 ≡ 批量平均\n\n**设前n个样本的平均值为 $Q_n$**，当收到第 $n+1$ 个新样本 $G_{n+1}$ 时：\n\n#### 批量平均方法：\n$$\nQ_{n+1} = \\frac{1}{n+1} \\sum_{i=1}^{n+1} G_i\n$$\n\n#### 增量更新方法：\n从批量平均公式推导：\n\n$$\n\\begin{align}\nQ_{n+1} &= \\frac{1}{n+1} \\sum_{i=1}^{n+1} G_i \\\\\n&= \\frac{1}{n+1} \\left( \\sum_{i=1}^{n} G_i + G_{n+1} \\right) \\\\\n&= \\frac{1}{n+1} \\left( n \\cdot Q_n + G_{n+1} \\right) \\\\\n&= \\frac{n}{n+1} Q_n + \\frac{1}{n+1} G_{n+1} \\\\\n&= Q_n + \\frac{1}{n+1} (G_{n+1} - Q_n) \\quad \\checkmark\n\\end{align}\n$$\n\n**结论**：两种方法在数学上**完全等价**！\n\n---\n\n## 🔍 具体例子：数值验证\n\n### 场景设置\n假设状态-动作对 $(s_0, a_0)$ 在3个回合中的回报分别为：\n- 第1个回合：$G_1 = 5.0$\n- 第2个回合：$G_2 = 8.0$  \n- 第3个回合：$G_3 = 6.0$\n\n### 方法1：批量平均\n\n```python\nreturns = [5.0, 8.0, 6.0]\nQ = np.mean(returns) = (5.0 + 8.0 + 6.0) / 3 = 6.333...\n```\n\n### 方法2：增量更新\n\n```python\n# 初始化\nQ = 0.0\nN = 0\n\n# 第1个样本 G₁ = 5.0\nN = 1\nQ = Q + (1/N) * (5.0 - Q)\n  = 0.0 + 1.0 * (5.0 - 0.0)\n  = 5.0\n\n# 第2个样本 G₂ = 8.0\nN = 2\nQ = Q + (1/N) * (8.0 - Q)\n  = 5.0 + 0.5 * (8.0 - 5.0)\n  = 5.0 + 1.5\n  = 6.5\n\n# 第3个样本 G₃ = 6.0\nN = 3\nQ = Q + (1/N) * (6.0 - Q)\n  = 6.5 + (1/3) * (6.0 - 6.5)\n  = 6.5 - 0.1666...\n  = 6.333...  ✅ 相同！\n```\n\n**验证成功**：两种方法得到相同结果 `6.333...`\n\n---\n\n## ⚖️ 两种方法的对比\n\n| 维度 | 批量平均（本笔记） | 增量更新（图片算法） |\n|------|-------------------|---------------------|\n| **内存消耗** | O(总样本数) | O(1) |\n| **计算复杂度** | O(N) 每次更新 | O(1) 每次更新 |\n| **实现难度** | 简单（Python一行） | 中等（需要维护N） |\n| **数学等价性** | ✅ 等价 | ✅ 等价 |\n| **适合场景** | 教学、调试、小规模问题 | 实际应用、大规模问题 |\n| **扩展性** | 不适合在线学习 | 适合在线学习 |\n\n---\n\n## 🎓 为什么本笔记选择批量平均？\n\n### ✅ 教学优势\n1. **概念清晰**：直接对应\"平均回报\"的定义\n2. **易于理解**：`np.mean(returns)` 一目了然\n3. **便于调试**：可以查看所有历史样本\n4. **代码简洁**：无需额外的计数器变量\n\n### 📚 教学流程的优先级\n```\n概念理解 > 内存效率\n直观性 > 计算速度\n```\n\n在教学环境中，我们优先让你理解\"Q值是回报的平均值\"这个核心概念，而不是一开始就引入增量更新的技巧。\n\n---\n\n## 🚀 增量更新的实际应用价值\n\n### 为什么实际应用中更常用增量更新？\n\n#### 1. **内存效率**\n```python\n# 批量平均：需要存储所有样本\nreturns = defaultdict(list)  # 可能存储数百万个样本\n\n# 增量更新：只需要O(1)内存\nQ = np.zeros((num_states, num_actions))\nN = np.zeros((num_states, num_actions))\n```\n\n#### 2. **计算效率**\n```python\n# 批量平均：每次O(N)计算\nQ[s, a] = np.mean(returns[(s, a)])  # 遍历所有样本\n\n# 增量更新：每次O(1)计算\nQ[s, a] += (1/N[s, a]) * (G - Q[s, a])  # 常数时间\n```\n\n#### 3. **在线学习**\n- 增量更新可以实时更新，不需要保存历史数据\n- 适合永不停止的连续学习场景（如机器人、游戏AI）\n\n---\n\n## 💡 增量更新的代码实现示例\n\n如果你想按照图片中的算法实现，可以这样修改：\n\n```python\ndef run_mc_epsilon_greedy_incremental(gif_filename='mc_incremental.gif', fps=10):\n    \"\"\"使用增量更新的MC ε-贪心算法\"\"\"\n    \n    # 初始化\n    num_states = GRID_SIZE * GRID_SIZE\n    num_actions = 4\n    \n    Q = np.zeros((num_states, num_actions))\n    N = np.zeros((num_states, num_actions))  # 🔑 访问次数计数器\n    \n    # 主循环\n    for k in range(1, NUM_EPISODES + 1):\n        # 动态调整 ε（如图片算法）\n        epsilon = 1.0 / k  # ε衰减\n        \n        # 创建 ε-贪心策略\n        policy = create_epsilon_greedy_policy(Q, epsilon, num_actions)\n        \n        # 生成回合\n        episode = env.generate_episode(policy, MAX_STEPS)\n        \n        # 计算回报并增量更新 Q 值\n        G = 0\n        visited_sa_pairs = set()\n        \n        for state, action, reward in reversed(episode):\n            G = reward + GAMMA * G\n            sa_pair = (state, action)\n            \n            # First-visit 检查\n            if sa_pair not in visited_sa_pairs:\n                N[state, action] += 1  # 🔑 增加访问次数\n                \n                # 🔑 增量更新公式\n                alpha = 1.0 / N[state, action]  # 学习率 = 1/N\n                Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])\n                \n                visited_sa_pairs.add(sa_pair)\n    \n    return Q\n\n# ✅ 这个版本与图片算法完全一致！\n```\n\n### 🔑 关键修改点\n\n1. **添加 N 计数器**：`N = np.zeros((num_states, num_actions))`\n2. **增量更新公式**：`Q += (1/N) * (G - Q)`\n3. **ε衰减**：`epsilon = 1.0 / k`（可选，图片中有）\n\n---\n\n## 📋 完整对照表：图片算法 vs 本笔记实现\n\n| 图片算法步骤 | 图片伪代码 | 本笔记实现 | 数学等价性 |\n|-------------|-----------|-----------|----------|\n| 初始化Q | `Q(s,a) ← 0` | `Q = np.zeros(...)` | ✅ |\n| 初始化计数器 | `N(s,a) ← 0` | `returns = defaultdict(list)` | ✅（隐式） |\n| 生成回合 | 采样回合 | `env.generate_episode()` | ✅ |\n| 计算回报 | `G_t ← R_{t+1} + γG_{t+1}` | `G = reward + GAMMA * G` | ✅ |\n| 更新访问次数 | `N(s,a) ← N(s,a) + 1` | `len(returns[(s,a)])` | ✅（隐式） |\n| 更新Q值 | `Q += (1/N)*(G-Q)` | `Q = mean(returns)` | ✅ 等价！ |\n| 策略更新 | `π ← ε-贪心(Q)` | `create_epsilon_greedy_policy(Q)` | ✅ |\n\n---\n\n## 🎯 总结与建议\n\n### ✅ 你现在应该理解的关键点\n\n1. **图片算法用增量更新，本笔记用批量平均**\n2. **两种方法数学上完全等价**\n3. **批量平均更直观，增量更新更高效**\n4. **本笔记优先教学清晰性，实际应用优先效率**\n\n### 🚀 学习路径建议\n\n1. **当前阶段**：使用批量平均（本笔记实现）\n   - 理解\"Q值 = 回报平均值\"这个核心概念\n   - 便于调试和可视化\n\n2. **进阶阶段**：学习增量更新\n   - 理解增量公式的推导\n   - 适应更高效的实现方式\n\n3. **实战阶段**：使用增量更新\n   - 处理大规模问题\n   - 在线学习场景\n\n### 💡 下一步\n\n- 如果你想实践增量更新，可以复制上面的 `run_mc_epsilon_greedy_incremental()` 函数\n- 运行并对比两种实现的结果（应该完全相同！）\n- 思考：如果用常数学习率 `α = 0.1` 代替 `1/N`，会有什么影响？\n\n---\n\n⏸️ **理解了Q值更新的两种方法后，你就完全掌握了图片算法与本笔记实现的对应关系！**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}