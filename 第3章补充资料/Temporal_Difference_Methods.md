# 第 7 章

# 时序差分方法

![](images/3%20-%20Chapter%207%20Temporal-Difference%20Methods/0.jpg)  
*图 7.1：我们在本书中的位置*

本章介绍强化学习中的时序差分（TD）方法。与蒙特卡洛（MC）学习类似，TD学习也是无模型的，但由于其增量形式而具有一些优势。通过第6章的铺垫，读者在看到TD学习算法时不会感到陌生。实际上，TD学习算法可以视为求解贝尔曼方程或贝尔曼最优方程的特殊随机算法。

由于本章介绍了相当多的TD算法，我们首先概述这些算法并阐明它们之间的关系。

- **第7.1节**介绍最基础的TD算法，该算法可以估计给定策略的状态价值。在学习其他TD算法前，首先理解这个基础算法非常重要。

- **第7.2节**介绍Sarsa算法，该算法可以估计给定策略的动作价值。此算法可与策略改进步骤结合以寻找最优策略。通过将状态价值估计替换为动作价值估计，可以轻松从第7.1节的TD算法推导出Sarsa算法。

- **第7.3节**介绍$n$步Sarsa算法，这是Sarsa算法的推广形式。我们将展示Sarsa和MC学习是$n$步Sarsa的两个特例。

- **第7.4节**介绍Q学习算法，这是最经典的强化学习算法之一。其他TD算法旨在求解给定策略的贝尔曼方程，而Q学习旨在直接求解贝尔曼最优方程以获得最优策略。

- **第7.5节**对比本章介绍的TD算法，并提供统一的理论视角。

# 7.1 状态值的TD学习

TD学习通常指代一大类强化学习算法。例如，本章介绍的所有算法都属于TD学习的范畴。然而，本节中的TD学习特指一种用于估计状态值的经典算法。

# 7.1.1 算法描述

给定策略 $\pi$，我们的目标是估计所有 $s \in S$ 的状态价值 $v_{\pi}(s)$。假设我们有一些按照 $\pi$ 生成的经历样本 $(s_0, r_1, s_1, \ldots, s_t, r_{t+1}, s_{t+1}, \ldots)$。这里，$t$ 表示时间步。以下 TD 算法可以使用这些样本来估计状态价值：

$$
\begin{array}{rl}
v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t) \Big[ v_t(s_t) - \big( r_{t+1} + \gamma v_t(s_{t+1}) \big) \Big], \\
v_{t+1}(s) = v_t(s), \quad \text{对于所有 } s \neq s_t,
\end{array}
$$

其中 $t = 0, 1, 2, \ldots$。这里，$v_t(s_t)$ 是在时间 $t$ 对 $v_{\pi}(s_t)$ 的估计，$\alpha_t(s_t)$ 是在时间 $t$ 时 $s_t$ 的学习率。

需要注意的是，在时间 $t$，只有被访问状态 $s_t$ 的价值会被更新。未被访问状态 $s \neq s_t$ 的价值保持不变，如 (7.2) 所示。为简洁起见，方程 (7.2) 经常被省略，但应当牢记这一点，因为如果没有这个方程，算法在数学上将是不完整的。

初次见到 TD 学习算法的读者可能会好奇为什么它要这样设计。实际上，它可以被视为求解贝尔曼方程的一种特殊随机逼近算法。为了理解这一点，首先回顾状态价值的定义：

$$
v_{\pi}(s) = \mathbb{E} \big[ R_{t+1} + \gamma G_{t+1} \vert S_t = s \big], \quad s \in \mathcal{S}.
$$

我们可以将 (7.3) 重写为：

$$
v_{\pi}(s) = \mathbb{E} \big[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \big], \quad s \in \mathcal{S}.
$$

这是因为：

$$
\mathbb{E} [ G_{t+1} | S_t = s ] = \sum_a \pi(a|s) \sum_{s^{\prime}} p(s^{\prime}|s,a) v_{\pi}(s^{\prime}) = \mathbb{E} [ v_{\pi}(S_{t+1}) | S_t = s ]
$$

方程 (7.4) 是贝尔曼方程的另一种表达形式。它有时被称为贝尔曼期望方程。

TD 算法可以通过应用 Robbins-Monro 算法（第 6 章）来求解 (7.4) 中的贝尔曼方程而推导出来。感兴趣的读者可以在框 7.1 中查看详细信息。

# 框 7.1：TD 算法的推导

接下来我们将证明，(7.1) 中的 TD 算法可以通过应用 Robbins-Monro 算法求解 (7.4) 得到。

对于状态 $s_{t}$，我们定义函数如下：

$$
g(v_{\pi}(s_{t})) \doteq v_{\pi}(s_{t}) - \mathbb{E}\bigl[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_{t} = s_{t}\bigr].
$$

那么，(7.4) 等价于

$$
g(v_{\pi}(s_{t})) = 0.
$$

我们的目标是使用 Robbins-Monro 算法求解上述方程以获得 $v_{\pi}(s_{t})$。由于我们可以获得 $r_{t+1}$ 和 $s_{t+1}$（它们是 $R_{t+1}$ 和 $S_{t+1}$ 的样本），因此我们能获得的 $g(v_{\pi}(s_{t}))$ 的带噪声观测值为

$$
\begin{array}{rl}
& \tilde{g}(v_{\pi}(s_{t})) = v_{\pi}(s_{t}) - \left[r_{t+1} + \gamma v_{\pi}(s_{t+1})\right] \\
& \qquad = \underbrace{\Bigl(v_{\pi}(s_{t}) - {\mathbb{E}}\bigl[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_{t} = s_{t}\bigr]\Bigr)}_{g(v_{\pi}(s_{t}))} \\
& \qquad + \underbrace{\Bigl({\mathbb{E}}\bigl[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_{t} = s_{t}\bigr] - \bigl[r_{t+1} + \gamma v_{\pi}(s_{t+1})\bigr]\Bigr)}_{\eta}.
\end{array}
$$

因此，用于求解 $g(v_{\pi}(s_{t})) = 0$ 的 Robbins-Monro 算法（第 6.2 节）为

$$
\begin{array}{rl}
& v_{t+1}\big(s_{t}\big) = v_{t}\big(s_{t}\big) - \alpha_{t}\big(s_{t}\big) \tilde{g}\big(v_{t}\big(s_{t}\big)\big) \\
& \qquad = v_{t}\big(s_{t}\big) - \alpha_{t}\big(s_{t}\big) \bigg(v_{t}\big(s_{t}\big) - \big[r_{t+1} + \gamma v_{\pi}\big(s_{t+1}\big)\big]\bigg),
\end{array}
$$

其中 $v_{t}(s_{t})$ 是 $v_{\pi}(s_{t})$ 在时刻 $t$ 的估计值，$\alpha_{t}(s_{t})$ 是学习率。

(7.5) 中的算法与 (7.1) 中的 TD 算法具有相似的表达式。唯一的区别在于，(7.5) 的右侧包含 $v_{\pi}(s_{t+1})$，而 (7.1) 包含 $v_{t}(s_{t+1})$。这是因为 (7.5) 的设计仅是为了估计 $s_{t}$ 的状态值，并假设其他状态的状态值已知。如果我们想要估计所有状态的状态值，那么右侧的 $v_{\pi}(s_{t+1})$ 应替换为 $v_{t}(s_{t+1})$。这样，(7.5) 就与 (7.1) 完全相同。但是，这样的替换是否仍能保证收敛性？答案是肯定的，这将在后续的定理 7.1 中予以证明。

# 7.1.2 属性分析

接下来讨论TD算法的一些重要属性。

首先，我们更仔细地考察TD算法的表达式。具体来说，(7.1)式可以描述为

$$
\underbrace { v _ { t + 1 } ( s _ { t } ) } _ { \mathrm { 新估计值 } } = \underbrace { v _ { t } ( s _ { t } ) } _ { \mathrm { 当前估计值 } } - \alpha _ { t } ( s _ { t } ) \Big [ \overbrace { v _ { t } ( s _ { t } ) - \big ( \underbrace { r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) } _ { \mathrm { TD \ 目标 } \bar { v } _ { t } } \big ) } ^ { \mathrm { TD \ 误差 \ } \delta _ { t } } \Big ] ,
$$

其中

$$
\bar { v } _ { t } \doteq r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } )
$$

被称为$TD$目标，而

$$
\delta _ { t } \doteq v ( s _ { t } ) - \bar { v } _ { t } = v _ { t } \bigl ( s _ { t } \bigr ) - \bigl ( r _ { t + 1 } + \gamma v _ { t } \bigl ( s _ { t + 1 } \bigr ) \bigr )
$$

被称为$TD$误差。可以看出，新的估计值$v _ { t + 1 } ( s _ { t } )$是当前估计值$v _ { t } ( s _ { t } )$与TD误差$\delta _ { t }$的组合：

$\diamond$ 为什么$\bar{v}_t$被称为TD目标？

这是因为$\bar { v } _ { t }$是算法试图将$v ( s _ { t } )$驱动到的目标值。为了理解这一点，将(7.6)式两边同时减去$\bar { v } _ { t }$得到

$$
\begin{array} { r l } & { v _ { t + 1 } ( s _ { t } ) - \bar { v } _ { t } = \big [ v _ { t } ( s _ { t } ) - \bar { v } _ { t } \big ] - \alpha _ { t } ( s _ { t } ) \big [ v _ { t } ( s _ { t } ) - \bar { v } _ { t } \big ] } \\ & { \qquad = [ 1 - \alpha _ { t } ( s _ { t } ) ] \big [ v _ { t } ( s _ { t } ) - \bar { v } _ { t } \big ] . } \end{array}
$$

取上述等式两边的绝对值得到

$$
| v _ { t + 1 } ( s _ { t } ) - \bar { v } _ { t } | = | 1 - \alpha _ { t } ( s _ { t } ) | | v _ { t } ( s _ { t } ) - \bar { v } _ { t } | .
$$

由于$\alpha _ { t } ( s _ { t } )$是一个小的正数，我们有$0 < 1 - \alpha _ { t } ( s _ { t } ) < 1$。因此可得

$$
| v _ { t + 1 } ( s _ { t } ) - \bar { v } _ { t } | < | v _ { t } ( s _ { t } ) - \bar { v } _ { t } | .
$$

上述不等式很重要，因为它表明新值$\boldsymbol { v } _ { t + 1 } ( s _ { t } )$比旧值$v _ { t } ( s _ { t } )$更接近$\bar{v}_t$。因此，该算法在数学上将$v _ { t } ( s _ { t } )$驱动向$\bar{v}_t$。这就是为什么$\bar{v}_t$被称为TD目标。

TD误差的解释是什么？

首先，这个误差被称为时间差分是因为$\delta _ { t } = v _ { t } ( s _ { t } ) - ( r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) )$反映了两个时间步$t$和$t + 1$之间的差异。其次，当状态值估计准确时，TD误差在期望意义下为零。为了理解这一点，当${ \boldsymbol { v } } _ { t } = { \boldsymbol { v } } _ { \pi }$时，TD误差的期望值为

$$
\begin{array} { r l } & { \mathbb { E } [ \delta _ { t } | S _ { t } = s _ { t } ] = \mathbb { E } \big [ v _ { \pi } ( S _ { t } ) - ( R _ { t + 1 } + \gamma v _ { \pi } ( S _ { t + 1 } ) ) | S _ { t } = s _ { t } \big ] } \\ & { \qquad = v _ { \pi } ( s _ { t } ) - \mathbb { E } \big [ R _ { t + 1 } + \gamma v _ { \pi } ( S _ { t + 1 } ) | S _ { t } = s _ { t } \big ] } \\ & { \qquad = 0 . \qquad \mathrm { ~ ( 由于 ~ (7.3) ) ~ } } \end{array}
$$

因此，TD误差不仅反映了两个时间步之间的差异，更重要的是，它还反映了估计值$v _ { t }$与真实状态值$v _ { \pi }$之间的差异。

在更抽象的层面上，TD误差可以解释为新息，它表示从经验样本$( s _ { t } , r _ { t + 1 } , s _ { t + 1 } )$中获得的新信息。TD学习的基本思想是基于新获得的信息来修正我们对状态值的当前估计。新息在许多估计问题中都是基础性的，例如卡尔曼滤波[33,34]。

其次，(7.1)式中的TD算法只能估计给定策略的状态值。为了找到最优策略，我们仍然需要进一步计算动作值，然后进行策略改进。这将在第7.2节中介绍。尽管如此，本节介绍的TD算法对于理解本章中的其他算法非常基础和重要。

第三，虽然TD学习和MC学习都是无模型的，但它们的优缺点是什么？答案总结在表7.1中。

**表7.1：TD学习与MC学习的比较**

| TD学习 | MC学习 |
|-------------|-------------|
| **增量式**：TD学习是增量式的。它可以在接收到一个经验样本后立即更新状态/动作值。 | **非增量式**：MC学习是非增量式的。它必须等到一个回合完全收集完毕。这是因为它必须计算该回合的折扣回报。 |
| **持续任务**：由于TD学习是增量式的，它可以处理回合式任务和持续任务。持续任务可能没有终止状态。 | **回合式任务**：由于MC学习是非增量式的，它只能处理回合式任务，其中回合在有限步数后终止。 |
| **自举**：TD学习使用自举，因为状态/动作值的更新依赖于该值的先前估计。因此，TD学习需要值的初始猜测。 | **非自举**：MC不使用自举，因为它可以直接估计状态/动作值而无需初始猜测。 |
| **低估计方差**：TD的估计方差低于MC，因为它涉及更少的随机变量。例如，要估计一个动作值$q_\pi(S_t, a_t)$，Sarsa仅需要三个随机变量的样本：$R_{t+1}$、$S_{t+1}$、$A_{t+1}$。 | **高估计方差**：MC的估计方差较高，因为涉及许多随机变量。例如，要估计$q_\pi(S_t, a_t)$，我们需要$R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots$的样本。假设每个回合的长度为$L$。假设每个状态具有相同数量的动作$|\mathcal{A}|$。那么，在软策略下，有$|\mathcal{A}|$个可能的后续回合。如果我们仅使用少量回合进行估计，估计方差高也就不足为奇了。 |

# 7.1.3 收敛性分析

下面给出(7.1)中TD算法的收敛性分析。

**定理7.1** (TD学习的收敛性)。给定策略$\pi$，通过(7.1)中的TD算法，若对所有$s \in S$满足$\sum_{t} \alpha_{t}(s) = \infty$且$\sum_{t} \alpha_{t}^{2}(s) < \infty$，则当$t \to \infty$时，$v_{t}(s)$几乎必然收敛到$v_{\pi}(s)$。

以下给出关于$\alpha_{t}$的一些说明。

首先，$\sum_{t} \alpha_{t}(s) = \infty$和$\sum_{t} \alpha_{t}^{2}(s) < \infty$的条件必须对所有$s \in S$成立。注意在时刻$t$，若状态$s$被访问则$\alpha_{t}(s) > 0$，否则$\alpha_{t}(s) = 0$。条件$\sum_{t} \alpha_{t}(s) = \infty$要求状态$s$被访问无限次（或足够多次）。这需要满足探索性起点条件或采用探索性策略，以确保每个状态-动作对都可能被多次访问。

其次，在实践中学习率$\alpha_{t}$通常选择为一个小的正常数。在这种情况下，$\sum_{t} \alpha_{t}^{2}(s) < \infty$的条件不再成立。当$\alpha$为常数时，仍可以证明算法在期望意义下收敛[24, Section 1.5]。

# 框 7.2：定理 7.1 的证明

我们基于第 6 章中的定理 6.3 来证明收敛性。为此，我们首先需要构建一个如定理 6.3 中所述的随机过程。考虑任意状态 $s \in S$。在时间 $t$，根据 (7.1) 中的 TD 算法，有

$$
v_{t+1}(s) = v_t(s) - \alpha_t(s) \Big( v_t(s) - (r_{t+1} + \gamma v_t(s_{t+1})) \Big), \quad \mathrm{若} \ s = s_t,
$$

或

$$
v_{t+1}(s) = v_t(s), \quad \mathrm{若} \ s \neq s_t.
$$

估计误差定义为

$$
\Delta_t(s) \doteq v_t(s) - v_\pi(s),
$$

其中 $v_\pi(s)$ 是策略 $\pi$ 下状态 $s$ 的状态价值。从 (7.7) 两边减去 $v_\pi(s)$ 得到

$$
\begin{array}{rl}
& \Delta_{t+1}(s) = (1 - \alpha_t(s)) \Delta_t(s) + \alpha_t(s) (\underbrace{r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s)}_{\eta_t(s)}) \\
& \qquad = (1 - \alpha_t(s)) \Delta_t(s) + \alpha_t(s) \eta_t(s), \qquad s = s_t.
\end{array}
$$

从 (7.8) 两边减去 $v_\pi(s)$ 得到

$$
\Delta_{t+1}(s) = \Delta_t(s) = (1 - \alpha_t(s)) \Delta_t(s) + \alpha_t(s) \eta_t(s), \qquad s \neq s_t,
$$

其表达式与 (7.9) 相同，只是 $\alpha_t(s) = 0$ 且 $\eta_t(s) = 0$。因此，无论 $s$ 是否等于 $s_t$，我们得到以下统一表达式：

$$
\Delta_{t+1}(s) = (1 - \alpha_t(s)) \Delta_t(s) + \alpha_t(s) \eta_t(s).
$$

这就是定理 6.3 中的过程。我们的目标是证明定理 6.3 中的三个条件得到满足，因此该过程收敛。

第一个条件在定理 7.1 中已假设成立。接下来我们证明第二个条件成立。即，对于所有 $s \in S$，有 $\| \mathbb{E} [ \eta_t(s) \vert \mathcal{H}_t ] \|_\infty \leq \gamma \| \Delta_t(s) \|_\infty$。这里，$\mathcal{H}_t$ 表示历史信息（参见定理 6.3 中的定义）。由于马尔可夫性质，一旦给定 $s$，$\eta_t(s) = r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s)$ 或 $\eta_t(s) = 0$ 就不依赖于历史信息。因此，我们有 $\mathbb{E} [ \eta_t(s) \vert \mathcal{H}_t ] = \mathbb{E} [ \eta_t(s) ]$。对于 $s \neq s_t$，我们有 $\eta_t(s) = 0$。那么显然有

$$
| \mathbb{E} [ \eta_t(s) ] | = 0 \leq \gamma \| \Delta_t(s) \|_\infty.
$$

对于 $s = s_t$，我们有

$$
\begin{array}{rl}
& \mathbb{E} [ \eta_t(s) ] = \mathbb{E} [ \eta_t(s_t) ] \\
& \quad \quad = \mathbb{E} [ r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s_t) \vert s_t ] \\
& \quad \quad = \mathbb{E} [ r_{t+1} + \gamma v_t(s_{t+1}) \vert s_t ] - v_\pi(s_t).
\end{array}
$$

由于 $v_\pi(s_t) = \mathbb{E} [ r_{t+1} + \gamma v_\pi(s_{t+1}) | s_t ]$，上述等式意味着

$$
\begin{array}{rl}
& \mathbb{E} [ \eta_t(s) ] = \gamma \mathbb{E} [ v_t(s_{t+1}) - v_\pi(s_{t+1}) | s_t ] \\
& \quad \quad = \gamma \displaystyle \sum_{s^\prime \in S} p(s^\prime | s_t) [ v_t(s^\prime) - v_\pi(s^\prime) ].
\end{array}
$$

由此可得

$$
\begin{array}{rl}
& \| \mathbb{E} [ \eta_t(s) ] | = \gamma \left| \displaystyle \sum_{s^\prime \in S} p(s^\prime | s_t) [ v_t(s^\prime) - v_\pi(s^\prime) ] \right| \\
& \qquad \le \gamma \displaystyle \sum_{s^\prime \in S} p(s^\prime | s_t) \operatorname*{max}_{s^\prime \in S} | v_t(s^\prime) - v_\pi(s^\prime) | \\
& \qquad = \gamma \displaystyle \operatorname*{max}_{s^\prime \in S} | v_t(s^\prime) - v_\pi(s^\prime) | \\
& \qquad = \gamma \| v_t(s^\prime) - v_\pi(s^\prime) \|_\infty \\
& \qquad = \gamma \| \Delta_t(s) \|_\infty.
\end{array}
$$

因此，在时间 $t$，我们从 (7.10) 和 (7.11) 可知，对于所有 $s \in S$，无论 $s$ 是否等于 $s_t$，都有 $| \mathbb{E} [ \eta_t(s) ] | \leq \gamma \| \Delta_t(s) \|_\infty$。因此，

$$
\begin{array}{r}
\| \mathbb{E} [ \eta_t(s) ] \|_\infty \leq \gamma \| \Delta_t(s) \|_\infty,
\end{array}
$$

这就是定理 6.3 中的第二个条件。最后，关于第三个条件，对于 $s = s_t$，我们有 $\mathrm{Var} [ \eta_t(s) | \mathcal{H}_t ] = \mathrm{var} [ r_{t+1} + \gamma v_t(s_{t+1}) - v_\pi(s_t) | s_t ] = \mathrm{var} [ r_{t+1} + \gamma v_t(s_{t+1}) | s_t ]$，而对于 $s \neq s_t$，有 $\mathrm{var} [ \eta_t(s) | \mathcal{H}_t ] = 0$。由于 $r_{t+1}$ 是有界的，第三个条件不难证明。

上述证明灵感来源于 [32]。

# 7.2 动作价值的时序差分学习：Sarsa

第7.1节介绍的TD算法只能估计状态价值。本节将介绍另一种名为Sarsa的TD算法，它可以直接估计动作价值。估计动作价值非常重要，因为它可以与策略改进步骤结合来学习最优策略。

# 7.2.1 算法描述

给定策略 $\pi$，我们的目标是估计动作值。假设我们有一些按照 $\pi$ 生成的经验样本：$(s_0, a_0, r_1, s_1, a_1, \ldots, s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \ldots)$。我们可以使用以下 Sarsa 算法来估计动作值：

$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[ q_t(s_t, a_t) - (r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})) \right], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \text{对于所有 } (s, a) \neq (s_t, a_t),
\end{aligned}
$$

其中 $t = 0, 1, 2, \ldots$，$\alpha_t(s_t, a_t)$ 是学习率。这里，$q_t(s_t, a_t)$ 是对 $q_\pi(s_t, a_t)$ 的估计。在时间 $t$，只有 $(s_t, a_t)$ 的 q 值被更新，而其他的 q 值保持不变。

下面讨论 Sarsa 算法的一些重要特性。

## 为什么这个算法被称为 "Sarsa"？

这是因为算法的每次迭代都需要 $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$。Sarsa 是状态-动作-奖励-状态-动作（state-action-reward-state-action）的缩写。Sarsa 算法首次在 [35] 中提出，其名称由 [3] 创造。

> 为什么 Sarsa 要这样设计？有人可能已经注意到 Sarsa 与 (7.1) 中的 TD 算法相似。实际上，通过将状态值估计替换为动作值估计，可以很容易地从 TD 算法得到 Sarsa。

> Sarsa 在数学上做了什么？与 (7.1) 中的 TD 算法类似，Sarsa 是一个用于求解给定策略的贝尔曼方程的随机近似算法：

$$
q_\pi(s, a) = \mathbb{E} \left[ R + \gamma q_\pi(S', A') \mid s, a \right], \quad \text{对于所有 } (s, a).
$$

方程 (7.13) 是用动作值表示的贝尔曼方程。证明在框 7.3 中给出。

# 框文 7.3：证明 (7.13) 是贝尔曼方程

如第 2.8.2 节所述，以动作值表示的贝尔曼方程为

$$
\begin{aligned}
q_{\pi}(s,a) &= \sum_{r} r p(r|s,a) + \gamma \sum_{s^{\prime}} \sum_{a^{\prime}} q_{\pi}(s^{\prime},a^{\prime}) p(s^{\prime}|s,a) \pi(a^{\prime}|s^{\prime}) \\
&= \sum_{r} r p(r|s,a) + \gamma \sum_{s^{\prime}} p(s^{\prime}|s,a) \sum_{a^{\prime}} q_{\pi}(s^{\prime},a^{\prime}) \pi(a^{\prime}|s^{\prime})
\end{aligned}
$$

该方程建立了动作值之间的关系。由于

$$
\begin{aligned}
p(s^{\prime},a^{\prime}|s,a) &= p(s^{\prime}|s,a) p(a^{\prime}|s^{\prime},s,a) \\
&= p(s^{\prime}|s,a) p(a^{\prime}|s^{\prime}) \quad \text{(由于条件独立性)} \\
&\doteq p(s^{\prime}|s,a) \pi(a^{\prime}|s^{\prime})
\end{aligned}
$$

(7.14) 可以重写为

$$
q_{\pi}(s,a) = \sum_{r} r p(r|s,a) + \gamma \sum_{s^{\prime}} \sum_{a^{\prime}} q_{\pi}(s^{\prime},a^{\prime}) p(s^{\prime},a^{\prime}|s,a)
$$

根据期望值的定义，上述方程等价于 (7.13)。  
因此，(7.13) 是贝尔曼方程。

Sarsa 算法是否收敛？由于 Sarsa 是 (7.1) 中 TD 算法的动作值版本，其收敛结果与定理 7.1 类似，如下所示。

**定理 7.2** (Sarsa 的收敛性)。给定策略 $\pi$，通过 (7.12) 中的 Sarsa 算法，若对所有 $(s,a)$ 满足 $\sum_{t} \alpha_{t}(s,a) = \infty$ 且 $\sum_{t} \alpha_{t}^{2}(s,a) < \infty$，则当 $t \to \infty$ 时，$q_{t}(s,a)$ 几乎必然收敛到动作值 $q_{\pi}(s,a)$。

证明过程与定理 7.1 类似，此处省略。$\sum_{t} \alpha_{t}(s,a) = \infty$ 和 $\sum_{t} \alpha_{t}^{2}(s,a) < \infty$ 的条件应对所有 $(s,a)$ 成立。特别地，$\sum_{t} \alpha_{t}(s,a) = \infty$ 要求每个状态-动作对必须被访问无限次（或足够多次）。在时刻 $t$，若 $(s,a) = (s_{t},a_{t})$，则 $\alpha_{t}(s,a) > 0$；否则 $\alpha_{t}(s,a) = 0$。

# 7.2.2 通过Sarsa学习最优策略

(7.12)中的Sarsa算法只能估计给定策略的动作值。为了找到最优策略，我们可以将其与策略改进步骤相结合。这种组合也常被称为Sarsa，其实现过程在算法7.1中给出。

**算法7.1：通过Sarsa学习最优策略**

**初始化**：
- 对所有(s,a)和所有t ∈ (0,1)，设置α_t(s,a) = α > 0
- 对所有(s,a)，初始化q₀(s,a)
- 从q₀导出初始ε-贪婪策略π₀

**目标**：学习一个能够使智能体从初始状态s₀到达目标状态的最优策略。

**对每个回合执行**：
1. 根据π₀(s₀)在s₀生成a₀
2. 如果s_t (t = 0,1,2,...)不是目标状态，执行：
   - 收集经验样本(r_{t+1}, s_{t+1}, a_{t+1})：通过与环境交互生成r_{t+1}, s_{t+1}；根据π_t(s_{t+1})生成a_{t+1}
   - 更新(s_t, a_t)的q值：
     ```
     q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - α_t(s_t,a_t)[q_t(s_t,a_t) - (r_{t+1} + q_t(s_{t+1},a_{t+1}))]
     ```
   - 更新s_t的策略：
     ```
     如果a = argmax_a q_{t+1}(s_t,a)，则π_{t+1}(a|s_t) = 1 - ε + ε/|A(s_t)|
     否则π_{t+1}(a|s_t) = ε/|A(s_t)|
     ```

如算法7.1所示，每次迭代有两个步骤。第一步是更新访问过的状态-动作对的q值。第二步是将策略更新为ε-贪婪策略。q值更新步骤只更新在时间$t$访问的单个状态-动作对。随后，$s_{t}$的策略会立即更新。因此，我们在更新策略之前没有充分评估给定策略。这是基于广义策略迭代的思想。此外，策略更新后，会立即使用该策略生成下一个经验样本。这里的策略是ε-贪婪的，因此具有探索性。

图7.2展示了一个模拟示例来演示Sarsa算法。与本书中见过的所有任务不同，这里的任务旨在找到从特定起始状态到目标状态的最优路径。它并不旨在为所有状态找到最优策略。这种任务在实践中经常遇到，其中起始状态（例如家）和目标状态（例如工作场所）是固定的，我们只需要找到连接它们的最优路径。这个任务相对简单，因为我们只需要探索靠近路径的状态，而不需要探索所有状态。然而，如果我们不探索所有状态，最终路径可能是局部最优而非全局最优。

下面讨论模拟设置和模拟结果。

## 模拟设置

在这个例子中，所有回合都从左上角状态开始，并在目标状态终止。奖励设置如下：
- $r_{\mathrm{目标}} = 0$
- $r_{\mathrm{禁止}} = r_{\mathrm{边界}} = -10$
- $r_{\mathrm{其他}} = -1$

此外，对所有$t$，α_t(s,a) = 0.1，且ε = 0.1。动作值的初始猜测为对所有(s,a)，q₀(s,a) = 0。初始策略具有均匀分布：对所有s,a，π₀(a|s) = 0.2。

![](images/3 - Chapter 7 Temporal-Difference Methods/1.jpg)

**图7.2**：演示Sarsa的示例。所有回合从左上角状态开始，在到达目标状态（蓝色单元格）时终止。目标是找到从起始状态到目标状态的最优路径。奖励设置为$r_{\mathrm{目标}} = 0$，$r_{\mathrm{禁止}} = r_{\mathrm{边界}} = -10$，以及$r_{\mathrm{其他}} = -1$。学习率α = 0.1，ε值为0.1。左图显示了算法获得的最终策略。右图显示了每个回合的总奖励和长度。

## 学习到的策略

图7.2中的左图显示了Sarsa学习到的最终策略。可以看出，该策略能够成功地从起始状态引导至目标状态。然而，其他一些状态的策略可能不是最优的。这是因为其他状态没有被充分探索。

## 每个回合的总奖励

图7.2右上角的子图显示了每个回合的总奖励。这里，总奖励是所有即时奖励的非折扣和。可以看出，每个回合的总奖励逐渐增加。这是因为初始策略不好，因此经常获得负奖励。随着策略变好，总奖励增加。

## 每个回合的长度

图7.2右下角的子图显示每个回合的长度逐渐下降。这是因为初始策略不好，可能在到达目标之前绕很多弯路。随着策略变好，轨迹长度变短。值得注意的是，一个回合的长度可能会突然增加（例如第460个回合），相应的总奖励也会急剧下降。这是因为策略是ε-贪婪的，有可能采取非最优动作。解决这个问题的一种方法是使用衰减的ε，其值逐渐收敛到零。

最后，Sarsa也有一些变体，如期望Sarsa。感兴趣的读者可以查阅框7.4。

# 框 7.4：期望 Sarsa

给定策略 $\pi$，其动作值可以通过期望 Sarsa 进行评估，这是 Sarsa 的一种变体。期望 Sarsa 算法如下：

$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \Big[ q_t(s_t, a_t) - (r_{t+1} + \gamma \mathbb{E}[q_t(s_{t+1}, A)]) \Big], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \text{对所有 } (s, a) \neq (s_t, a_t),
\end{aligned}
$$

其中

$$
\mathbb{E}[q_t(s_{t+1}, A)] = \sum_a \pi_t(a|s_{t+1}) q_t(s_{t+1}, a) \doteq v_t(s_{t+1})
$$

是在策略 $\pi_t$ 下 $q_t(s_{t+1}, a)$ 的期望值。

期望 Sarsa 算法的表达式与 Sarsa 非常相似。它们仅在 TD 目标方面有所不同。具体来说，期望 Sarsa 中的 TD 目标是 $r_{t+1} + \gamma \mathbb{E}[q_t(s_{t+1}, A)]$，而 Sarsa 的 TD 目标是 $r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。由于该算法涉及期望值，因此被称为期望 Sarsa。虽然计算期望值可能会略微增加计算复杂度，但从减少估计方差的角度来看是有益的，因为它将 Sarsa 中的随机变量从 $\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\}$ 减少到 $\{s_t, a_t, r_{t+1}, s_{t+1}\}$。

与 (7.1) 中的 TD 学习算法类似，期望 Sarsa 可以视为求解以下方程的随机逼近算法：

$$
q_{\pi}(s, a) = \mathbb{E} \Big[ R_{t+1} + \gamma \mathbb{E}[q_{\pi}(S_{t+1}, A_{t+1}) | S_{t+1}] \Big| S_t = s, A_t = a \Big], \quad \text{对所有 } s, a.
$$

上述方程初看可能有些奇怪。实际上，它是贝尔曼方程的另一种表达形式。为了理解这一点，将

$$
\mathbb{E}[q_{\pi}(S_{t+1}, A_{t+1}) | S_{t+1}] = \sum_{A'} q_{\pi}(S_{t+1}, A') \pi(A'|S_{t+1}) = v_{\pi}(S_{t+1})
$$

代入 (7.15) 可得

$$
q_{\pi}(s, a) = \mathbb{E} \Big[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \vert S_t = s, A_t = a \Big],
$$

这显然是贝尔曼方程。

期望 Sarsa 的实现与 Sarsa 类似。更多细节可以在 [3,36,37] 中找到。

# 7.3 动作价值的 TD 学习：$n$ 步 Sarsa

本节介绍 $n$ 步 Sarsa，它是 Sarsa 的一种扩展。我们将看到 Sarsa 和 MC 学习是 $n$ 步 Sarsa 的两个极端情况。

回顾动作价值的定义：

$$
q_{\pi}(s,a) = \mathbb{E}[G_t | S_t = s, A_t = a],
$$

其中 $G_t$ 是满足以下条件的折扣回报：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots.
$$

实际上，$G_t$ 也可以分解为不同形式：

$$
\begin{array}{rcl}
\mathrm{Sarsa} \longleftarrow & \mathcal{G}_t^{(1)} = R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}), & \\
& \mathcal{G}_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}), & \\
\vdots & & \\
n\mathrm{-步\ Sarsa} \longleftarrow & \mathcal{G}_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_{\pi}(S_{t+n}, A_{t+n}), & \\
& & \vdots \\
\mathrm{MC} \longleftarrow & \mathcal{G}_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots &
\end{array}
$$

需要注意的是 $G_t = G_t^{(1)} = G_t^{(2)} = G_t^{(n)} = G_t^{(\infty)}$，其中上标仅表示 $G_t$ 的不同分解结构。

将 $G_t^{(n)}$ 的不同分解代入 (7.16) 中的 $q_{\pi}(s,a)$ 会产生不同的算法。

$\diamond$ 当 $n = 1$ 时，我们有

$$
q_{\pi}(s,a) = \mathbb{E}[G_t^{(1)} | s,a] = \mathbb{E}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | s,a].
$$

求解该方程的相应随机近似算法是

$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t) \Big[ q_t(s_t,a_t) - (r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})) \Big],
$$

这就是 (7.12) 中的 Sarsa 算法。

$\diamond$ 当 $n = \infty$ 时，我们有

$$
q_{\pi}(s,a) = \mathbb{E}[G_t^{(\infty)} | s,a] = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | s,a].
$$

求解该方程的相应算法是

$$
q_{t+1}(s_t,a_t) = g_t \doteq r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots,
$$

其中 $g_t$ 是 $G_t$ 的一个样本。实际上，这就是 MC 学习算法，它使用从 $(s_t,a_t)$ 开始的一个回合的折扣回报来近似 $(s_t,a_t)$ 的动作价值。

$\diamond$ 对于一般的 $n$ 值，我们有

$$
q_{\pi}(s,a) = \mathbb{E}[G_t^{(n)} | s,a] = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_{\pi}(S_{t+n}, A_{t+n}) | s,a].
$$

求解上述方程的相应算法是

$$
\begin{array}{rl}
q_{t+1}(s_t,a_t) = & q_t(s_t,a_t) \\
& - \alpha_t(s_t,a_t) \Big[ q_t(s_t,a_t) - \big( r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^n q_t(s_{t+n},a_{t+n}) \big) \Big].
\end{array}
$$

该算法称为 $n$ 步 Sarsa。

总之，$n$ 步 Sarsa 是一种更通用的算法，因为当 $n = 1$ 时它变为（一步）Sarsa 算法，而当 $n = \infty$ 时（通过设置 $\alpha_t = 1$）它变为 MC 学习算法。

为了在 (7.17) 中实现 $n$ 步 Sarsa 算法，我们需要经验样本 $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1},\dots,r_{t+n},s_{t+n},a_{t+n})$。由于 $(r_{t+n},s_{t+n},a_{t+n})$ 在时间 $t$ 尚未收集到，我们必须等到时间 $t+n$ 才能更新 $(s_t,a_t)$ 的 q 值。为此，(7.17) 可以重写为

$$
\begin{array}{c}
q_{t+n}(s_t,a_t) = q_{t+n-1}(s_t,a_t) \\
- \alpha_{t+n-1}(s_t,a_t) \Big[ q_{t+n-1}(s_t,a_t) - (r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^n q_{t+n-1}(s_{t+n},a_{t+n})) \Big],
\end{array}
$$

其中 $q_{t+n}(s_t,a_t)$ 是在时间 $t+n$ 对 $q_{\pi}(s_t,a_t)$ 的估计。

由于 $n$ 步 Sarsa 包含 Sarsa 和 MC 学习作为两个极端情况，因此 $n$ 步 Sarsa 的性能介于 Sarsa 和 MC 学习之间也就不足为奇了。具体来说，如果 $n$ 选择为一个较大的数，$n$ 步 Sarsa 接近 MC 学习：估计值具有相对较高的方差但偏差较小。如果 $n$ 选择较小，$n$ 步 Sarsa 接近 Sarsa：估计值具有相对较大的偏差但方差较低。最后，这里介绍的 $n$ 步 Sarsa 算法仅用于策略评估。它必须与策略改进步骤结合才能学习最优策略。其实现与 Sarsa 类似，此处省略。感兴趣的读者可以查阅 [3, 第 7 章] 以了解多步 TD 学习的详细分析。

# 7.4 最优动作价值的时序差分学习：Q学习

本节我们将介绍Q学习算法，这是最经典的强化学习算法之一[38,39]。回顾之前内容，Sarsa只能估计给定策略的动作价值，必须结合策略改进步骤才能找到最优策略。相比之下，Q学习可以直接估计最优动作价值并找到最优策略。

# 7.4.1 算法描述

Q-learning 算法如下：

$$
\begin{aligned}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma \max_{a \in A(s_{t+1})} q_t(s_{t+1}, a) \right) \right], \\
q_{t+1}(s, a) &= q_t(s, a), \quad \text{对于所有 } (s, a) \neq (s_t, a_t),
\end{aligned}
$$

其中 $t = 0, 1, 2, \ldots$。这里，$q_t(s_t, a_t)$ 是 $(s_t, a_t)$ 的最优动作值估计，$\alpha_t(s_t, a_t)$ 是 $(s_t, a_t)$ 的学习率。

Q-learning 的表达式与 Sarsa 相似。它们仅在 TD 目标方面有所不同：Q-learning 的 TD 目标是 $r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)$，而 Sarsa 的是 $r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。此外，给定 $(s_t, a_t)$，Sarsa 在每次迭代中需要 $(r_{t+1}, s_{t+1}, a_{t+1})$，而 Q-learning 仅需要 $(r_{t+1}, s_{t+1})$。

为什么 Q-learning 被设计为 (7.18) 中的表达式？它在数学上实现了什么？Q-learning 是用于求解以下方程的随机逼近算法：

$$
q(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_a q(S_{t+1}, a) \Big| S_t = s, A_t = a \right].
$$

这是用动作值表示的贝尔曼最优方程。证明见框 7.5。Q-learning 的收敛性分析与定理 7.1 类似，此处省略。更多信息可参考文献 [32,39]。

# 框 7.5：证明 (7.19) 是贝尔曼最优方程

根据期望的定义，(7.19) 可以改写为

$$
q(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) \max_{a \in A(s')} q(s',a).
$$

对等式两边取最大值可得

$$
\max_{a \in \mathcal{A}(s)} q(s,a) = \max_{a \in \mathcal{A}(s)} \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) \max_{a \in \mathcal{A}(s')} q(s',a) \right].
$$

通过记 $v(s) \doteq \max_{a \in \mathcal{A}(s)} q(s,a)$，我们可以将上述方程重写为

$$
\begin{aligned}
v(s) &= \max_{a \in A(s)} \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right] \\
&= \max_{\pi} \sum_{a \in A(s)} \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right],
\end{aligned}
$$

这显然就是第 3 章介绍的状态值形式的贝尔曼最优方程。

# 7.4.2 离策略与同策略

接下来我们介绍两个重要概念：同策略学习和离策略学习。与其他时序差分算法相比，Q学习的一个稍显特殊之处在于它是离策略的，而其他算法则是同策略的。

任何强化学习任务中都存在两种策略：行为策略和目标策略。行为策略用于生成经验样本，目标策略则通过不断更新最终收敛到最优策略。当行为策略与目标策略相同时，这种学习过程称为同策略；当二者不同时，则称为离策略。

离策略学习的优势在于能够基于其他策略生成的经验样本来学习最优策略，例如这些样本可能来自人类操作者执行的策略。一个重要场景是，可以选择具有探索性的行为策略。比如，若要估计所有状态-动作对的动作值，必须生成访问每个状态-动作对足够多次的幕序列。虽然Sarsa使用$\epsilon$-贪婪策略来保持一定的探索能力，但$\epsilon$值通常较小，因此探索能力有限。相比之下，如果能使用具有强探索能力的策略生成幕序列，再通过离策略学习来获取最优策略，将显著提升学习效率。

判断算法属于同策略还是离策略，可以从两个维度进行考察：一是算法旨在解决的数学问题，二是算法所需的经验样本。

具体原因如下：Sarsa在每次迭代中包含两个步骤。第一步是通过求解贝尔曼方程来评估策略$\pi$，这需要由$\pi$生成的样本，因此$\pi$是行为策略。第二步是基于$\pi$的估计值获取改进策略，因此$\pi$又是不断更新并最终收敛到最优策略的目标策略。由此可见，行为策略与目标策略完全相同。

从另一角度观察算法所需的样本：Sarsa每次迭代需要样本$\left(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1}\right)$，其生成过程如下所示：

$$
s_{t} \xrightarrow[]{\pi_{b}} a_{t} \xrightarrow[]{\mathrm{model}} r_{t+1}, s_{t+1} \xrightarrow[]{\pi_{b}} a_{t+1}
$$

可见行为策略$\pi_{b}$负责在$s_{t}$处生成$a_{t}$，在$s_{t+1}$处生成$a_{t+1}$。Sarsa算法旨在估计目标策略$\pi_{T}$在$(s_{t}, a_{t})$处的动作值，而该策略会基于估计值在每次迭代中改进。实际上$\pi_{T}$与$\pi_{b}$相同，因为对$\pi_{T}$的评估依赖于样本$(r_{t+1}, s_{t+1}, a_{t+1})$，其中$a_{t+1}$正是遵循$\pi_{b}$生成的。换言之，Sarsa评估的策略就是生成样本所用的策略。

Q学习是离策略算法。

根本原因在于Q学习是求解贝尔曼最优方程的算法，而Sarsa是求解给定策略的贝尔曼方程的算法。求解贝尔曼方程可以评估对应策略，而求解贝尔曼最优方程能直接得到最优值和最优策略。

具体而言，Q学习每次迭代所需样本为$\left(s_{t}, a_{t}, r_{t+1}, s_{t+1}\right)$，其生成过程如下：

$$
s_{t} \xrightarrow{\pi_{b}} a_{t} \xrightarrow{\mathrm{model}} r_{t+1}, s_{t+1}
$$

可见行为策略$\pi_{b}$负责在$s_{t}$处生成$a_{t}$。Q学习算法旨在估计$(s_{t}, a_{t})$的最优动作值，该估计过程依赖于样本$(r_{t+1}, s_{t+1})$。而$(r_{t+1}, s_{t+1})$的生成不涉及$\pi_{b}$，因为它由系统模型（或与环境交互）决定。因此对$(s_{t}, a_{t})$最优动作值的估计与$\pi_{b}$无关，我们可以使用任意$\pi_{b}$在$s_{t}$处生成$a_{t}$。此外，这里的目标策略$\pi_{T}$是基于估计最优值得到的贪婪策略（算法7.3），行为策略不必与$\pi_{T}$相同。

蒙特卡洛学习属于同策略。其原理与Sarsa类似：待评估和改进的目标策略与生成样本的行为策略相同。

另一个易与同策略/离策略混淆的概念是在线/离线学习。在线学习指智能体在与环境交互的同时更新值和策略；离线学习指智能体使用预先收集的经验数据更新值和策略，无需与环境交互。若算法是同策略的，则可以在线方式实现，但不能使用其他策略生成的预收集数据；若算法是离策略的，则既可以在线实现也可离线实现。

# 算法 7.2：通过 $\mathbf{Q}$ 学习的最优策略学习（同策略版本）

**初始化：**
- 对所有 $(s, a)$ 和所有 $t$，设置 $\alpha_{t}(s, a) = \alpha > 0$
- $\epsilon \in (0, 1)$
- 对所有 $(s, a)$ 设置初始 $q_{0}(s, a)$
- 从 $q_{0}$ 推导出初始 $\epsilon$-贪婪策略 $\pi_{0}$

**目标：** 学习能够使智能体从初始状态 $s_{0}$ 到达目标状态的最优路径

**对每个回合执行：**
- 当 $s_{t}$ $(t = 0, 1, 2, \ldots)$ 不是目标状态时，执行：
  1. 收集给定 $s_{t}$ 的经验样本 $(a_{t}, r_{t+1}, s_{t+1})$：
     - 按照 $\pi_{t}(s_{t})$ 生成 $a_{t}$
     - 通过与环境的交互生成 $r_{t+1}, s_{t+1}$
  2. 更新 $(s_{t}, a_{t})$ 的 $q$ 值：
     $$
     q_{t+1}(s_{t}, a_{t}) = q_{t}(s_{t}, a_{t}) - \alpha_{t}(s_{t}, a_{t}) \Big[ q_{t}(s_{t}, a_{t}) - (r_{t+1} + \gamma \max_{a} q_{t}(s_{t+1}, a)) \Big]
     $$
  3. 更新 $s_{t}$ 的策略：
     $$
     \pi_{t+1}(a|s_{t}) = 
     \begin{cases}
     1 - \frac{\epsilon}{|A(s_{t})|}(|A(s_{t})| - 1) & \text{若 } a = \arg\max_{a} q_{t+1}(s_{t}, a) \\
     \frac{\epsilon}{|A(s_{t})|} & \text{否则}
     \end{cases}
     $$

# 算法 7.3：基于 Q-learning 的最优策略学习（离策略版本）

**初始化：**
- 对所有 $(s, a)$ 初始化猜测值 $q_{0}(s, a)$
- 对所有 $(s, a)$ 定义行为策略 $\pi_{b}(a|s)$
- 对所有 $(s, a)$ 和所有 $t$，设置 $\alpha_{t}(s, a) = \alpha > 0$

**目标：** 从 $\pi_{b}$ 生成的经验样本中学习所有状态的最优目标策略 $\pi_{T}$

**算法：**
对于每个由 $\pi_{b}$ 生成的回合 $\{s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, ...\}$，执行：
对于回合中的每个步骤 $t = 0, 1, 2, ...$，执行：

1. 更新 $(s_{t}, a_{t})$ 的 $q$ 值：
   $$q_{t+1}(s_{t}, a_{t}) = q_{t}(s_{t}, a_{t}) - \alpha_{t}(s_{t}, a_{t}) \Big[ q_{t}(s_{t}, a_{t}) - (r_{t+1} + \gamma \max_{a} q_{t}(s_{t+1}, a)) \Big]$$

2. 更新 $s_{t}$ 的目标策略：
   - 如果 $a = \arg \max_{a} q_{t+1}(s_{t}, a)$，则 $\pi_{T,t+1}(a|s_{t}) = 1$
   - 否则 $\pi_{T,t+1}(a|s_{t}) = 0$

![](images/3 - Chapter 7 Temporal-Difference Methods/2.jpg)

**图 7.3：** Q-learning 的演示示例。所有回合都从左上角状态开始，并在到达目标状态后终止。目标是找到从起始状态到目标状态的最优路径。奖励设置为 $r_{\mathrm{target}} = 0$，$r_{\mathrm{forbidden}} = r_{\mathrm{boundary}} = -10$，以及 $r_{\mathrm{other}} = -1$。学习率为 $\alpha = 0.1$，$\epsilon$ 值为 0.1。左图显示了算法获得的最终策略。右图显示了每个回合的总奖励和长度。

# 7.4.3 实现

由于 Q-learning 是离策略算法，它既可以用同策略方式实现，也可以用离策略方式实现。

同策略版本的 Q-learning 如算法 7.2 所示。该实现与算法 7.1 中的 Sarsa 实现类似。这里，行为策略与目标策略相同，都是 $\epsilon$-贪婪策略。

离策略版本如算法 7.3 所示。行为策略 $\pi_{b}$ 可以是任意策略，只要它能生成足够的经验样本。当 $\pi_{b}$ 具有探索性时通常更有利。这里，目标策略 $\pi_{T}$ 是贪婪策略而非 $\epsilon$-贪婪策略，因为它不用于生成样本，因此不需要具备探索性。此外，这里展示的 Q-learning 离策略版本是离线实现的：所有经验样本都先被收集，然后进行处理。

# 7.4.4 示例说明

接下来我们通过示例来演示 Q 学习算法。

## 第一个示例：同策略 Q 学习

第一个示例如图 7.3 所示，展示了同策略 Q 学习的应用。该示例的目标是找到从起始状态到目标状态的最优路径。具体设置见图 7.3 的图注说明。可以看出，Q 学习最终能够找到最优路径。在学习过程中，每个回合的路径长度逐渐缩短，而每个回合的总奖励值则持续增加。

## 第二个示例：异策略 Q 学习

第二组示例如图 7.4 和图 7.5 所示，展示了异策略 Q 学习的应用。这里的目标是为所有状态找到最优策略。奖励设置如下：

- $r_{\mathrm{boundary}} = r_{\mathrm{forbidden}} = -1$
- $r_{\mathrm{target}} = 1$

折扣率设为 $\gamma = 0.9$，学习率设为 $\alpha = 0.1$。

### 基准真值

为验证 Q 学习的有效性，我们首先需要了解最优策略和最优状态值的基准真值。此处基准真值通过基于模型的策略迭代算法获得，具体结果见图 7.4(a) 和 (b)。

### 经验样本

行为策略采用均匀分布：在任何状态下采取任意动作的概率均为 0.2（图 7.4(c)）。生成包含 100,000 步的单个回合（图 7.4(d)）。由于行为策略具有良好的探索能力，该回合会多次访问每个状态-动作对。

### 学习结果

基于行为策略生成的回合数据，通过 Q 学习得到的最优目标策略如图 7.4(e) 所示。该策略是最优的，因为如图 7.4(f) 所示，估计的状态值误差（均方根误差）收敛至零。此外可以注意到，学习得到的最优策略与图 7.4(a) 中的策略并不完全一致。实际上，存在多个具有相同最优状态值的最优策略。

### 不同初始值

由于 Q 学习采用自举法，算法性能依赖于动作值的初始猜测值。如图 7.4(g) 所示，当初始猜测值接近真实值时，估计值在大约 10,000 步内即可收敛。否则需要更多步数才能收敛（图 7.4(h)）。尽管如此，这些图表表明即使初始值不够准确，Q 学习仍能快速收敛。

### 不同行为策略

当行为策略缺乏探索性时，学习性能会显著下降。以图 7.5 所示的行为策略为例，这些是 $\epsilon$=0.5 或 0.1 的 $\epsilon$-贪婪策略（图 7.4(c) 中的均匀策略可视为 $\epsilon$=1 的 $\epsilon$-贪婪策略）。结果显示当 $\epsilon$ 从 1 降至 0.5 再降至 0.1 时，学习速度显著下降。这是因为策略的探索能力较弱，导致经验样本不足。

# 7.5 统一视角

到目前为止，我们已经介绍了不同的时序差分算法，例如 Sarsa、$n$步 Sarsa 和 Q-learning。在本节中，我们将介绍一个统一的框架来容纳所有这些算法以及蒙特卡洛学习。

具体而言，时序差分算法（用于动作值估计）可以用一个统一的表达式表示：

$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)[q_t(s_t, a_t) - \bar{q}_t],
$$

![](images/3 - Chapter 7 Temporal-Difference Methods/3.jpg)
图 7.4：通过 Q-learning 展示离策略学习的示例。最优策略和最优状态值分别显示在 (a) 和 (b) 中。行为策略和生成的轨迹分别显示在 (c) 和 (d) 中。估计的策略和估计误差的演变分别显示在 (e) 和 (f) 中。具有不同初始值的情况显示在 (g) 和 (h) 中。

![](images/3 - Chapter 7 Temporal-Difference Methods/4.jpg)
图 7.5：当行为策略不具备探索性时，Q-learning 的性能会下降。左列中的图显示了行为策略。中间列中的图显示了遵循相应行为策略生成的轨迹。每个示例中的轨迹有 100,000 步。右列中的图显示了估计状态值的均方根误差的演变。

| 算法 | 公式 (7.20) 中 TD 目标 $\bar{q}_t$ 的表达式 |
|-----------|---------------------------------------------------|
| Sarsa | $\bar{q}_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$ |
| $n$步 Sarsa | $\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^n q_t(s_{t+n}, a_{t+n})$ |
| Q-learning | $\bar{q}_t = r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)$ |
| 蒙特卡洛 | $\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots$ |

| 算法 | 待求解方程 |
|-----------|----------------------|
| Sarsa | 贝尔曼方程: $q_\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a]$ |
| $n$步 Sarsa | 贝尔曼方程: $q_\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_\pi(S_{t+n}, A_{t+n}) \mid S_t = s, A_t = a]$ |
| Q-learning | 贝尔曼最优方程: $q(s,a) = \mathbb{E}[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \mid S_t = s, A_t = a]$ |
| 蒙特卡洛 | 贝尔曼方程: $q_\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \mid S_t = s, A_t = a]$ |

表 7.2：时序差分算法的统一视角。此处，BE 和 BOE 分别表示贝尔曼方程和贝尔曼最优方程。

其中 $\bar{q}_t$ 是 TD 目标。不同的时序差分算法有不同的 $\bar{q}_t$。总结见表 7.2。蒙特卡洛学习算法可以看作是 (7.20) 的一个特例：我们可以设置 $\alpha_t(s_t, a_t) = 1$，那么 (7.20) 就变成了 $q_{t+1}(s_t, a_t) = \bar{q}_t$。

算法 (7.20) 可以看作是求解统一方程 $q(s, a) = \mathbb{E}[\bar{q}_t \mid s, a]$ 的随机近似算法。这个方程根据不同的 $\bar{q}_t$ 有不同的表达式。这些表达式总结在表 7.2 中。可以看出，除了 Q-learning 之外的所有算法都旨在求解贝尔曼方程，而 Q-learning 旨在求解贝尔曼最优方程。

# 7.6 本章小结

本章介绍了一类重要的强化学习算法——时序差分学习。我们介绍的具体算法包括 Sarsa、$n$步 Sarsa 和 Q-learning。所有这些算法都可以视为求解贝尔曼方程或贝尔曼最优方程的随机逼近算法。

除 Q-learning 外，本章介绍的 TD 算法都用于评估给定策略，即从经验样本中估计给定策略的状态/动作值。结合策略改进，它们可用于学习最优策略。此外，这些算法都是在策略的：目标策略被同时用作行为策略来生成经验样本。

与其他 TD 算法相比，Q-learning 的特殊之处在于它是离策略的。在 Q-learning 中，目标策略可以不同于行为策略。Q-learning 之所以是离策略的根本原因在于，它旨在求解贝尔曼最优方程，而非给定策略的贝尔曼方程。

值得一提的是，存在一些方法可以将在线策略算法转换为离线策略算法。重要性采样是一种广泛使用的方法[3,40]，将在第 10 章介绍。最后，本章介绍的 TD 算法还存在一些变体和扩展[41-45]。例如，TD($\lambda$) 方法为 TD 学习提供了更通用和统一的框架。更多信息可参考文献[3,20,46]。

# 7.7 问答环节

## 问：时序差分学习中的"TD"一词指的是什么？

**答：** 每个TD算法都有一个TD误差，它表示新样本与当前估计之间的差异。由于这种差异是在不同时间步之间计算的，因此被称为"时序差分"。

## 问：时序差分学习中的"学习"一词指的是什么？

**答：** 从数学角度来看，"学习"简单来说就是"估计"。即从一些样本中估计状态/动作值，然后基于估计值获得策略。

## 问：虽然Sarsa可以估计给定策略的动作值，但如何用它来学习最优策略？

**答：** 要获得最优策略，价值估计过程应该与策略改进过程相互作用。也就是说，在价值更新后，相应的策略也应该更新。然后，更新后的策略生成的新样本可以再次用于价值估计。这就是广义策略迭代的思想。

## 问：为什么Sarsa要将策略更新为$\epsilon$-贪心？

**答：** 这是因为策略也用于生成价值估计的样本。因此，它应该具有探索性，以生成足够的经验样本。

## 问：虽然定理7.1和7.2要求学习率$\alpha_{t}$逐渐收敛到零，为什么在实践中通常将其设置为一个小的常数？

**答：** 根本原因是要评估的策略在不断变化（或称为非平稳的）。具体来说，像Sarsa这样的TD学习算法旨在估计给定策略的动作值。如果策略是固定的，使用衰减的学习率是可以接受的。然而，在最优策略学习过程中，Sarsa要评估的策略在每次迭代后都在不断变化。在这种情况下，我们需要一个恒定的学习率；否则，衰减的学习率可能会变得太小而无法有效评估策略。尽管恒定学习率的一个缺点是价值估计最终可能会波动，但只要恒定学习率足够小，这种波动是可以忽略的。

## 问：我们应该学习所有状态的最优策略，还是只学习部分状态的最优策略？

**答：** 这取决于任务。有人可能会注意到，本章考虑的一些任务（例如图7.2）不需要找到所有状态的最优策略。相反，它们只需要找到从给定起始状态到目标状态的最优路径。这类任务对数据的要求不高，因为智能体不需要充分访问每个状态-动作对多次。然而，必须注意的是，这样获得的路径不能保证是最优的。这是因为如果没有充分探索所有状态-动作对，可能会错过更好的路径。尽管如此，给定足够的数据，我们仍然可以找到一个好的或局部最优的路径。

## 问：为什么Q-learning是离策略的，而本章中所有其他TD算法都是在策略的？

**答：** 根本原因是Q-learning旨在解决贝尔曼最优方程，而其他TD算法旨在解决给定策略的贝尔曼方程。详细信息可以在第7.4.2节中找到。

## 问：为什么离策略版本的Q-learning将策略更新为贪心而不是$\epsilon$-贪心？

**答：** 这是因为目标策略不需要生成经验样本。因此，它不需要具有探索性。