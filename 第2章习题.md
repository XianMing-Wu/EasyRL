# 第二章 马尔可夫决策过程（MDP）

## 关键词

- **马尔可夫性质（Markov property, MP）**：如果某一过程的未来状态与过去状态无关，只由当前状态决定，则该过程具有马尔可夫性质。换言之，下一状态只取决于当前状态，与之前的状态序列无关。
- **马尔可夫链（Markov chain）**：在离散指数集和离散状态空间上，满足马尔可夫性质的随机过程。
- **状态转移矩阵（state transition matrix）**：描述在当前状态下转移到各个可能下一状态的概率，每一行对应从一个状态到所有其他状态的条件概率。
- **马尔可夫奖励过程（Markov reward process, MRP）**：在马尔可夫链的基础上增加奖励函数的结构，奖励函数给出在某状态下获得的期望即时奖励。
- **范围（horizon）**：定义一个回合或一条完整轨迹的长度，由有限步数构成。
- **回报（return）**：对整条轨迹上的奖励进行折扣加总得到的折扣回报。
- **贝尔曼方程（Bellman equation）**：刻画当前状态价值与未来状态价值之间递推关系。其标量形式为：
  $$V(s) = R(s) + \gamma \sum_{s' \in S} P(s' \mid s)\, V(s')$$
  矩阵形式为：
  $$V = R + \gamma P V$$
- **蒙特卡洛算法（Monte Carlo algorithm, MC）**：通过采样完整轨迹并折扣累计奖励估计价值函数。
- **动态规划算法（dynamic programming, DP）**：迭代贝尔曼方程直至收敛，用于计算价值函数。
- **$Q$ 函数（$Q$-function）**：表示在状态 $s$ 下采取动作 $a$ 后可能得到的折扣回报期望。
- **预测问题（policy evaluation）**：在给定马尔可夫决策过程与策略 $\pi$ 时，估计每个状态的价值函数，可用动态规划求解。
- **控制问题（control problem）**：在给定马尔可夫决策过程时寻找最优策略及对应最优价值函数，可用动态规划求解。
- **最优价值函数（optimal value function）**：对所有策略取最大值得到的状态价值函数 $V^*$；实现该价值的策略即为最优策略。

## 习题

### 2-1 为什么在马尔可夫奖励过程中需要有折扣因子？

- 某些马尔可夫过程是环状且无终点，引入折扣因子可避免回报发散。
- 折扣因子体现不确定性偏好，更倾向于尽早获得奖励。
- 即便奖励具备现实价值，也常希望立即获得奖励，这一偏好可通过折扣因子建模。
- 折扣因子可取 $0$ 只关注当前奖励，也可取 $1$ 表示不区分当前与未来奖励。
- 因此折扣因子可作为超参数调整，以塑造不同行为偏好的智能体。

### 2-2 为什么矩阵形式的贝尔曼方程的解析解比较难求得？

- 解析解需要对矩阵 $I - \gamma P$ 求逆，时间复杂度为 $O(N^3)$。
- 当状态数目很大（如达到 $10^6$）时，转移矩阵规模巨大，求逆不切实际。
- 因此解析解仅适用于状态空间较小的马尔可夫奖励过程。

### 2-3 计算贝尔曼方程的常见方法有哪些，它们有什么区别？

- 蒙特卡洛方法：通过采样完整轨迹得到折扣回报 $\hat{g}$，用样本均值估计价值函数。
- 动态规划方法：迭代贝尔曼方程，直到相邻两次估计差距小于阈值 $\epsilon$。
- 时序差分学习：结合动态规划与蒙特卡洛思想，使用单步（或多步）采样更新。

### 2-4 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？

- 马尔可夫决策过程在马尔可夫奖励过程的基础上增加了动作集合，状态转移概率为 $P(s' \mid s, a)$，与当前状态和动作共同决定。
- 奖励函数同样依赖于状态与动作 $R(s, a)$。
- 给定策略 $\pi$ 时，可通过对动作概率加权，将马尔可夫决策过程转化为对应的马尔可夫奖励过程。

### 2-5 马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的差异有哪些？

- 马尔可夫链中，下一状态由当前状态和转移概率直接决定。
- 马尔可夫决策过程在状态转移前需由智能体选择动作，状态转移受 $(s, a)$ 共同影响，因此多了一层决策结构。

### 2-6 我们如何寻找最佳策略，寻找最佳策略方法有哪些？

- 通过最优价值函数推导最优策略，最大化 $Q$ 函数即可得到最优动作。
- 常见方法：
  1. 穷举法：对有限状态与动作集合枚举所有策略，比较其价值函数（通常不可行）。
  2. 策略迭代：在策略评估与策略提升之间交替迭代，直至收敛。
  3. 价值迭代：直接迭代贝尔曼最优方程，以获取 $V^*$ 并导出最优策略。

## 面试题

### 2-1 友善的面试官：马尔可夫过程与马尔可夫决策过程分别是什么？最重要的性质是什么？

- 马尔可夫过程是二元组 $\langle S, P \rangle$，其中 $S$ 为状态集合，$P$ 为状态转移函数。
- 马尔可夫决策过程是五元组 $\langle S, P, A, R, \gamma \rangle$，其中 $A$ 为动作集合，$R$ 为奖励函数，$\gamma$ 为折扣因子。
- 马尔可夫最重要的性质是下一状态仅与当前状态有关，即：
  $$p(s_{t+1} \mid s_t) = p(s_{t+1} \mid s_1, s_2, \ldots, s_t)$$

### 2-2 友善的面试官：我们一般怎么求解马尔可夫决策过程？

- 可通过贝尔曼方程：
  $$V(s) = R(s) + \gamma \sum_{s' \in S} p(s' \mid s) V(s')$$
- 矩阵形式为：
  $$V = R + \gamma P V$$
- 当直接求解困难时，可采用动态规划、蒙特卡洛或时序差分等数值方法。

### 2-3 友善的面试官：如果数据流不具备马尔可夫性质怎么办？

- 当下一状态依赖于长历史时，单一当前状态表示会降低泛化能力。
- 可使用循环神经网络、注意力机制等方法对历史信息建模，构造包含历史的状态表征，再在新的状态空间求解马尔可夫决策过程。

### 2-4 友善的面试官：请写出基于状态价值函数与动作价值函数的贝尔曼方程。

- 基于状态价值函数的贝尔曼方程：
  $$V_\pi(s) = \sum_a \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r(s, a) + \gamma V_\pi(s') \right]$$
- 基于动作价值函数的贝尔曼方程：
  $$Q_\pi(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r(s, a) + \gamma V_\pi(s') \right]$$

### 2-5 友善的面试官：为什么最佳价值函数 $V^*$ 和最佳策略 $\pi^*$ 等价？

- 最优价值函数定义为 $V^*(s) = \max_\pi V_\pi(s)$，即对所有策略取最大值。
- 实现该最大价值的策略即为最优策略 $\pi^*$，满足 $\pi^*(s) = \arg\max_\pi V_\pi(s)$。
- 虽可能存在多个实现相同 $V^*$ 的策略，但其最优价值一致，因此求得 $V^*$ 即意味着求解了 MDP 环境。

### 2-6 友善的面试官：第 $n$ 步的价值函数更新公式是什么？当 $n$ 越来越大时，期望和方差如何变化？

- $n$ 步时序差分的 $Q$ 更新公式：
  $$Q(S, A) \leftarrow Q(S, A) + \alpha \left[ \sum_{i=1}^{n} \gamma^{i-1} r_{t+i} + \gamma^{n} \max_a Q(S', a) - Q(S, A) \right]$$
- 随着 $n$ 增大，估计的期望偏差减小，但方差增大。
