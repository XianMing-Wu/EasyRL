{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3108d228",
   "metadata": {},
   "source": [
    "# Gymnasium 基础教程：为值迭代和策略迭代准备环境\n",
    "\n",
    "## 📖 为什么需要 Gymnasium？\n",
    "\n",
    "在学习**贝尔曼最优公式**、**值迭代**和**策略迭代**时，我们需要一个简单、可控的环境来验证算法。Gymnasium 是强化学习的标准环境接口，让我们可以：\n",
    "\n",
    "- 🎮 创建标准化的 MDP（马尔可夫决策过程）环境\n",
    "- 🔄 轻松实现状态转移和奖励函数\n",
    "- 📊 可视化智能体的学习过程\n",
    "- ✅ 验证值迭代、策略迭代等算法的正确性\n",
    "\n",
    "## 🎯 本教程目标\n",
    "\n",
    "我们将从零开始创建一个**网格世界（GridWorld）环境**，这是学习动态规划算法的经典环境：\n",
    "\n",
    "- **环境特性**：\n",
    "  - 可自定义网格大小（如 5×5、10×10）\n",
    "  - 随机初始化智能体和目标位置\n",
    "  - 支持自定义奖励函数\n",
    "  - 可设置障碍物（进阶功能）\n",
    "  - 文本和图形可视化\n",
    "\n",
    "- **学习路径**：\n",
    "  1. 理解 Gymnasium 的核心概念\n",
    "  2. 逐步构建自定义环境\n",
    "  3. 测试和验证环境\n",
    "  4. 为后续算法做准备\n",
    "\n",
    "---\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mqn81jfu7bk",
   "metadata": {},
   "source": [
    "## 第一步：安装和导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3xxkxqlq07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gymnasium 版本: 1.2.1\n",
      "✅ NumPy 版本: 2.3.3\n",
      "✅ Matplotlib 版本: 3.10.7\n",
      "✅ Matplotlib 后端: Agg\n",
      "✅ 中文字体配置完成\n",
      "✅ imageio 已导入，支持 GIF 动画生成\n"
     ]
    }
   ],
   "source": [
    "# 首先安装 gymnasium（如果还没安装的话）\n",
    "# !pip install gymnasium numpy matplotlib imageio\n",
    "\n",
    "# 导入必要的库\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Dict, Any, Annotated\n",
    "\n",
    "# 设置 matplotlib 后端（必须在导入 pyplot 之前）\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 使用非交互式后端,适合生成图像文件\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, clear_output, Image as IPImage\n",
    "import imageio\n",
    "from io import BytesIO\n",
    "\n",
    "# 配置 matplotlib 中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "print(f\"✅ Gymnasium 版本: {gym.__version__}\")\n",
    "print(f\"✅ NumPy 版本: {np.__version__}\")\n",
    "print(f\"✅ Matplotlib 版本: {matplotlib.__version__}\")\n",
    "print(f\"✅ Matplotlib 后端: {matplotlib.get_backend()}\")\n",
    "print(f\"✅ 中文字体配置完成\")\n",
    "print(f\"✅ imageio 已导入，支持 GIF 动画生成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wo9nqhbfs7",
   "metadata": {},
   "source": [
    "## 第二步：理解 Gymnasium 环境的核心概念\n",
    "\n",
    "在创建自定义环境之前，我们需要理解 Gymnasium 的四个核心概念：\n",
    "\n",
    "### 🔑 核心概念\n",
    "\n",
    "1. **观察空间 (Observation Space)**\n",
    "   - 定义智能体能\"看到\"什么\n",
    "   - 例如：网格世界中，智能体可以观察到自己和目标的位置\n",
    "\n",
    "2. **动作空间 (Action Space)**\n",
    "   - 定义智能体能做什么\n",
    "   - 例如：上、下、左、右四个移动方向\n",
    "\n",
    "3. **状态转移 (State Transition)**\n",
    "   - 定义环境如何响应智能体的动作\n",
    "   - 例如：向右移动会让智能体的 x 坐标 +1\n",
    "\n",
    "4. **奖励函数 (Reward Function)**\n",
    "   - 定义智能体做某个动作后获得的奖励\n",
    "   - 例如：到达目标 +1，其他情况 0 或 -0.01\n",
    "\n",
    "### 📋 Gymnasium 环境必须实现的方法\n",
    "\n",
    "| 方法 | 作用 | 返回值 |\n",
    "|------|------|--------|\n",
    "| `__init__()` | 初始化环境，定义观察空间和动作空间 | - |\n",
    "| `reset()` | 重置环境到初始状态 | `(observation, info)` |\n",
    "| `step(action)` | 执行一个动作，返回新状态和奖励 | `(observation, reward, terminated, truncated, info)` |\n",
    "| `render()` | 可视化当前状态（可选） | - |\n",
    "\n",
    "让我们先用一个简单的示例来理解这些概念："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blliy1o7f1e",
   "metadata": {},
   "source": [
    "> **💡 重要提示**：由于我们在云端容器环境中运行代码，无法直接显示图形窗口。因此本教程将使用以下可视化方式：\n",
    "> - ✅ **文本渲染**：使用 ASCII 字符直接在 Jupyter 中显示\n",
    "> - ✅ **静态图片**：使用 matplotlib 生成网格图\n",
    "> - ✅ **GIF 动画**：使用 imageio 生成可在 Jupyter 中播放的动画"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cnzujx9fi",
   "metadata": {},
   "source": [
    "## 第三步：构建最简单的网格世界环境\n",
    "\n",
    "现在让我们从最简单的版本开始，逐步构建一个完整的网格世界环境。我们将分为以下几个步骤：\n",
    "\n",
    "### 步骤 3.1：定义环境的骨架\n",
    "\n",
    "首先，我们创建一个继承自 `gym.Env` 的类，并定义观察空间和动作空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pwbexau2h1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 创建了一个 5×5 的网格世界环境\n",
      "   观察空间: Dict('agent': Box(0, 4, (2,), int32), 'target': Box(0, 4, (2,), int32))\n",
      "   动作空间: Discrete(4) (0=右, 1=上, 2=左, 3=下)\n",
      "\n",
      "环境类型: <class '__main__.SimpleGridWorldEnv'>\n"
     ]
    }
   ],
   "source": [
    "class SimpleGridWorldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    最简单的网格世界环境\n",
    "    \n",
    "    环境描述：\n",
    "    - 一个 size × size 的网格\n",
    "    - 智能体（A）需要从随机位置移动到目标（T）\n",
    "    - 4个动作：上、下、左、右\n",
    "    - 到达目标获得奖励 +1，其他情况奖励为 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size: Annotated[int, \"网格的边长\"] = 5):\n",
    "        \"\"\"\n",
    "        初始化环境\n",
    "        \n",
    "        参数：\n",
    "            size: 网格的大小（默认 5×5）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size = size  # 网格大小\n",
    "        \n",
    "        # 定义观察空间：智能体和目标的位置坐标\n",
    "        # 使用 Dict 空间让观察更加结构化和易读\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int32),   # 智能体位置 [x, y]\n",
    "            \"target\": spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int32),  # 目标位置 [x, y]\n",
    "        })\n",
    "        \n",
    "        # 定义动作空间：4个离散动作\n",
    "        # 0: 右, 1: 上, 2: 左, 3: 下\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # 定义动作到方向的映射\n",
    "        # 注意：我们使用数学坐标系，y 轴向上为正\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),    # 右：x+1\n",
    "            1: np.array([0, 1]),    # 上：y+1\n",
    "            2: np.array([-1, 0]),   # 左：x-1\n",
    "            3: np.array([0, -1]),   # 下：y-1\n",
    "        }\n",
    "        \n",
    "        # 初始化位置（将在 reset 中设置）\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "        \n",
    "        print(f\"✅ 创建了一个 {size}×{size} 的网格世界环境\")\n",
    "        print(f\"   观察空间: {self.observation_space}\")\n",
    "        print(f\"   动作空间: {self.action_space} (0=右, 1=上, 2=左, 3=下)\")\n",
    "\n",
    "# 测试：创建一个环境实例\n",
    "env = SimpleGridWorldEnv(size=5)\n",
    "print(f\"\\n环境类型: {type(env)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4qdu8vxq",
   "metadata": {},
   "source": [
    "### 步骤 3.2：实现 reset() 方法\n",
    "\n",
    "`reset()` 方法用于重置环境到初始状态，这是每个 episode 开始时必须调用的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zki0clm6qz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "测试 reset() 方法\n",
      "============================================================\n",
      "✅ 创建了一个 5×5 的网格世界环境\n",
      "   观察空间: Dict('agent': Box(0, 4, (2,), int32), 'target': Box(0, 4, (2,), int32))\n",
      "   动作空间: Discrete(4) (0=右, 1=上, 2=左, 3=下)\n",
      "\n",
      "初始观察:\n",
      "  智能体位置: [0 3]\n",
      "  目标位置: [3 2]\n",
      "\n",
      "额外信息:\n",
      "  曼哈顿距离: 4.0\n",
      "\n",
      "第二次重置（不同种子）:\n",
      "  智能体位置: [0 3]\n",
      "  目标位置: [2 0]\n",
      "\n",
      "额外信息:\n",
      "  曼哈顿距离: 5.0\n"
     ]
    }
   ],
   "source": [
    "# 为 SimpleGridWorldEnv 添加 reset 方法\n",
    "# 注意：这里我们使用独立的函数来演示，稍后会整合到完整的类中\n",
    "\n",
    "def reset(\n",
    "    self,\n",
    "    seed: Annotated[Optional[int], \"随机数种子，用于保证可重现性\"] = None,\n",
    "    options: Annotated[Optional[dict], \"额外的配置选项\"] = None,\n",
    ") -> Annotated[Tuple[Dict[str, np.ndarray], Dict[str, float]], \"初始观察和额外信息\"]:\n",
    "    \"\"\"\n",
    "    重置环境到初始状态\n",
    "    \n",
    "    参数：\n",
    "        seed: 随机数种子，用于保证可重现性\n",
    "        options: 额外的配置选项（本例中未使用）\n",
    "    \n",
    "    返回：\n",
    "        observation: 初始观察（包含智能体和目标位置）\n",
    "        info: 额外信息（如智能体到目标的距离）\n",
    "    \"\"\"\n",
    "    # ⚠️ 重要：必须首先调用父类的 reset 方法来设置随机数生成器\n",
    "    # 这样使用 self.np_random 才能保证可重现性\n",
    "    super(SimpleGridWorldEnv, self).reset(seed=seed)\n",
    "    \n",
    "    # 随机放置智能体\n",
    "    self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=np.int32)\n",
    "    \n",
    "    # 随机放置目标，确保不与智能体重叠\n",
    "    self._target_location = self._agent_location.copy()\n",
    "    while np.array_equal(self._target_location, self._agent_location):\n",
    "        self._target_location = self.np_random.integers(0, self.size, size=2, dtype=np.int32)\n",
    "    \n",
    "    # 构造观察\n",
    "    observation = self._get_obs()\n",
    "    \n",
    "    # 构造额外信息（曼哈顿距离）\n",
    "    info = self._get_info()\n",
    "    \n",
    "    return observation, info\n",
    "\n",
    "def _get_obs(self) -> Annotated[Dict[str, np.ndarray], \"当前观察字典\"]:\n",
    "    \"\"\"获取当前观察\"\"\"\n",
    "    return {\n",
    "        \"agent\": self._agent_location,\n",
    "        \"target\": self._target_location\n",
    "    }\n",
    "\n",
    "def _get_info(self) -> Annotated[Dict[str, float], \"额外信息字典\"]:\n",
    "    \"\"\"获取额外信息：智能体到目标的曼哈顿距离\"\"\"\n",
    "    return {\n",
    "        \"distance\": np.linalg.norm(\n",
    "            self._agent_location - self._target_location, \n",
    "            ord=1  # 曼哈顿距离（L1范数）\n",
    "        )\n",
    "    }\n",
    "\n",
    "# 将方法添加到类中\n",
    "SimpleGridWorldEnv.reset = reset\n",
    "SimpleGridWorldEnv._get_obs = _get_obs\n",
    "SimpleGridWorldEnv._get_info = _get_info\n",
    "\n",
    "# 测试 reset 方法\n",
    "print(\"=\" * 60)\n",
    "print(\"测试 reset() 方法\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleGridWorldEnv(size=5)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(f\"\\n初始观察:\")\n",
    "print(f\"  智能体位置: {obs['agent']}\")\n",
    "print(f\"  目标位置: {obs['target']}\")\n",
    "print(f\"\\n额外信息:\")\n",
    "print(f\"  曼哈顿距离: {info['distance']}\")\n",
    "\n",
    "# 再次重置，观察随机性\n",
    "obs2, info2 = env.reset(seed=123)\n",
    "print(f\"\\n第二次重置（不同种子）:\")\n",
    "print(f\"  智能体位置: {obs2['agent']}\")\n",
    "print(f\"  目标位置: {obs2['target']}\")\n",
    "print(f\"\\n额外信息:\")\n",
    "print(f\"  曼哈顿距离: {info2['distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u4urm6ojqv8",
   "metadata": {},
   "source": [
    "### 步骤 3.3：实现 step() 方法\n",
    "\n",
    "`step()` 方法是环境的核心，它定义了状态转移和奖励函数。这个方法接收一个动作，返回新的观察、奖励和是否结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "g9bwtm19th5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "测试 step() 方法\n",
      "============================================================\n",
      "✅ 创建了一个 5×5 的网格世界环境\n",
      "   观察空间: Dict('agent': Box(0, 4, (2,), int32), 'target': Box(0, 4, (2,), int32))\n",
      "   动作空间: Discrete(4) (0=右, 1=上, 2=左, 3=下)\n",
      "初始状态:\n",
      "  智能体: [0 3], 目标: [3 2]\n",
      "  距离: 4.0\n",
      "\n",
      "步骤 1: 动作=右\n",
      "  新位置: [1 3]\n",
      "  奖励: 0.0\n",
      "  到达目标: False\n",
      "  距离: 3.0\n",
      "\n",
      "步骤 2: 动作=右\n",
      "  新位置: [2 3]\n",
      "  奖励: 0.0\n",
      "  到达目标: False\n",
      "  距离: 2.0\n",
      "\n",
      "步骤 3: 动作=上\n",
      "  新位置: [2 4]\n",
      "  奖励: 0.0\n",
      "  到达目标: False\n",
      "  距离: 3.0\n",
      "\n",
      "步骤 4: 动作=上\n",
      "  新位置: [2 4]\n",
      "  奖励: 0.0\n",
      "  到达目标: False\n",
      "  距离: 3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def step(\n",
    "    self,\n",
    "    action: Annotated[int, \"要执行的动作编号\"],\n",
    ") -> Annotated[Tuple[Dict[str, np.ndarray], float, bool, bool, Dict[str, float]], \"新观察、奖励、终止标志、截断标志及额外信息\"]:\n",
    "    \"\"\"\n",
    "    执行一个动作，环境进行状态转移\n",
    "    \n",
    "    参数：\n",
    "        action: 要执行的动作 (0=右, 1=上, 2=左, 3=下)\n",
    "    \n",
    "    返回：\n",
    "        observation: 新的观察\n",
    "        reward: 获得的奖励\n",
    "        terminated: 是否到达终止状态（达到目标）\n",
    "        truncated: 是否被截断（超过最大步数，本例中未使用）\n",
    "        info: 额外信息\n",
    "    \"\"\"\n",
    "    # 1. 根据动作获取移动方向\n",
    "    direction = self._action_to_direction[action]\n",
    "    \n",
    "    # 2. 更新智能体位置，使用 np.clip 确保不会超出边界\n",
    "    # clip(a, min, max) 将 a 限制在 [min, max] 范围内\n",
    "    self._agent_location = np.clip(\n",
    "        self._agent_location + direction,\n",
    "        0,  # 最小值\n",
    "        self.size - 1  # 最大值\n",
    "    )\n",
    "    \n",
    "    # 3. 检查是否到达目标\n",
    "    terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "    \n",
    "    # 4. 计算奖励\n",
    "    # 简单策略：到达目标 +1，否则 0\n",
    "    reward = 1.0 if terminated else 0.0\n",
    "    \n",
    "    # 5. 截断标志（本例中不使用，但 Gymnasium 要求返回）\n",
    "    truncated = False\n",
    "    \n",
    "    # 6. 构造返回值\n",
    "    observation = self._get_obs()\n",
    "    info = self._get_info()\n",
    "    \n",
    "    return observation, reward, terminated, truncated, info\n",
    "\n",
    "# 将方法添加到类中\n",
    "SimpleGridWorldEnv.step = step\n",
    "\n",
    "# 测试 step 方法\n",
    "print(\"=\" * 60)\n",
    "print(\"测试 step() 方法\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleGridWorldEnv(size=5)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(f\"初始状态:\")\n",
    "print(f\"  智能体: {obs['agent']}, 目标: {obs['target']}\")\n",
    "print(f\"  距离: {info['distance']}\\n\")\n",
    "\n",
    "# 执行几个动作\n",
    "actions = [0, 0, 1, 1]  # 右, 右, 上, 上\n",
    "action_names = ['右', '右', '上', '上']\n",
    "\n",
    "for i, (action, name) in enumerate(zip(actions, action_names)):\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"步骤 {i+1}: 动作={name}\")\n",
    "    print(f\"  新位置: {obs['agent']}\")\n",
    "    print(f\"  奖励: {reward}\")\n",
    "    print(f\"  到达目标: {terminated}\")\n",
    "    print(f\"  距离: {info['distance']}\")\n",
    "    print()\n",
    "    \n",
    "    if terminated:\n",
    "        print(\"🎉 成功到达目标！\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1uikwyfh0a3",
   "metadata": {},
   "source": [
    "### 步骤 3.4：添加文本可视化功能\n",
    "\n",
    "现在我们添加 `render()` 方法，使用 ASCII 字符在 Jupyter 中直接显示网格状态。这对于调试和理解环境非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bqjshqwwg9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试 render() 方法\n",
      "============================================================\n",
      "✅ 创建了一个 5×5 的网格世界环境\n",
      "   观察空间: Dict('agent': Box(0, 4, (2,), int32), 'target': Box(0, 4, (2,), int32))\n",
      "   动作空间: Discrete(4) (0=右, 1=上, 2=左, 3=下)\n",
      "初始状态:\n",
      "\n",
      "===========\n",
      "|· · · · · |\n",
      "|A · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体位置: [0 3], 目标位置: [3 2]\n",
      "曼哈顿距离: 4\n",
      "\n",
      "\n",
      "执行动作序列: 右 → 上 → 右 → 上\n",
      "------------------------------------------------------------\n",
      "\n",
      "步骤 1 后:\n",
      "\n",
      "===========\n",
      "|· · · · · |\n",
      "|· A · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体位置: [1 3], 目标位置: [3 2]\n",
      "曼哈顿距离: 3\n",
      "\n",
      "\n",
      "步骤 2 后:\n",
      "\n",
      "===========\n",
      "|· A · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体位置: [1 4], 目标位置: [3 2]\n",
      "曼哈顿距离: 4\n",
      "\n",
      "\n",
      "步骤 3 后:\n",
      "\n",
      "===========\n",
      "|· · A · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体位置: [2 4], 目标位置: [3 2]\n",
      "曼哈顿距离: 3\n",
      "\n",
      "\n",
      "步骤 4 后:\n",
      "\n",
      "===========\n",
      "|· · A · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体位置: [2 4], 目标位置: [3 2]\n",
      "曼哈顿距离: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def render(self):\n",
    "    \"\"\"\n",
    "    渲染当前环境状态（文本模式）\n",
    "    \n",
    "    使用 ASCII 字符显示网格：\n",
    "    - A: 智能体 (Agent)\n",
    "    - T: 目标 (Target)\n",
    "    - .: 空格子\n",
    "    \"\"\"\n",
    "    # 从上到下绘制网格（y 从大到小）\n",
    "    print(\"\\n\" + \"=\" * (self.size * 2 + 1))\n",
    "    for y in range(self.size - 1, -1, -1):  # 从顶部开始\n",
    "        row = \"|\"\n",
    "        for x in range(self.size):\n",
    "            # 检查当前位置是什么\n",
    "            if np.array_equal([x, y], self._agent_location):\n",
    "                row += \"A\"  # 智能体\n",
    "            elif np.array_equal([x, y], self._target_location):\n",
    "                row += \"T\"  # 目标\n",
    "            else:\n",
    "                row += \"·\"  # 空格子\n",
    "            row += \" \"\n",
    "        print(row + \"|\")\n",
    "    print(\"=\" * (self.size * 2 + 1))\n",
    "    \n",
    "    # 显示位置信息\n",
    "    print(f\"智能体位置: {self._agent_location}, 目标位置: {self._target_location}\")\n",
    "    print(f\"曼哈顿距离: {np.linalg.norm(self._agent_location - self._target_location, ord=1):.0f}\\n\")\n",
    "\n",
    "# 将方法添加到类中\n",
    "SimpleGridWorldEnv.render = render\n",
    "\n",
    "# 测试渲染功能\n",
    "print(\"测试 render() 方法\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleGridWorldEnv(size=5)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(\"初始状态:\")\n",
    "env.render()\n",
    "\n",
    "print(\"\\n执行动作序列: 右 → 上 → 右 → 上\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "actions = [0, 1, 0, 1]  # 右, 上, 右, 上\n",
    "for i, action in enumerate(actions, 1):\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"\\n步骤 {i} 后:\")\n",
    "    env.render()\n",
    "    \n",
    "    if terminated:\n",
    "        print(\"🎉 到达目标！\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "id6bs3gnifk",
   "metadata": {},
   "source": [
    "### 步骤 3.5：添加图形可视化（适合云端环境）\n",
    "\n",
    "文本渲染很好，但图形可视化更直观。由于我们在云端容器中，我们将使用 matplotlib 生成静态图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "la8p7wh69k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试图形渲染\n",
      "============================================================\n",
      "✅ 创建了一个 5×5 的网格世界环境\n",
      "   观察空间: Dict('agent': Box(0, 4, (2,), int32), 'target': Box(0, 4, (2,), int32))\n",
      "   动作空间: Discrete(4) (0=右, 1=上, 2=左, 3=下)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "显示初始网格状态:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def render_rgb(self):\n",
    "    \"\"\"\n",
    "    使用 matplotlib 渲染网格（适合云端环境）\n",
    "    返回 RGB 数组，可在 Jupyter 中显示\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # 设置坐标轴范围\n",
    "    ax.set_xlim(0, self.size)\n",
    "    ax.set_ylim(0, self.size)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # 绘制网格线\n",
    "    for i in range(self.size + 1):\n",
    "        ax.axhline(i, color='gray', linewidth=0.5)\n",
    "        ax.axvline(i, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # 绘制所有空格子（浅灰色）\n",
    "    for x in range(self.size):\n",
    "        for y in range(self.size):\n",
    "            rect = patches.Rectangle((x, y), 1, 1, \n",
    "                                     linewidth=1, \n",
    "                                     edgecolor='gray', \n",
    "                                     facecolor='white')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 绘制目标（绿色）\n",
    "    target_rect = patches.Rectangle(\n",
    "        (self._target_location[0], self._target_location[1]), 1, 1,\n",
    "        linewidth=2,\n",
    "        edgecolor='darkgreen',\n",
    "        facecolor='lightgreen'\n",
    "    )\n",
    "    ax.add_patch(target_rect)\n",
    "    ax.text(self._target_location[0] + 0.5, self._target_location[1] + 0.5, \n",
    "            'T', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 绘制智能体（蓝色）\n",
    "    agent_rect = patches.Rectangle(\n",
    "        (self._agent_location[0], self._agent_location[1]), 1, 1,\n",
    "        linewidth=2,\n",
    "        edgecolor='darkblue',\n",
    "        facecolor='lightblue'\n",
    "    )\n",
    "    ax.add_patch(agent_rect)\n",
    "    ax.text(self._agent_location[0] + 0.5, self._agent_location[1] + 0.5, \n",
    "            'A', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 设置标题和标签（使用英文避免中文显示问题）\n",
    "    distance = np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "    ax.set_title(f'Grid World ({self.size}x{self.size}) | Distance: {distance:.0f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    \n",
    "    # 添加坐标刻度\n",
    "    ax.set_xticks(range(self.size + 1))\n",
    "    ax.set_yticks(range(self.size + 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# 将方法添加到类中\n",
    "SimpleGridWorldEnv.render_rgb = render_rgb\n",
    "\n",
    "# 测试图形渲染\n",
    "print(\"测试图形渲染\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleGridWorldEnv(size=5)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# 显示初始状态\n",
    "fig = env.render_rgb()\n",
    "\n",
    "# 在云端 Jupyter 环境中显示图片（使用 Agg 后端）\n",
    "# 将 matplotlib figure 转换为图片并显示\n",
    "buf = BytesIO()\n",
    "fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "buf.seek(0)\n",
    "plt.close(fig)\n",
    "\n",
    "# 使用 IPython.display.Image 显示\n",
    "print(\"\\n显示初始网格状态:\")\n",
    "display(IPImage(buf.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3jnql08kqs",
   "metadata": {},
   "source": [
    "## 第四步：整合完整的环境类\n",
    "\n",
    "现在让我们将所有代码整合成一个完整的、可重用的环境类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "esntzvyjcau",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GridWorldEnv 类定义完成！\n"
     ]
    }
   ],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    网格世界环境 - 完整版\n",
    "    \n",
    "    这是一个用于强化学习的标准网格世界环境，特别适合学习：\n",
    "    - 贝尔曼最优公式\n",
    "    - 值迭代算法\n",
    "    - 策略迭代算法\n",
    "    \n",
    "    环境说明：\n",
    "        - 网格大小可自定义 (size × size)\n",
    "        - 智能体从随机位置开始\n",
    "        - 目标在随机位置（不与智能体重合）\n",
    "        - 智能体可以向四个方向移动：上、下、左、右\n",
    "        - 到达边界时停在原地\n",
    "        - 到达目标获得 +1 奖励，其他情况 0 奖励\n",
    "    \n",
    "    观察空间：\n",
    "        Dict({\n",
    "            \"agent\": Box(0, size-1, shape=(2,)),   # 智能体的 [x, y] 坐标\n",
    "            \"target\": Box(0, size-1, shape=(2,)),  # 目标的 [x, y] 坐标\n",
    "        })\n",
    "    \n",
    "    动作空间：\n",
    "        Discrete(4):\n",
    "            0 - 向右移动\n",
    "            1 - 向上移动\n",
    "            2 - 向左移动\n",
    "            3 - 向下移动\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        size: Annotated[int, \"网格的边长\"] = 5,\n",
    "        render_mode: Annotated[Optional[str], \"渲染模式\"] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化网格世界环境\n",
    "        \n",
    "        参数：\n",
    "            size: 网格的大小（默认 5×5）\n",
    "            render_mode: 渲染模式 (\"human\" 或 \"rgb_array\")\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size = size\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # 定义观察空间\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int32),\n",
    "            \"target\": spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int32),\n",
    "        })\n",
    "        \n",
    "        # 定义动作空间\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # 动作到方向的映射（数学坐标系，y 轴向上）\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),    # 右\n",
    "            1: np.array([0, 1]),    # 上\n",
    "            2: np.array([-1, 0]),   # 左\n",
    "            3: np.array([0, -1]),   # 下\n",
    "        }\n",
    "        \n",
    "        # 初始化位置\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "    \n",
    "    def _get_obs(self) -> Annotated[Dict[str, np.ndarray], \"当前观察字典\"]:\n",
    "        \"\"\"返回当前观察\"\"\"\n",
    "        return {\n",
    "            \"agent\": self._agent_location.copy(),\n",
    "            \"target\": self._target_location.copy()\n",
    "        }\n",
    "    \n",
    "    def _get_info(self) -> Annotated[Dict[str, float], \"额外信息字典\"]:\n",
    "        \"\"\"返回额外信息（曼哈顿距离）\"\"\"\n",
    "        return {\n",
    "            \"distance\": float(np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            ))\n",
    "        }\n",
    "    \n",
    "    def reset(\n",
    "        self,\n",
    "        seed: Annotated[Optional[int], \"随机数种子\"] = None,\n",
    "        options: Annotated[Optional[dict], \"额外选项\"] = None,\n",
    "    ) -> Annotated[Tuple[Dict[str, np.ndarray], Dict[str, float]], \"初始观察和额外信息\"]:\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \n",
    "        参数：\n",
    "            seed: 随机种子\n",
    "            options: 额外选项\n",
    "        \n",
    "        返回：\n",
    "            observation: 初始观察\n",
    "            info: 额外信息\n",
    "        \"\"\"\n",
    "        # 设置随机种子\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # 随机放置智能体\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=np.int32)\n",
    "        \n",
    "        # 随机放置目标（确保不与智能体重合）\n",
    "        self._target_location = self._agent_location.copy()\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(0, self.size, size=2, dtype=np.int32)\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        action: Annotated[int, \"要执行的动作编号\"],\n",
    "    ) -> Annotated[Tuple[Dict[str, np.ndarray], float, bool, bool, Dict[str, float]], \"新观察、奖励、终止标志、截断标志及额外信息\"]:\n",
    "        \"\"\"\n",
    "        执行一个动作\n",
    "        \n",
    "        参数：\n",
    "            action: 要执行的动作 (0-3)\n",
    "        \n",
    "        返回：\n",
    "            observation: 新观察\n",
    "            reward: 奖励\n",
    "            terminated: 是否终止\n",
    "            truncated: 是否截断\n",
    "            info: 额外信息\n",
    "        \"\"\"\n",
    "        # 获取移动方向\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # 更新智能体位置（使用 clip 防止越界）\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction,\n",
    "            0,\n",
    "            self.size - 1\n",
    "        )\n",
    "        \n",
    "        # 检查是否到达目标\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        \n",
    "        # 计算奖励\n",
    "        reward = 1.0 if terminated else 0.0\n",
    "        \n",
    "        # 本环境不使用 truncated\n",
    "        truncated = False\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"渲染当前状态\"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_text()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return self._render_rgb()\n",
    "    \n",
    "    def _render_text(self):\n",
    "        \"\"\"文本模式渲染\"\"\"\n",
    "        print(\"\\\\n\" + \"=\" * (self.size * 2 + 1))\n",
    "        for y in range(self.size - 1, -1, -1):\n",
    "            row = \"|\"\n",
    "            for x in range(self.size):\n",
    "                if np.array_equal([x, y], self._agent_location):\n",
    "                    row += \"A \"\n",
    "                elif np.array_equal([x, y], self._target_location):\n",
    "                    row += \"T \"\n",
    "                else:\n",
    "                    row += \"· \"\n",
    "            print(row + \"|\")\n",
    "        print(\"=\" * (self.size * 2 + 1))\n",
    "        print(f\"智能体: {self._agent_location}, 目标: {self._target_location}, \"\n",
    "              f\"距离: {self._get_info()['distance']:.0f}\\\\n\")\n",
    "    \n",
    "    def _render_rgb(self):\n",
    "        \"\"\"图形模式渲染（返回 matplotlib figure）\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        # 绘制网格\n",
    "        for i in range(self.size + 1):\n",
    "            ax.axhline(i, color='gray', linewidth=0.5)\n",
    "            ax.axvline(i, color='gray', linewidth=0.5)\n",
    "        \n",
    "        # 绘制格子\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                rect = patches.Rectangle((x, y), 1, 1, \n",
    "                                         linewidth=1, edgecolor='gray', \n",
    "                                         facecolor='white')\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        # 绘制目标\n",
    "        target_rect = patches.Rectangle(\n",
    "            (self._target_location[0], self._target_location[1]), 1, 1,\n",
    "            linewidth=2, edgecolor='darkgreen', facecolor='lightgreen'\n",
    "        )\n",
    "        ax.add_patch(target_rect)\n",
    "        ax.text(self._target_location[0] + 0.5, self._target_location[1] + 0.5, \n",
    "                'T', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # 绘制智能体\n",
    "        agent_rect = patches.Rectangle(\n",
    "            (self._agent_location[0], self._agent_location[1]), 1, 1,\n",
    "            linewidth=2, edgecolor='darkblue', facecolor='lightblue'\n",
    "        )\n",
    "        ax.add_patch(agent_rect)\n",
    "        ax.text(self._agent_location[0] + 0.5, self._agent_location[1] + 0.5, \n",
    "                'A', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # 标题（使用英文避免中文显示问题）\n",
    "        distance = self._get_info()['distance']\n",
    "        ax.set_title(f'Grid World ({self.size}x{self.size}) | Distance: {distance:.0f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_xticks(range(self.size + 1))\n",
    "        ax.set_yticks(range(self.size + 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"清理资源\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"✅ GridWorldEnv 类定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25uaplm0d8g",
   "metadata": {},
   "source": [
    "## 第五步：测试完整环境\n",
    "\n",
    "让我们全面测试这个环境，确保它能正确工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "r38dcav3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "完整环境测试\n",
      "============================================================\n",
      "\\n初始观察: {'agent': array([0, 3], dtype=int32), 'target': array([3, 2], dtype=int32)}\n",
      "初始信息: {'distance': 4.0}\n",
      "\\n初始状态:\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|A · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 3], 目标: [3 2], 距离: 4\\n\n",
      "\\n开始交互（随机动作）:\n",
      "------------------------------------------------------------\n",
      "\\n步骤 1: 动作 = 左\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|A · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 3], 目标: [3 2], 距离: 4\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 2: 动作 = 上\n",
      "\\n===========\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 4], 目标: [3 2], 距离: 5\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 3: 动作 = 上\n",
      "\\n===========\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 4], 目标: [3 2], 距离: 5\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 4: 动作 = 左\n",
      "\\n===========\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 4], 目标: [3 2], 距离: 5\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 5: 动作 = 上\n",
      "\\n===========\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 4], 目标: [3 2], 距离: 5\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 6: 动作 = 上\n",
      "\\n===========\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 4], 目标: [3 2], 距离: 5\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 7: 动作 = 下\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|A · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 3], 目标: [3 2], 距离: 4\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 8: 动作 = 下\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|A · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 2], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 9: 动作 = 左\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|A · · T · |\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 2], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 10: 动作 = 下\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|A · · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [0 1], 目标: [3 2], 距离: 4\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 11: 动作 = 右\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· A · · · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [1 1], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 12: 动作 = 下\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· A · · · |\n",
      "===========\n",
      "智能体: [1 0], 目标: [3 2], 距离: 4\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 13: 动作 = 右\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · A · · |\n",
      "===========\n",
      "智能体: [2 0], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 14: 动作 = 下\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · A · · |\n",
      "===========\n",
      "智能体: [2 0], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 15: 动作 = 左\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· A · · · |\n",
      "===========\n",
      "智能体: [1 0], 目标: [3 2], 距离: 4\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 16: 动作 = 右\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · A · · |\n",
      "===========\n",
      "智能体: [2 0], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 17: 动作 = 右\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · A · |\n",
      "===========\n",
      "智能体: [3 0], 目标: [3 2], 距离: 2\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 18: 动作 = 左\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · A · · |\n",
      "===========\n",
      "智能体: [2 0], 目标: [3 2], 距离: 3\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 19: 动作 = 右\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · · · |\n",
      "|· · · A · |\n",
      "===========\n",
      "智能体: [3 0], 目标: [3 2], 距离: 2\\n\n",
      "奖励: 0.0, 终止: False\n",
      "\\n步骤 20: 动作 = 上\n",
      "\\n===========\n",
      "|· · · · · |\n",
      "|· · · · · |\n",
      "|· · · T · |\n",
      "|· · · A · |\n",
      "|· · · · · |\n",
      "===========\n",
      "智能体: [3 1], 目标: [3 2], 距离: 1\\n\n",
      "奖励: 0.0, 终止: False\n"
     ]
    }
   ],
   "source": [
    "# 创建环境实例\n",
    "env = GridWorldEnv(size=5, render_mode=\"human\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"完整环境测试\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 重置环境\n",
    "obs, info = env.reset(seed=42)\n",
    "print(f\"\\\\n初始观察: {obs}\")\n",
    "print(f\"初始信息: {info}\")\n",
    "print(\"\\\\n初始状态:\")\n",
    "env.render()\n",
    "\n",
    "# 模拟一个 episode\n",
    "print(\"\\\\n开始交互（随机动作）:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for step in range(20):  # 最多 20 步\n",
    "    # 随机选择一个动作\n",
    "    action = env.action_space.sample()\n",
    "    action_names = ['右', '上', '左', '下']\n",
    "    \n",
    "    # 执行动作\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    print(f\"\\\\n步骤 {step + 1}: 动作 = {action_names[action]}\")\n",
    "    env.render()\n",
    "    print(f\"奖励: {reward}, 终止: {terminated}\")\n",
    "    \n",
    "    if terminated:\n",
    "        print(\"\\\\n🎉 成功到达目标！\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91wi09ytezr",
   "metadata": {},
   "source": [
    "## 第六步：使用 Gymnasium 的环境检查器验证\n",
    "\n",
    "Gymnasium 提供了一个环境检查器，可以自动验证我们的环境是否符合标准："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48sr49jca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "使用 Gymnasium 环境检查器验证环境\n",
      "============================================================\n",
      "\\n✅ 环境通过所有检查！\n",
      "   - reset() 方法正确\n",
      "   - step() 方法正确\n",
      "   - 观察空间和动作空间定义正确\n",
      "   - 随机种子功能正常\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"使用 Gymnasium 环境检查器验证环境\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = GridWorldEnv(size=5)\n",
    "\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"\\\\n✅ 环境通过所有检查！\")\n",
    "    print(\"   - reset() 方法正确\")\n",
    "    print(\"   - step() 方法正确\")\n",
    "    print(\"   - 观察空间和动作空间定义正确\")\n",
    "    print(\"   - 随机种子功能正常\")\n",
    "except Exception as e:\n",
    "    print(f\"\\\\n❌ 环境检查失败: {e}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72g1kv9lqw",
   "metadata": {},
   "source": [
    "## 第七步：生成 GIF 动画（适合云端环境）\n",
    "\n",
    "在云端环境中，我们可以将 episode 的过程保存为 GIF 动画，然后在 Jupyter 中直接查看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ew3zha53qm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_array(\n",
    "    env: Annotated[GridWorldEnv, \"网格世界环境实例\"],\n",
    ") -> Annotated[np.ndarray, \"用于生成动画的 RGB 数组\"]:\n",
    "    \"\"\"\n",
    "    将当前环境状态渲染为 numpy 数组（RGB 图像）\n",
    "    \n",
    "    参数：\n",
    "        env: GridWorldEnv 实例\n",
    "    \n",
    "    返回：\n",
    "        rgb_array: shape 为 (height, width, 3) 的 numpy 数组\n",
    "    \"\"\"\n",
    "    # 创建图形，设置 DPI 以确保清晰度\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), dpi=80)\n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # 绘制网格线\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(i, color='gray', linewidth=0.5)\n",
    "        ax.axvline(i, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # 绘制所有格子\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            rect = patches.Rectangle((x, y), 1, 1, \n",
    "                                     linewidth=1, edgecolor='gray', \n",
    "                                     facecolor='white')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 绘制目标（绿色）\n",
    "    target_rect = patches.Rectangle(\n",
    "        (env._target_location[0], env._target_location[1]), 1, 1,\n",
    "        linewidth=2, edgecolor='darkgreen', facecolor='lightgreen'\n",
    "    )\n",
    "    ax.add_patch(target_rect)\n",
    "    ax.text(env._target_location[0] + 0.5, env._target_location[1] + 0.5, \n",
    "            'T', ha='center', va='center', fontsize=20, fontweight='bold', color='darkgreen')\n",
    "    \n",
    "    # 绘制智能体（蓝色）\n",
    "    agent_rect = patches.Rectangle(\n",
    "        (env._agent_location[0], env._agent_location[1]), 1, 1,\n",
    "        linewidth=2, edgecolor='darkblue', facecolor='lightblue'\n",
    "    )\n",
    "    ax.add_patch(agent_rect)\n",
    "    ax.text(env._agent_location[0] + 0.5, env._agent_location[1] + 0.5, \n",
    "            'A', ha='center', va='center', fontsize=20, fontweight='bold', color='darkblue')\n",
    "    \n",
    "    # 设置标题（使用英文避免中文乱码问题）\n",
    "    distance = np.linalg.norm(env._agent_location - env._target_location, ord=1)\n",
    "    ax.set_title(f'Grid World ({env.size}x{env.size}) | Distance: {distance:.0f}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('X', fontsize=12)\n",
    "    ax.set_ylabel('Y', fontsize=12)\n",
    "    ax.set_xticks(range(env.size + 1))\n",
    "    ax.set_yticks(range(env.size + 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 将图形转换为 RGB 数组\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # 使用 buffer_rgba() 方法获取图像数据\n",
    "    buf = fig.canvas.buffer_rgba()\n",
    "    rgb_array = np.asarray(buf)\n",
    "    \n",
    "    # 转换 RGBA 到 RGB（去掉 alpha 通道）\n",
    "    rgb_array = rgb_array[:, :, :3]\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return rgb_array\n",
    "\n",
    "\n",
    "def create_episode_gif(\n",
    "    env: Annotated[GridWorldEnv, \"网格世界环境实例\"],\n",
    "    policy: Annotated[Optional[callable], \"策略函数\"] = None,\n",
    "    max_steps: Annotated[int, \"最大步数\"] = 50,\n",
    "    filename: Annotated[str, \"保存的文件名\"] = 'gridworld_episode.gif',\n",
    "    fps: Annotated[int, \"每秒帧数\"] = 2,\n",
    "    seed: Annotated[Optional[int], \"随机种子\"] = None,\n",
    ") -> Annotated[Tuple[str, float, int], \"保存的文件路径、episode总奖励及实际步数\"]:\n",
    "    \"\"\"\n",
    "    创建一个 episode 的 GIF 动画\n",
    "    \n",
    "    参数：\n",
    "        env: GridWorldEnv 实例\n",
    "        policy: 策略函数，输入 observation，输出 action。如果为 None，则使用随机策略\n",
    "        max_steps: 最大步数\n",
    "        filename: 保存的文件名\n",
    "        fps: 每秒帧数\n",
    "        seed: 随机种子\n",
    "    \n",
    "    返回：\n",
    "        filename: 保存的文件路径\n",
    "        total_reward: episode 总奖励\n",
    "        steps: 实际步数\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    # 重置环境\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    \n",
    "    # 保存初始帧\n",
    "    print(f\"生成第 0 帧...\")\n",
    "    frames.append(render_to_array(env))\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # 运行 episode\n",
    "    for step in range(max_steps):\n",
    "        # 选择动作\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()  # 随机策略\n",
    "        else:\n",
    "            action = policy(obs)\n",
    "        \n",
    "        # 执行动作\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # 保存帧\n",
    "        print(f\"生成第 {steps} 帧...\")\n",
    "        frames.append(render_to_array(env))\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # 保存为 GIF\n",
    "    print(f\"\\n正在保存 GIF（共 {len(frames)} 帧）...\")\n",
    "    imageio.mimsave(filename, frames, fps=fps, loop=0)\n",
    "    \n",
    "    print(f\"\\n✅ GIF 动画已保存到: {filename}\")\n",
    "    print(f\"   总步数: {steps}\")\n",
    "    print(f\"   总奖励: {total_reward}\")\n",
    "    print(f\"   是否成功: {'是' if terminated else '否'}\")\n",
    "    \n",
    "    return filename, total_reward, steps\n",
    "\n",
    "\n",
    "# 测试 GIF 生成功能\n",
    "print(\"=\" * 60)\n",
    "print(\"测试 GIF 动画生成\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = GridWorldEnv(size=5)\n",
    "\n",
    "# 生成一个随机策略的 episode\n",
    "gif_file, reward, steps = create_episode_gif(\n",
    "    env, \n",
    "    policy=None,  # 随机策略\n",
    "    max_steps=50, \n",
    "    filename='gridworld_random.gif',\n",
    "    fps=2,  # 每秒 2 帧\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n正在显示 GIF...\")\n",
    "# 在 Jupyter 中显示 GIF\n",
    "display(IPImage(filename=gif_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5y9tlx22nqg",
   "metadata": {},
   "source": [
    "### 示例：使用简单策略生成 GIF\n",
    "\n",
    "我们可以定义一个简单的贪心策略（每次都朝目标方向移动），看看效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3zcnw2ltm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(\n",
    "    obs: Annotated[Dict[str, np.ndarray], \"观察字典，包含agent和target的位置\"],\n",
    ") -> Annotated[int, \"选择的动作编号\"]:\n",
    "    \"\"\"\n",
    "    贪心策略：每次都朝目标方向移动\n",
    "    \n",
    "    参数：\n",
    "        obs: 观察，包含 'agent' 和 'target' 的位置\n",
    "    \n",
    "    返回：\n",
    "        action: 选择的动作 (0=右, 1=上, 2=左, 3=下)\n",
    "    \"\"\"\n",
    "    agent_pos = obs['agent']\n",
    "    target_pos = obs['target']\n",
    "    \n",
    "    # 计算差值\n",
    "    dx = target_pos[0] - agent_pos[0]  # x 方向差值\n",
    "    dy = target_pos[1] - agent_pos[1]  # y 方向差值\n",
    "    \n",
    "    # 优先移动距离更远的方向\n",
    "    if abs(dx) > abs(dy):\n",
    "        # x 方向距离更远\n",
    "        if dx > 0:\n",
    "            return 0  # 向右\n",
    "        else:\n",
    "            return 2  # 向左\n",
    "    else:\n",
    "        # y 方向距离更远（或相等）\n",
    "        if dy > 0:\n",
    "            return 1  # 向上\n",
    "        else:\n",
    "            return 3  # 向下\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"使用贪心策略生成 GIF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = GridWorldEnv(size=8)  # 使用更大的网格\n",
    "\n",
    "# 生成贪心策略的 episode\n",
    "gif_file, reward, steps = create_episode_gif(\n",
    "    env, \n",
    "    policy=greedy_policy,  # 使用贪心策略\n",
    "    max_steps=50, \n",
    "    filename='gridworld_greedy.gif',\n",
    "    fps=3,  # 每秒 3 帧\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# 在 Jupyter 中显示 GIF\n",
    "display(IPImage(filename=gif_file))\n",
    "\n",
    "print(f\"\\n💡 提示：贪心策略总能找到最短路径（曼哈顿距离）！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvwhuaj7xpd",
   "metadata": {},
   "source": [
    "## 📚 总结与下一步\n",
    "\n",
    "### 🎯 你已经学会了：\n",
    "\n",
    "1. **Gymnasium 的核心概念**\n",
    "   - 观察空间 (Observation Space)\n",
    "   - 动作空间 (Action Space)\n",
    "   - 状态转移函数 (Transition Function)\n",
    "   - 奖励函数 (Reward Function)\n",
    "\n",
    "2. **如何创建自定义环境**\n",
    "   - 继承 `gym.Env` 基类\n",
    "   - 实现必需的方法：`__init__()`, `reset()`, `step()`\n",
    "   - 添加可视化：文本渲染和图形渲染\n",
    "\n",
    "3. **云端环境的可视化方案**\n",
    "   - ✅ 文本渲染（ASCII 字符）\n",
    "   - ✅ 静态图片（matplotlib）\n",
    "   - ✅ **GIF 动画**（imageio + matplotlib）\n",
    "\n",
    "4. **环境的测试与验证**\n",
    "   - 使用 `check_env()` 验证环境符合标准\n",
    "   - 通过交互测试环境行为\n",
    "\n",
    "### 🔜 接下来可以做什么？\n",
    "\n",
    "现在你已经有了一个标准的网格世界环境，可以用它来学习：\n",
    "\n",
    "1. **贝尔曼最优公式**\n",
    "   - 计算状态价值函数 V(s)\n",
    "   - 计算动作价值函数 Q(s, a)\n",
    "\n",
    "2. **值迭代算法**\n",
    "   - 实现值迭代求解最优策略\n",
    "   - 可视化值函数的收敛过程\n",
    "   - 生成最优策略的 GIF 动画\n",
    "\n",
    "3. **策略迭代算法**\n",
    "   - 实现策略评估\n",
    "   - 实现策略改进\n",
    "   - 对比值迭代与策略迭代\n",
    "   - 可视化策略改进过程\n",
    "\n",
    "### 💡 环境的扩展方向\n",
    "\n",
    "你可以根据需要扩展这个环境：\n",
    "\n",
    "- **添加障碍物**：某些格子不可通过\n",
    "- **添加陷阱**：踩到陷阱获得负奖励\n",
    "- **不同的奖励函数**：\n",
    "  - 每步给小的负奖励（鼓励最短路径）\n",
    "  - 基于距离的奖励塑形\n",
    "- **随机转移**：动作有一定概率失败（随机 MDP）\n",
    "- **更大的网格**：测试算法的可扩展性\n",
    "\n",
    "### 🎨 GIF 动画的使用场景\n",
    "\n",
    "生成的 GIF 动画特别适合：\n",
    "- 📊 **可视化学习过程**：观察智能体如何改进策略\n",
    "- 📝 **写报告和论文**：展示算法效果\n",
    "- 🎓 **教学演示**：直观展示强化学习概念\n",
    "- 🐛 **调试策略**：发现策略的问题\n",
    "\n",
    "---\n",
    "\n",
    "**恭喜你完成了 Gymnasium 基础教程！现在你可以开始实现值迭代和策略迭代算法了。** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
